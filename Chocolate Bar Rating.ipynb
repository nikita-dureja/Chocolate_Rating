{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company \n",
       "(Maker-if known)</th>\n",
       "      <th>Specific Bean Origin\n",
       "or Bar Name</th>\n",
       "      <th>REF</th>\n",
       "      <th>Review\n",
       "Date</th>\n",
       "      <th>Cocoa\n",
       "Percent</th>\n",
       "      <th>Company\n",
       "Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Bean\n",
       "Type</th>\n",
       "      <th>Broad Bean\n",
       "Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valrhona</td>\n",
       "      <td>Manjari</td>\n",
       "      <td>129</td>\n",
       "      <td>2007</td>\n",
       "      <td>64%</td>\n",
       "      <td>France</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Criollo, Trinitario</td>\n",
       "      <td>Madagascar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original Beans (Felchlin)</td>\n",
       "      <td>Grand Cru Blend No.1, 5 yr. Anniversary Ed</td>\n",
       "      <td>1442</td>\n",
       "      <td>2014</td>\n",
       "      <td>80%</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Blend</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potomac</td>\n",
       "      <td>Upala w/ nibs</td>\n",
       "      <td>647</td>\n",
       "      <td>2011</td>\n",
       "      <td>70%</td>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Matina</td>\n",
       "      <td>Costa Rica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middlebury</td>\n",
       "      <td>Matagalpa, Cacao Bisiesto</td>\n",
       "      <td>1538</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>3.50</td>\n",
       "      <td></td>\n",
       "      <td>Nicaragua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlotta Chocolat</td>\n",
       "      <td>Cesar</td>\n",
       "      <td>1888</td>\n",
       "      <td>2016</td>\n",
       "      <td>65%</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>3.50</td>\n",
       "      <td>CCN51</td>\n",
       "      <td>Colombia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company \\n(Maker-if known)           Specific Bean Origin\\nor Bar Name  \\\n",
       "0                   Valrhona                                     Manjari   \n",
       "1  Original Beans (Felchlin)  Grand Cru Blend No.1, 5 yr. Anniversary Ed   \n",
       "2                    Potomac                               Upala w/ nibs   \n",
       "3                 Middlebury                   Matagalpa, Cacao Bisiesto   \n",
       "4          Carlotta Chocolat                                       Cesar   \n",
       "\n",
       "    REF  Review\\nDate Cocoa\\nPercent Company\\nLocation  Rating  \\\n",
       "0   129          2007            64%            France    4.00   \n",
       "1  1442          2014            80%       Switzerland    3.25   \n",
       "2   647          2011            70%            U.S.A.    3.50   \n",
       "3  1538          2015            70%            U.S.A.    3.50   \n",
       "4  1888          2016            65%          Colombia    3.50   \n",
       "\n",
       "            Bean\\nType Broad Bean\\nOrigin  \n",
       "0  Criollo, Trinitario         Madagascar  \n",
       "1                Blend                     \n",
       "2               Matina         Costa Rica  \n",
       "3                               Nicaragua  \n",
       "4                CCN51           Colombia  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing the csv data files \n",
    "data = pd.read_csv('chocolate.csv',error_bad_lines=False, warn_bad_lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 9)\n",
      "Columns of dataset- \n",
      "Index([u'Company \\n(Maker-if known)', u'Specific Bean Origin\\nor Bar Name',\n",
      "       u'REF', u'Review\\nDate', u'Cocoa\\nPercent', u'Company\\nLocation',\n",
      "       u'Rating', u'Bean\\nType', u'Broad Bean\\nOrigin'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 9 columns):\n",
      "Company \n",
      "(Maker-if known)           1500 non-null object\n",
      "Specific Bean Origin\n",
      "or Bar Name    1500 non-null object\n",
      "REF                                 1500 non-null int64\n",
      "Review\n",
      "Date                         1500 non-null int64\n",
      "Cocoa\n",
      "Percent                       1500 non-null object\n",
      "Company\n",
      "Location                    1500 non-null object\n",
      "Rating                              1500 non-null float64\n",
      "Bean\n",
      "Type                           1499 non-null object\n",
      "Broad Bean\n",
      "Origin                   1499 non-null object\n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 105.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Printing some information about the dataset\n",
    "print data.shape\n",
    "print 'Columns of dataset- \\n', data.columns\n",
    "print data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>bean_origin</th>\n",
       "      <th>REF</th>\n",
       "      <th>review_date</th>\n",
       "      <th>cocoa_percent</th>\n",
       "      <th>company_location</th>\n",
       "      <th>rating</th>\n",
       "      <th>bean_typ</th>\n",
       "      <th>country_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valrhona</td>\n",
       "      <td>Manjari</td>\n",
       "      <td>129</td>\n",
       "      <td>2007</td>\n",
       "      <td>64%</td>\n",
       "      <td>France</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Criollo, Trinitario</td>\n",
       "      <td>Madagascar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original Beans (Felchlin)</td>\n",
       "      <td>Grand Cru Blend No.1, 5 yr. Anniversary Ed</td>\n",
       "      <td>1442</td>\n",
       "      <td>2014</td>\n",
       "      <td>80%</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>3.25</td>\n",
       "      <td>Blend</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potomac</td>\n",
       "      <td>Upala w/ nibs</td>\n",
       "      <td>647</td>\n",
       "      <td>2011</td>\n",
       "      <td>70%</td>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Matina</td>\n",
       "      <td>Costa Rica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middlebury</td>\n",
       "      <td>Matagalpa, Cacao Bisiesto</td>\n",
       "      <td>1538</td>\n",
       "      <td>2015</td>\n",
       "      <td>70%</td>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>3.50</td>\n",
       "      <td></td>\n",
       "      <td>Nicaragua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlotta Chocolat</td>\n",
       "      <td>Cesar</td>\n",
       "      <td>1888</td>\n",
       "      <td>2016</td>\n",
       "      <td>65%</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>3.50</td>\n",
       "      <td>CCN51</td>\n",
       "      <td>Colombia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     company                                 bean_origin  \\\n",
       "0                   Valrhona                                     Manjari   \n",
       "1  Original Beans (Felchlin)  Grand Cru Blend No.1, 5 yr. Anniversary Ed   \n",
       "2                    Potomac                               Upala w/ nibs   \n",
       "3                 Middlebury                   Matagalpa, Cacao Bisiesto   \n",
       "4          Carlotta Chocolat                                       Cesar   \n",
       "\n",
       "    REF  review_date cocoa_percent company_location  rating  \\\n",
       "0   129         2007           64%           France    4.00   \n",
       "1  1442         2014           80%      Switzerland    3.25   \n",
       "2   647         2011           70%           U.S.A.    3.50   \n",
       "3  1538         2015           70%           U.S.A.    3.50   \n",
       "4  1888         2016           65%         Colombia    3.50   \n",
       "\n",
       "              bean_typ country_origin  \n",
       "0  Criollo, Trinitario     Madagascar  \n",
       "1                Blend                 \n",
       "2               Matina     Costa Rica  \n",
       "3                           Nicaragua  \n",
       "4                CCN51       Colombia  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the column name\n",
    "new_col_names = ['company', 'bean_origin', 'REF', 'review_date', 'cocoa_percent',\n",
    "                'company_location', 'rating', 'bean_typ', 'country_origin']\n",
    "data_clean = data.rename(columns=dict(zip(data.columns, new_col_names)))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for the column: company\n",
      "['Valrhona' 'Original Beans (Felchlin)' 'Potomac' 'Middlebury'\n",
      " 'Carlotta Chocolat' 'Salgado' 'Cacao Store' 'Kaoka (Cemoi)' 'Woodblock'\n",
      " 'Dormouse' 'Soma' 'Mast Brothers' 'Bittersweet Origins'\n",
      " 'Chocolate Tree, The' 'Mayacama' 'Fruition' 'Mars' 'Fresco'\n",
      " 'Tablette (aka Vanillabeans)' 'Compania de Chocolate (Salgado)' 'Pralus'\n",
      " 'Coppeneur' 'Chchukululu (Tulicorp)' 'Black Mountain' 'Bonnat' 'hexx'\n",
      " 'Askinosie' 'Danta' 'Quetzalli (Wolter)' \"K'ul\" 'Chocolats Privilege'\n",
      " 'Momotombo' 'Republica del Cacao (aka Confecta)' 'Tejas'\n",
      " 'Svenska Kakaobolaget' 'Benoit Nihant' 'Amedei' 'Chloe Chocolat'\n",
      " 'Montecristi' 'Domori' 'Guido Castagna' 'Madre' 'Emerald Estate'\n",
      " 'Starchild' 'Lonohana' 'Ah Cacao' 'TCHO' 'Violet Sky' 'Mission'\n",
      " 'French Broad' 'Kto' 'Theo' 'Kyya' 'Letterpress' 'Ocho' 'Sirene'\n",
      " 'Palette de Bine' 'Arete' 'Dandelion' 'Georgia Ramon' 'Malmo' 'Bright'\n",
      " 'Garden Island' 'A. Morin' 'Parliament' 'Kallari (Ecuatoriana)'\n",
      " 'Harper Macaw' 'Friis Holm (Bonnat)' 'Cacao Barry' 'hello cocoa'\n",
      " 'Terroir' \"Ethel's Artisan (Mars)\" 'Cello' 'Bisou'\n",
      " 'Hotel Chocolat (Coppeneur)' 'Marsatta' 'Peppalo' 'Naive' 'Durci'\n",
      " 'Santander (Compania Nacional)' 'Ambrosia' 'Doble & Bignall'\n",
      " 'Amatller (Simon Coll)' 'Meadowlands' 'Brazen' 'Cravve' 'Cacao Hunters'\n",
      " 'Isidro' 'Adi' 'Hummingbird' 'Bahen & Co.' 'Damson' \"Duffy's\"\n",
      " 'Q Chocolate' 'Guittard' 'Vintage Plantations' 'Mutari' 'Zotter'\n",
      " 'Felchlin' 'Castronovo' 'Vao Vao (Chocolaterie Robert)' 'Martin Mayer'\n",
      " 'Menakao (aka Cinagra)' 'Bar Au Chocolat' 'Belcolade' \"Zak's\" 'S.A.I.D.'\n",
      " 'Sacred' 'Muchomas (Mesocacao)' 'Noble Bean aka Jerjobo' 'Lillie Belle'\n",
      " 'Pacari' 'Tobago Estate (Pralus)' 'Davis' 'Santome' 'Sibu'\n",
      " 'Scharffen Berger' 'Bellflower' 'Just Good Chocolate'\n",
      " 'Millcreek Cacao Roasters' 'Batch' \"Shark's\" 'L.A. Burdick (Felchlin)'\n",
      " 'Szanto Tibor' 'Wm' 'Feitoria Cacao' \"Willie's Cacao\"\n",
      " 'Heirloom Cacao Preservation (Fruition)' 'Olivia' 'Izard' 'Marou'\n",
      " 'twenty-four blackbirds' 'Soul' 'Patric' 'Chocolate Con Amor' 'Shattel'\n",
      " 'Mana' 'C-Amaro' 'Vanleer (Barry Callebaut)' 'Altus aka Cao Artisan'\n",
      " \"Wilkie's Organic\" 'Smooth Chocolator, The' 'Cacao de Origen'\n",
      " 'Cacao Sampaka' 'Machu Picchu Trading Co.' 'SRSLY' 'Rozsavolgyi' 'Nuance'\n",
      " 'Amano' 'Caoni (Tulicorp)' 'Michel Cluizel' 'Shark Mountain'\n",
      " 'Wellington Chocolate Factory' 'Manifesto Cacao'\n",
      " 'Robert (aka Chocolaterie Robert)' 'Stone Grindz' 'Belyzium' 'Holy Cacao'\n",
      " 'Cacaoyere (Ecuatoriana)' 'Zart Pralinen' 'Neuhaus (Callebaut)' 'Choklat'\n",
      " 'Laia aka Chat-Noir' 'Park 75' 'Artisan du Chocolat (Casa Luker)'\n",
      " 'Debauve & Gallais (Michel Cluizel)' \"Emily's\" 'Idilio (Felchlin)'\n",
      " 'Cacao Market' 'Alain Ducasse' 'Malagasy (Chocolaterie Robert)' 'Obolo'\n",
      " 'Spagnvola' 'Blue Bandana' 'Kah Kow' 'Map Chocolate' 'Silvio Bessone'\n",
      " 'Luker' 'iQ Chocolate' 'Dick Taylor' 'Pascha' 'Bowler Man' 'Chocolarder'\n",
      " 'El Rey' 'DAR' 'Videri' 'Artisan du Chocolat' 'Maglio' 'Molucca' 'Pangea'\n",
      " 'Alexandre' 'Maverick' 'Haigh' 'Vintage Plantations (Tulicorp)'\n",
      " 'La Maison du Chocolat (Valrhona)' 'Chocovic' 'Spencer' 'Bakau' 'Rogue'\n",
      " 'Pump Street Bakery' 'Caribeans' 'Moho' 'Beschle (Felchlin)'\n",
      " 'Pierre Marcolini' 'Undone' 'Beau Cacao' 'El Ceibo' 'Habitual' 'Loiza'\n",
      " 'Ritual' 'Manoa' 'Cacao Prieto' 'Snake & Butterfly' 'Mita' 'Matale'\n",
      " 'Chocovivo' 'Charm School' 'Daintree' 'Xocolat' 'Un Dimanche A Paris'\n",
      " 'StRita Supreme' 'Kakao' 'Cao' 'Orquidea' 'Pura Delizia' 'Franceschi'\n",
      " 'DeVries' 'East Van Roasters' 'organicfair' 'Tan Ban Skrati' 'Pinellas'\n",
      " 'Goodnow Farms' 'Dolfin (Belcolade)' 'Omnom' 'Oakland Chocolate Co.'\n",
      " 'Summerbird' 'Chocolate Alchemist-Philly' 'Beehive' 'De Villiers'\n",
      " 'Ranger' 'Kiskadee' 'Solstice' 'Metiisto' \"L'Amourette\"\n",
      " 'Aequare (Gianduja)' 'Zokoko' 'Bouga Cacao (Tulicorp)'\n",
      " 'Grenada Chocolate Co.' \"Brasstown aka It's Chocolate\" 'Nibble'\n",
      " 'Frederic Blondeel' 'Ohiyo' 'Na\\xef\\xbf\\xbdve' 'Dalloway'\n",
      " 'Chokolat Elot (Girard)' 'Chocolate Makers' 'Honest'\n",
      " 'Heirloom Cacao Preservation (Guittard)' 'Pitch Dark' 'Majani'\n",
      " 'Eclat (Felchlin)' 'Jordis' 'Sprungli (Felchlin)' 'Nathan Miller'\n",
      " 'Erithaj (A. Morin)' 'Mesocacao' 'Ara' 'Cacao Atlanta' 'Sublime Origins'\n",
      " 'Vivra' 'Paul Young' 'Lake Champlain (Callebaut)' 'Solomons Gold'\n",
      " 'Hoja Verde (Tulicorp)' 'Green Bean to Bar' 'Chequessett' 'Whittakers'\n",
      " 'Forteza (Cortes)' 'Manufaktura Czekolady' 'La Chocolaterie Nanairo'\n",
      " 'Roasting Masters' 'Madecasse (Cinagra)' 'Vicuna' 'Coleman & Davis'\n",
      " 'Olive and Sinclair' \"Noir d' Ebine\" 'Choco Dong' 'Chocablog'\n",
      " 'Metropolitan' 'Shattell' 'Sjolinds' 'AMMA' 'Two Ravens'\n",
      " 'Dole (Guittard)' 'Escazu' 'Kerchner' 'Cacao Arabuco' 'Dark Forest'\n",
      " 'Dean and Deluca (Belcolade)' 'Chocolate Conspiracy' 'Blanxart'\n",
      " 'Theobroma' 'Ethereal' 'Durand' 'Marana' 'Britarev' 'Tocoti'\n",
      " 'Hacienda El Castillo' 'Heirloom Cacao Preservation (Zokoko)'\n",
      " 'Christopher Morel (Felchlin)' 'Glennmade' 'Malie Kai (Guittard)' 'Indi'\n",
      " 'Nanea' 'The Barn' 'Chaleur B' 'Dulcinea' 'Anahata' 'Hogarth' 'Tabal'\n",
      " 'Confluence' 'Sol Cacao' 'Forever Cacao' 'Rain Republic'\n",
      " 'Monsieur Truffe' 'Indah' 'Suruca Chocolate' 'Acalli' \"Chocola'te\"\n",
      " 'Somerville' 'Heirloom Cacao Preservation (Millcreek)' 'Mindo' 'Omanhene'\n",
      " 'Condor' 'Taza' 'Amazona' 'Minimal' 'Claudio Corallo' 'Nahua' 'Urzi'\n",
      " 'Pomm (aka Dead Dog)' \"Akesson's (Pralus)\" 'Tsara (Cinagra)' 'Nova Monda'\n",
      " 'Raw Cocoa' 'Timo A. Meyer' 'Malagos' 'Hachez' \"To'ak (Ecuatoriana)\"\n",
      " \"Cote d' Or (Kraft)\" 'Stella (aka Bernrain)' 'Oialla by Bojessen (Malmo)'\n",
      " 'Night Owl' 'Raaka' 'Upchurch' 'Fossa' 'Fearless (AMMA)' 'ChocoReko'\n",
      " 'Original Hawaiin Chocolate Factory' 'Rococo (Grenada Chocolate Co.)'\n",
      " 'Finca' 'Indaphoria' 'Lindt & Sprungli' 'La Pepa de Oro' 'Ocelot'\n",
      " 'Captain Pembleton' 'Hotel Chocolat' 'Creo' 'Sibu Sura' 'La Oroquidea'\n",
      " 'Xocolla' 'Callebaut' 'Choco Del Sol' 'Nugali' 'Chuao Chocolatier'\n",
      " \"Marigold's Finest\" \"Baravelli's\" 'Friis Holm' 'Raoul Boulanger'\n",
      " \"Green & Black's (ICAM)\" 'Choocsol' 'Eau de Rose' 'Jacque Torres'\n",
      " 'Burnt Fork Bend' 'Chuao Chocolatier (Pralus)' 'Desbarres'\n",
      " 'Bronx Grrl Chocolate' 'Heirloom Cacao Preservation (Mindo)' 'Cemoi'\n",
      " 'Love Bar' 'Monarque']\n",
      "\n",
      "\n",
      "Unique values for the column: bean_origin\n",
      "['Manjari' 'Grand Cru Blend No.1, 5 yr. Anniversary Ed' 'Upala w/ nibs'\n",
      " 'Matagalpa, Cacao Bisiesto' 'Cesar' 'Rio Arriba' 'Madagascar' 'Noir'\n",
      " 'Gran Couva' 'Colombia, Batch 9' 'Ocumare' 'Dominican Republic, Coop'\n",
      " 'Sambirano, 2009' 'Peru' 'Shake Shack' 'Hispaniola'\n",
      " 'Coto Brus, Heirloom, Batch 1' 'Matina 1-6, prototype'\n",
      " 'Jamaica, #209, DR, SC' 'Vietnam' 'Colombie' 'Los Rios, H. Iara' 'Arriba'\n",
      " 'Matiguas' 'Selva Maya' 'Ecuador' 'Cortes' 'Los Ujuxtes'\n",
      " 'Mexico, Lot 28022016' 'Maranon, Fortunato No. 4' 'Xoconusco' 'Mombacho'\n",
      " 'El Oro' 'Capistrano*' 'Sri Lanka'\n",
      " 'Bali, Sukrama Bros. Farm, Melaya, 62hr C' 'Cahabon Region'\n",
      " 'Sambirano, Menava P.' 'Toscano Black' 'Blend No. 1' 'Manabi' 'Ilblend'\n",
      " 'Puerto Rico' 'Emerald Estate' \"Opaeula Estate, O'ahu, Ele'ele\"\n",
      " 'Sambirano' 'Bellavista Coop, #225, LR, MC, CG Exclusive' 'Tabasco'\n",
      " 'Criollo, Hawaii' 'Peru- Ecuador' 'Jamaica, #205, DR, MC'\n",
      " 'Jamaica, #206, DR, LC' 'Nyangbo' 'Fazenda Camboa, Bahia' 'San Andres'\n",
      " 'Porcelana, Pedegral' 'Ocumare 61' 'Ivory Coast' 'Uganda' 'Choobua, Kona'\n",
      " 'Oko Caribe, Duarte P., Collab w Chocosol' 'Sang Yum Coop'\n",
      " 'Camino Verde P., Balao, Guayas' 'Kokoa Kamili' 'PNG, Nib Bar'\n",
      " 'Fazenda Camboa' 'Trinidad' 'Upala' 'Conacado Coop' 'Chuao'\n",
      " 'Marabel Farms' \"Kaua'I, Alea Estate +world\" 'Akata'\n",
      " \"Lachua, Q'egchi families\" 'Roberto' 'La Red'\n",
      " 'Vale do Juliana E., Atlantic Forest' 'Medagla, Xoco' 'Venezuela'\n",
      " 'Bolivia' 'San Andres, silk' 'Santo Domingo' 'Dominican Republic'\n",
      " 'Maranon, Cajamarca' 'Maranon Canyon, Fortunato No. 4' 'Papua New Guinea'\n",
      " 'Corona Arriba' 'Colombian Semi Dark' 'Panama, Raven' 'Belize'\n",
      " 'Alta Verapaz, 2014' 'Vanuatu' 'Quilla' 'Perla Negra' 'Vanua Levu'\n",
      " 'Papua' 'Tien Giang' 'Bolivia, Bo-nib-ia, w/ nibs'\n",
      " 'San Francisco de Macoris, Cibao region' 'Huila' 'Robson Estate'\n",
      " 'Djual Island' 'Mexico' 'Star of Peru' 'Brazil' 'Los Rios, Quevedo'\n",
      " 'San Martin' 'Ambanja, batch 1 SRB' 'Peru, Madagascar' 'Sierra Nevada'\n",
      " 'Madagascar w/ nibs' 'Tumaco' 'Chiapas' 'Peru, Batch 1' '100 percent'\n",
      " 'Ambolikapiky P.' 'Midnight' 'Nicaragua' 'Maranon Canyon'\n",
      " 'Markham Valley, #219, LR, MC' 'La Selva' 'Sierra Nevada, Tutu Iku'\n",
      " 'PNG, Revolution' 'Roxborough, Tobago' 'Ghana' 'Sao Tome' 'Oro' 'Cuyagua'\n",
      " 'Markham Valley' 'Costa Rica' \"Haleiwa E, O'ahu, 2014\"\n",
      " 'Kakao Kamili, Kilombero Valley' 'Colombian'\n",
      " 'Dominican Republic, Batch 3' 'Tanzania' 'Hilo' 'Trinidad & Tobago'\n",
      " 'Venzuela' 'Baracoa' 'Ghana, 2013, batch 129' 'Oko Caribe'\n",
      " 'Somia Plantation, 2012' 'Alto Beni, Palos Blancos'\n",
      " 'Three Amigos(Chuao, Wild Bolivia, D.R.)' 'Maya Mountain' 'Madagared'\n",
      " 'Java' 'Cota Brus, Terciopelo, 2015' 'Sao Tome & Principe'\n",
      " 'Lever du Soleil' 'Carribean-Raw' 'Somia Plantation, Sambirano, 70hr C'\n",
      " 'Maya Mountain, Toledo, Batch 29' 'Cacao Nacional W.F.'\n",
      " 'Tien Giang, Gao Co-op' 'Bahia' 'Cedeno, lot 271'\n",
      " 'Rio Caribe, Paria Penninsula' 'Tingo Maria' 'Purple Haze'\n",
      " '2009 Hapa Nibby' 'Napa' 'Ghana, Panama, Ecuador' 'CIAAB Coop' 'Tumbes'\n",
      " 'Amina' \"Akesson's Estate\" 'Chuao, Aragua region' 'PNG, Devotion'\n",
      " 'Non Pariel Estate' 'Costa Esmeraldas' 'Grenada' 'House Blend, Batch 2'\n",
      " 'Conacado' 'Porcelana, Apotequil' 'Principe' 'Ecuador, 2013'\n",
      " 'Piura Blanco' 'Esmeraldas' 'Venezuela, Trinidad' 'Alpaco'\n",
      " 'Carenero Superior, Concepcion' 'Belize, 2013' 'Piura Blanco, Norandino'\n",
      " 'Colombia' 'Rio Dulce, Xoco' 'Ecuador, Twilght Dark' 'Papua Kerafat'\n",
      " 'Belize south' 'Maranon' 'Australia' 'San Juan Estate'\n",
      " 'Manickchand Estate' 'Porcelana, Tabasco' 'Autumn, Primary Harvest, 2012'\n",
      " 'Porcelana' 'Maranon, Joya Rara' 'Lam Dong, Batch 153' 'Jamaica'\n",
      " 'Concepcion' 'Maralumi P.' 'South America' 'Cumbia'\n",
      " 'Orinoqua Region, Arauca' 'Ambolikapkly P.' 'Carenero'\n",
      " 'Coopertiva Amazona' 'Jutiapa, lot 050916D' 'Carenero Superior'\n",
      " 'Zorzal Reserva w/ Charles Kerchner' 'Arauca' 'Mora Mora 2006'\n",
      " 'San Martin, Bellavista Coop, #226, DR, MC' 'Pangoa, w/ nibs'\n",
      " 'AgroCriso Plantation' 'Barba, Xoco' 'Le Noir Extra Amer' 'Lachua'\n",
      " 'Kokoa Kamili Coop' 'Madagascar, Batch 59/100'\n",
      " 'Rizek Cacao, Cibao Valley, Domin. Rep.' 'Maranon, 2014'\n",
      " 'Porcelana, Colombia, Amazonas' 'Brazil, Mitzi Blue' 'Macondo'\n",
      " 'Satipo region, white label' 'Bahia, Batch 148' 'Las Acacias E.'\n",
      " 'Houseblend' 'Pangoa' 'Cacao Verapaz' 'Los Rios' 'Complexite' 'Nibby'\n",
      " 'Satipo Pangoa region, 16hr conche' 'Los Rios, H. Iara, 96hr c.'\n",
      " 'Wild Bolivian, Batch 2' 'Carenero Superior, Bucare' 'DUO, batch 002'\n",
      " 'Classic' 'Panama' 'Fazenda Sempre Firme, Bahia'\n",
      " 'Chuno, double turned, Xoco' 'Indonesia' 'Johe, Xoco' 'Matasawalevu'\n",
      " 'Winak Coop, Napo' 'Duarte Province' 'Bocas del Toro, Cocabo Co-op'\n",
      " 'Elvesia' 'Porcelana, Zulia' \"Haleiwa, O'ahu; Lonohana E., Kanahiku\"\n",
      " 'Diego 60hr/ W.F. blend prototype' 'South America and Africa'\n",
      " 'Gruppo Salinas' 'Tamarina' 'Ocumare 67, Puertofino'\n",
      " \"Opaeula Estate, O'ahu, Nene, CG Exclusive\"\n",
      " 'Los Rios, Rancho Grande 2004/2007' 'Alto Beni' 'Tobago'\n",
      " 'Cusco, Cacao Cusco' 'Rio Caribe' 'Guyave' 'Cuba, Batch 59/100'\n",
      " 'Dominican Republic, lot D82R' 'Huallabamba, 2015' 'Momotombo'\n",
      " 'Guantupi River' 'Pichincha' 'San Juan' 'Island Growers, 96hr c.'\n",
      " 'Toledo District, w/ nibs' 'Rizek Cacao, Domin. Rep.'\n",
      " 'Porcelana, Premier Cru, Quizas No. 1' 'Sambirano, Ambanja, Madagascar'\n",
      " 'Nourish' 'Asajaya E, NW Borneo, b. #132/4500'\n",
      " 'Trinidad, Heritage, Limited ed.' 'Sangre Grande P., Trinidad'\n",
      " 'Somia Plantation' 'Hispaniola, 2008' 'Ankasa' 'Sharkey' 'Lam Dong'\n",
      " 'Piura, Apotequil, \"Porcelana\" 72hr c.' 'Carenero Superior, Mijao'\n",
      " 'Elvesia P., Black Science' 'D.R. Congo, Cru Virunga'\n",
      " 'Madagascar, 100% criollo' 'Porcelana, Venezuela'\n",
      " 'Hilo, w/ added cocoa butter' 'Camino Verde P., Balao, Guayas, 2014'\n",
      " 'Loma Los Pinos, Yacao region, D.R.' 'Coto Brus'\n",
      " 'Criollo, Dominican Republic' 'Alto Beni, Covendo Region' 'Oscuro'\n",
      " 'Carribean' 'Carenero Superior, Urrutia, Barlovento' 'Blend'\n",
      " 'Anselmo Paraiso Estate' 'Granella' 'Ben Tre, Surprise Valley' 'Montanya'\n",
      " 'Umoho R., Toledo District, San Felipe' 'Cuba'\n",
      " 'Dancing in Your Head, 5 bean blend' 'ABOCFA Coop' 'Chuao 2002 P.'\n",
      " 'Maya Mtn' 'Goodman Estate' 'Akosombo' 'Haiti' 'Bali, Singaraja'\n",
      " 'Bali, Jembrana' 'Amazonia' 'Camino Verde P., Balao, Guayas, 2012'\n",
      " 'La Tronca, Matagalpa' 'Guatemala' 'Coto Brus, Terciopelo' 'Brazilian'\n",
      " 'Samar, East Visayas region' 'Nocturne' 'Oko Caribe, DOR005'\n",
      " 'Rio Caribe, Tepui Treasure' 'Raw' 'Choroni' 'Ghana, 2013'\n",
      " 'Sambirano Valley, #216, MR, LC' \"Akesson's, batch 4411\"\n",
      " 'Vanua Levu, Ami-Ami-CA' 'Finisterra' 'Paramaribo, batch 20160043-01'\n",
      " 'Wampusirpi Region' 'San Joaquin' 'Asochivite, batch 1005'\n",
      " 'Sambirano, Akesson Estate' 'Kendari' 'Boyaca, Aprocampa Coop, Pauna'\n",
      " 'Ben Tre, Mekong Delta' 'Trintade, Sao Tome'\n",
      " 'Papua New Guinea, triple roast, batch 1' 'Hawaiian Crown, Kona Vanilla'\n",
      " 'Maya Mtn, Batch 454, Heirloom' 'Nacional' 'Chuao, batch 3'\n",
      " 'Philly Blend, 5 plantations' \"Grand 'Anse\" 'Brazil, Batch 20316'\n",
      " 'Esmeraldas, Salazar Farm' 'Mid Mountain, 2014'\n",
      " 'Sambirano Valley, batch 2477' 'Kpime'\n",
      " 'Akessons Estate, Sambirano, Ambanja' 'Chulucanas, Batch 1'\n",
      " \"Jamaica a l'ancienne\" 'La Red, Guanconjeco' 'San Jose' 'Amazonas'\n",
      " 'Akesson Estate' 'Mantuano, 2012' 'Los Rios, Quevedo, Arriba'\n",
      " 'Carenero S., Barlovento, Grand Cru' 'Tokiala' 'TCHOPro 68'\n",
      " 'El Oro, Hacienda de Oro' 'Los Llanos' 'Rio Caribe, Batch 7'\n",
      " 'Diego 48hr/ W.F. blend prototype' 'Toledo District' 'Balinese, Java'\n",
      " 'Chiapas, Triple Cacao' 'Guasare, Zulia Prov.' 'Xoconusco, Chiapas'\n",
      " 'Dark' 'Carupano, H. San Jose' 'Somia, 2013' 'Malekula Island'\n",
      " 'Sisa 36hr/ W. F. blend prototype' 'Maracaibo, El Rosario'\n",
      " 'Ambanja, Sambirano Valley' 'Ghana Puristique'\n",
      " 'San Juan Estate, Gran Couva' 'Bittersweet' 'Barinas'\n",
      " 'Dominican Republic, batch 7' 'Elvesia P.' 'Guadeloupe'\n",
      " 'Chanchamayo Province' 'Orinoco' 'Hispaniola w/ nibs'\n",
      " 'Camino Verde P., Balao, Guayas, 2013' 'Tres Hombres'\n",
      " 'Kakoa Kamili, Both Man & Bird & Beast' 'Concepcion*'\n",
      " 'Chiapas, Mokaya P.' 'Ecuador, w/ nibs' 'Signature Blend'\n",
      " 'Wild Beni, Lower Rio Beni, Tranquilidad, 2014' 'Noir Infini'\n",
      " 'Sambirano Valley' 'Dual Origins, Sambirano, Elvesia'\n",
      " 'Camino Verde P., Balao, Guayas, \"Fruity\"'\n",
      " 'Carenero Superior, Gran Saman' 'Alto Beni, Cru Savage' 'Bolivian' 'Mara'\n",
      " 'Trinite' 'French Laundry 20th Anniversary'\n",
      " 'Maranon, Good & Evil, w/ nibs' 'La Red de Guanconejo, N. Highlands coop'\n",
      " 'Chocolatey-beta' 'Puerto Plata' 'Kokoa Kamili, batch 1 SRB'\n",
      " 'Cuyagua Village' 'Cuana, 2013' 'Trinidad-Tobago' 'Star of Ecuador'\n",
      " 'Tan Phu Dong, Treasure Island' 'Piaroa, Amazonas, Batch 350'\n",
      " 'Bocas del Toro, Tierra Oscura' 'Ben Tre' 'Honduras' 'Equateur'\n",
      " 'Patanemo' 'Madagascar, w/ shell' 'Machu Pichu' 'Ghana, #211, MR, MC'\n",
      " 'Moxos' 'Dominican Republic, Batch 31616' 'La Dalia'\n",
      " 'Duo- Gran Couva & Camino Verde' 'Solomon Island' 'Brazil Rio Doce'\n",
      " 'Palo Blanco, Chulucanas' 'Africa' 'San Jose del Tambo' 'Selva'\n",
      " 'Tranquilidad, Batch 1' 'Congo' 'Lachua w/ cane sugar' 'Rugoso'\n",
      " 'la Amistad' 'Peruvian' 'Duarte, Batch 360' 'Rio Caribe, Macuro' 'Johe'\n",
      " 'Sur del Lago' 'Lumas, 2015 Harvest, Batch 7' 'Norandino, batch 161208'\n",
      " 'Maleku' 'Xoconusco, cacao Real' 'Palos Blancos' 'Bolivar'\n",
      " 'Puerto Cabello' 'Belize south, low fermentation' 'Cacao Blanco'\n",
      " 'Baracoa, Cuba' 'Asante' 'Ecuador, Batch 31516' 'Huiwani Coop' 'Dark 75'\n",
      " 'Alto Beni, Upper Rio Beni, 2014' 'Tawau, Oct. 2015 Harvest'\n",
      " 'Colombia, Casa Luker' 'Acopagro' 'Gran Couva 2005 P.'\n",
      " 'Ocumare, Premier Cru, Quizas No. 2' 'Semisweet'\n",
      " 'Monte Alegre, 3 diff. plantations' 'Conacado, #213, DR, -C'\n",
      " 'Chuno, San Jose de Bocay, Pantasma R.,B.S.' 'Colombian Dark' 'Onyx'\n",
      " 'Guaniamo' 'Indio Rojo, Xoco' 'Andoa, Grand Cru blend'\n",
      " \"O'ahu, N. Shore, Waialua Estate\" 'Corazon del Ecuador, Calceta beans'\n",
      " 'La Red, Project Reserva, Guaconejo' 'Antigua, Special Reserve'\n",
      " 'Millot Plantation' 'Maragda' 'Matagalpa' 'Chuao, Venezuela'\n",
      " 'Djakarta, Java and Ghana' 'Kendem Lembu, Java' 'Elvesia P., Batch 32'\n",
      " 'Kolumbia' 'Maracaibo, El Vigia' 'Tanzania, batch a1'\n",
      " 'Markham Valley, #221, DR, MC' 'Uba Budo'\n",
      " 'Ecuador, Choc. Garage Exclusive' 'Sambirano Valley, Le 100%'\n",
      " 'Loma Sotavento' 'Hamakua Coast, Kokoleka'\n",
      " 'Zorzal Reserva, 2015 H., Kerchner' 'Sur del Lago Classificado'\n",
      " 'A case of the Xerces Blues, triple roast'\n",
      " 'Akessons Estate, Sambirano, 2013' 'Lacri Blend'\n",
      " 'Caranero, Choc. Garage Exclusive'\n",
      " 'Tien Giang, Black S., batch VIT60420.0' 'Maya Mtn, Batch 18, Heirloom'\n",
      " \"Bachelor's Hall E., St. Thomas Parish\" 'Java, Javablond' 'Latino'\n",
      " 'Organic Dark' 'Trinitario' 'Bahia, Floresta Azul,Good Friends Reserve#3'\n",
      " 'San Martin, Amazonian Highlands' 'Ba Ria' 'Red Mayan, Xoco'\n",
      " 'Piura Select, Cacao Blanc' 'Ocumare, Puerto Cabello, Venezuela'\n",
      " 'Las Islas' 'Moho Valley' 'Los Rios, Hacienda Limon, Heirloom'\n",
      " 'Porcelana, Tabasco, Marfil de Blanco' 'Ecuador, Puristique'\n",
      " \"Akesson's E., Sambirano V.\" 'Rainforest' 'Teyuna' 'Goddess Blend'\n",
      " \"O'ahu, N. Shore, Waialua E., Kakoleka\" \"Sisa's Secret/ original micro\"\n",
      " 'Tenende, Uwate' 'Los Rios, H. Iara, 2012'\n",
      " 'Camino Verde P., 2012, Balao, Guayas' 'Don Homero- Cerecita Valley'\n",
      " 'Tumbes Coop' 'Wild Beni, Lower Rio Beni, Tranquilidad, 2015'\n",
      " 'Madagascar, Grand Cru' 'Colombian w/ nibs' 'Epique, Blend No. 49'\n",
      " 'Kuruba' 'Dong Nai' 'Ecuador, Midnight Dark' 'La Bahia, w/ cane sugar'\n",
      " 'Aranama' 'Misterio' 'Maranon, #230, DR, LC'\n",
      " 'Toledo District, 2015 Harvest' 'Rio Caribe, Cariaco' 'Los Ancones P.'\n",
      " 'Tangara' 'Ba Ria Vung Tau Province'\n",
      " \"O'ahu, N. Shore, Waialua Estate w/ nibs\" 'Patanemo, Epoch, Donaldo'\n",
      " 'Kerala State' 'Bocas del Toro' 'Manhattan' 'Chuno, Xoco'\n",
      " 'Diego/ original micro' 'Morobe' 'Monte Alegre (Itacare), Brazil'\n",
      " 'Cumboto, farmer Jose Lugo' 'Camino Verde' 'Criollo Blend'\n",
      " 'Ghana prototype' 'Maranon, #228, MR, SC' 'Java, Grand Cru' 'Phantom'\n",
      " 'Alto Beni, Wild Harvest, Limited Ed.' 'Bundibugyo District'\n",
      " 'Puerto Cabello, Mantuano' 'El Carmen, batch 1003' 'Rio Tuma'\n",
      " 'Winak, Sumaco' 'Surfin' 'Bahia Superior' 'Bahia Black, batch bra50722.1'\n",
      " 'Chiapas, Lacandon Jungle' 'Nicaliso, Xoco'\n",
      " 'Tien Giang, 2015, batch 10-2-16' 'Claudio Corallo w/ nibs'\n",
      " 'Ocumare 61, Puertomar' 'Camino Verde, Balao, Guayas' 'Kumasi Sambirano'\n",
      " 'Piura, Blanco de Criollo' 'Extra Dark' 'Rio Eni' 'Suchitepequez E.'\n",
      " 'Guadalcanal' 'Aragua, Trincheras' 'Colombian 2008'\n",
      " 'Canoabo, Hacienda San Jose' 'Papaua New Guinea' 'Dak Lak, Batch 2451'\n",
      " 'Cabosse' 'Jamaique' 'India (south)' 'Ceylan' 'Chuao, Med. Roast'\n",
      " 'Bahia, batch a1213' 'Ocumare, Cumboto' 'Los Rios, Rancho Grande 2007'\n",
      " 'Ocumare, Venezuela' 'Solomon Island w/ nibs' 'Tumbes, Norandino'\n",
      " 'Rico Rugoso, Xoco' 'ROIG, 2014' 'Nutty-beta'\n",
      " 'Los Rios, Hacienda Limon, Orecao, 2015' 'Caraibe' 'Grand Cru Ecuador'\n",
      " 'Malgascio' 'Oahu, Winward, #151, Maunawili district' 'Maya Belize'\n",
      " 'Barlovento, Venezuela' 'Porcelana, Maracaibo, Palmira P. 2006'\n",
      " 'Dominican Republic, Batch D2' 'Rio Peripa H.' 'Sao Tome, Batch 151'\n",
      " 'El Salvador' 'Carenero, Guapiles, Ocumare blend' 'Dark, Stone Ground'\n",
      " 'Cordoba' 'LamasdelChanka, San Martin, Oro Verde coop' 'Little Big Man'\n",
      " 'Acul-du-Nord, 2015' 'Peru, Las Pampas P.' 'Tumbes, \"Zarumilla\"'\n",
      " 'Tapanti, light roast' 'Puerto Quito, heirloom' 'Kulili P., 2013'\n",
      " 'black label' 'Dos Rios' 'Mababa' 'Wild Bolivia' 'Garaua'\n",
      " 'Madagascar, lot M0403R' 'Ecuador Puristique' 'The Other One, Grand Cru'\n",
      " 'San Juan de Cheni' 'Perfect Illusion' 'Papua New Guinea, Batch 2'\n",
      " 'Kaori' 'Bayou Blend' 'Costa Rica, Oscuro'\n",
      " 'Mekong Delta, early 2014 Harvest' 'PNG, Voodoo' 'ROIG'\n",
      " 'Sur del Lago, Merida' 'La Red, 2011' 'Crudo' 'Monte Alegre, D. Badero'\n",
      " 'Chuno, triple turned, Xoco' 'TCHOPro 60.5' 'Guanaja'\n",
      " 'Choroni, Finca Torres, 48hr c.' 'Ambanja, Tsara Valley'\n",
      " 'IL100, H. San Jose' 'Punta Galera, cacao Nacional, gold label'\n",
      " 'Belize, med roast' 'Agua Grande' 'Bundibugyo'\n",
      " 'Carenero Superior, Apamate' 'Piura, Perou'\n",
      " 'Porcelana, Tabasco, Finca La Joya' 'Davao, Mt. Talamo foothills'\n",
      " 'Vietnam, Batch 50/100' 'Peru Brutus' 'Porcelana, Batch 5163'\n",
      " 'Millot P., Ambanja' 'Carre Grand Noir' 'Ham Luong'\n",
      " 'San Andres, American style' 'Chuao 100hr' 'La Dalia, Matagalpa'\n",
      " 'Sambirano Valley, #215, MR, MC'\n",
      " 'the lost city, gracias a dias, batch 362' 'Eastern Promises' 'Morogoro'\n",
      " 'Sensations Intense' 'Bahia, Agri-Forestal Plantation, 2010'\n",
      " 'Alto Beni, Wild Harvest, Itenez R., 60hr c.' 'Espada'\n",
      " 'Sylvestre, Oialla' 'Porcelana, S. of Lake Maracaibo' 'Hacienda Victoria'\n",
      " 'Virunga' 'Toledo District, Maya' 'Santander' 'Oko Caribe, batch 1 SRB'\n",
      " 'Bellavista Gran Pajeten, San Martin' 'Tome Acu E., Amazon Rainforest'\n",
      " 'Vale do Juliana, w/ nibs' 'Madagascar, Sassy Bar'\n",
      " 'Oko Caribe, Duarte Province, 2016 H.' 'Tsaranta' 'Chuao 70hr'\n",
      " 'Kilombero Valley' 'Monte Alegre, D. Badaro, Raw, Organic'\n",
      " 'Amazonas Frucht' 'West Africa' 'Belize, 2014 Harvest, Batch 9' 'Libanio'\n",
      " 'Sambirano Valley, Black Science, B-60307.0' 'Caribe' 'Twilight'\n",
      " 'Supremo- SF' 'La Masica, Batch 7, FHIA' \"Hawai'i, Kona Estate Grown\"\n",
      " 'Dominican Republic prototype' 'Bolivar, Arriba'\n",
      " 'Gru Grococo, St. Andrews' 'Maranon, batch 2' 'Conacado, #212, LR, SC'\n",
      " 'Kafupbo, Petit Bourg, De Borgnes' 'Conacado, Manifesto'\n",
      " 'Java, Indonesian Black' 'Excellence (US Version)'\n",
      " 'Porcelana, Tabasco, Mexico' 'Black Science Blend 1' 'Hispaniola, 2013'\n",
      " 'Upala, Batch 18' 'New Ireland' 'Uranga, Lot 22032016' 'Vinces'\n",
      " 'Agua Fria; Sucre region' 'Juliana' 'Sambirano Valley, 2012'\n",
      " 'Chimelb, Lanquin, Alta Verapaz, b-GUA001' 'Arawak' 'Brazil Blend'\n",
      " 'India' 'Tumbes, 2013 Harvest, Batch 8' 'Piura' 'Nigeria'\n",
      " 'Java, Indonesie' 'La Masica, FHIA' 'Los Rios, H. Iara, 2012, 120hr c.'\n",
      " 'Samana' 'Porcelana, Sorotaima,Machiques,batch pcl001'\n",
      " \"Maunawili, O'ahu, Agri Research C., 2014\" 'Guayas'\n",
      " 'Maranon, #229, MR, LC' 'Chuao, Light Roast'\n",
      " 'Patanemo Vil., Carabobo State, Tisano family' 'Kulili Estate' 'Equator'\n",
      " 'Marcial, single Cote, 2012' 'Cooproagro' 'Chuao, #217, DR, MC'\n",
      " 'Peru + nibs' 'Indigena Amazonia, Grand Cru, Quizas'\n",
      " 'Makwale Village, Kyela' '\"heirloom\", Arriba Nacional'\n",
      " 'Dominican Republic-Organic' 'San Martin, Batch 2'\n",
      " 'Tumbes, Dear Mr. Finley, 2014' 'Liberia' 'Madagascar, Nosy Be Isle.'\n",
      " 'Porcelana, Tabasco, La Joya' 'Lago di Como, Blu' 'UNOCACE'\n",
      " 'Saidor Estate, Madang P.' 'Somia Plantation, Akesson, 2012'\n",
      " 'Conacado, #223, MR, SC' 'Dominican' 'Campesino w/ nibs'\n",
      " 'Acarigua, w/ nibs' 'Tarakan' 'Tenor' 'Bahia, Fazenda Venturosa'\n",
      " 'Peruvian Amazon' 'Bali' 'Sambirano, batch 170102' 'Baking'\n",
      " 'Nicaragua, American style' 'Caracas, Venezuela and Ghana' 'Quito'\n",
      " 'Grand Cru Dominican Republic' 'Camino Verde P., Balao, Guayas, \"Floral\"'\n",
      " 'Lachua w/ maple sugar, batch 5' 'Ivory Coast, Batch 56/100' 'Papouasie'\n",
      " 'Special Maker Reserve' 'Serian E., NW Borneo, b. #134/3800'\n",
      " 'Porcelana, Pariguan' 'San Juan Estate, Cherry Blossoms at Night'\n",
      " 'Abstract S. w/ Jamaica nibs,batch abs60323.0' 'Tien Giang, batch 1 SRB'\n",
      " 'Sambirano Valley, #214, LR, MC' \"O'Payo\" 'Madagascar, Ambolikapiky P.'\n",
      " 'Tan Phu Dong Island, Heart of Darkness' 'Villa Andina'\n",
      " 'Fazenda Leolinda' 'Cuyagua, 2013' \"O'payo, Waslala\" 'one hundred'\n",
      " 'Chuao, Dark Roast' 'Moho River' 'Maranon, #227, LR, MC' 'Tome Acu'\n",
      " 'Almendra Blanca, batch 1004' 'Cacao Nib Crunch' 'Arhuacos' 'Ocumare 77'\n",
      " 'Apurimac' 'Maragnam' 'Carenero, Empyrean Sabor' 'Bolivar, Guaranda'\n",
      " 'La Masica, Batch 1, FHIA Research Center' 'Namau Village'\n",
      " 'Bahia, Scavina' 'Zorzal Reserva' 'Dominican Republic, rustic'\n",
      " 'single estate' 'Alto Beni, Palos Blanco' 'Rugoso, Bad Fermentation'\n",
      " 'Cahabon' 'Sambirano, Ampamakia 2005, Millot P.' 'La Dorado, light roast'\n",
      " 'Palos Blancos + nibs' 'Birmanie' 'Noula Coop' 'Lanquin Estate'\n",
      " 'Los Rios, Vinces' 'Peru, Awagum bar' 'Spring, Secondary Harvest, 2012'\n",
      " 'Porcelana, Tabasco, Limited Ed.' 'Venezuela; Barinos, Merida, Tachron'\n",
      " 'Guasare, La Sierra de Perija, batch gua001'\n",
      " 'Chiapas, Lacandon Jungle, Oaxacom Mtn' 'Nicaraqua' 'Vanuatu, batch 2410'\n",
      " 'La Patriota, cacao Indio, purple label' 'Jamaica, #204, DR, SC'\n",
      " 'Pablino' 'Trinatario Treasure'\n",
      " 'Chocoan Rainforest, Teroro Escondido, ESM'\n",
      " 'Camino Verde P., Balao, 2015 harvest, batch8' 'Catongo' 'Chucuri'\n",
      " 'Sambirano, 2008' 'Tumbes, Batch 2' 'Chulucanas, El Platanal'\n",
      " 'Palo Blanco w/ panela, Chulucanas' 'Xocunusco, Chiapas, Pichucalco'\n",
      " 'Blue Mountain' 'Grenada, Black Science'\n",
      " 'Bahia Brazil, Fazenda Sao Pedro' 'Trincheras' 'Mindo' 'Vila Gracinda'\n",
      " 'Rugoso, Xoco' 'Wasatch' 'Buto' 'Kilombero, batch 41'\n",
      " 'Chuao, Mantuano blend' 'Gran Blanco' 'Dominican Republicm, rustic'\n",
      " 'Ambolikapiky' 'Elvesia, 2011' 'Fazenda Sempre Firme P., Bahia' 'Pisa'\n",
      " 'Markham Valley, #222, LR, 0C' 'Los Rios, Hacienda Limon, Orecao, 2014'\n",
      " 'Nicaragua, w/ inbs' 'Lumas, 2015 Harvest, Batch 6, brown sugar'\n",
      " 'Kongo, Highlands' 'Dark 67' 'Pinchincha, Mindo, Coop Nueva Esper., 2015'\n",
      " 'Chuno' 'Brooklyn Blend' 'Ghana, prototype' 'Beniamo'\n",
      " 'Alto Beni, Wild Harvest, Itenez R. 24hr c.'\n",
      " 'Namau Village, N. Taileva P., batch a2812' 'Guasare'\n",
      " 'Wild Beniano, 2016, batch 128, Heirloom'\n",
      " 'Alto Beni, Upper Rio Beni, 2015' 'CSB Chama' 'Guapiles'\n",
      " 'Dominican Republic, \"Love Bar\"' 'Piura, Choc. Garage Exclusive'\n",
      " 'Tranquilidad, Baures' 'Maya Mtn, Moho R., Toledo D.'\n",
      " 'Los Colorados, Santo Domingo, Equateur' 'Montubia' 'Maranura'\n",
      " 'Silvestre, Batch 7, 2013' 'Morropon, Norandiono Coop, Piura'\n",
      " 'Madagascar, Batch 8' 'Nine' 'Indianer, Raw' 'Hacienda Las Trincheras'\n",
      " 'Pepiniere, single Cote']\n",
      "\n",
      "\n",
      "Unique values for the column: REF\n",
      "[ 129 1442  647 1538 1888  292 1684  404 1042 1676  387  572  565  919\n",
      "  959 1728 1780  537  642 1450   40  558  486  256 1912 1550  661  987\n",
      " 1796 1852 1219  147  829 1618  757  951  196  170  672 1654  272  355\n",
      " 1085 1137 1692 1097  431 1149  316  995  915  370  395 1458 1880 1209\n",
      " 1422  184 1359 1089 1566 1820 1760 1343 1574 1411 1534    5  805 1646\n",
      " 1856 1231 1367 1363 1680 1542  245 1700 1034  141 1462 1323  666 1251\n",
      " 1486  552 1189 1454 1399 1243 1133 1015 1630   32 1494 1371  464 1287\n",
      " 1514  975 1704 1816 1279  705 1900 1311 1387 1610  523 1069   93 1205\n",
      "  983  879   48 1498 1347 1836  841  586 1578  615 1518  813 1299  899\n",
      "  903 1848  895 1093  761 1586 1117  135  781  688 1395 1800  825 1375\n",
      " 1840  296  713  597 1263 1916 1490 1339 1239  676 1736  192  593 1748\n",
      "  654 1121  773  263  845  999 1932  439 1768 1884  947  502 1936  963\n",
      "  188 1732 1169  935 1391 1904 1582  123 1113  341  765  629  237  117\n",
      " 1335 1756  837 1331 1061 1662 1291 1438 1872 1864 1824  230  377 1185\n",
      "  717 1626  785 1642  801   24 1415 1181  423 1315  725 1125 1860 1247\n",
      " 1215 1638  793 1752 1590 1474  741  943   75 1267  875  623  206 1920\n",
      "  991 1053  745 1708  308 1944  733 1383  248  701  153 1145 1792   81\n",
      "  213 1227  259 1724 1026 1073  508 1948  600  276 1193 1057  252 1201\n",
      " 1928 1049  288  682  363  331 1235  457  199  721 1466 1307 1522 1478\n",
      "  709  955  737 1223  777  531 1896  607  867  346  304 1506 1908 1940\n",
      " 1804  859  266  931 1355  887  749  166 1530 1161  227 1868 1772 1924\n",
      "  871   63 1430  478 1434 1668 1666 1688 1716 1784 1832 1562   56 1157\n",
      "  853  269 1672  636 1030  241 1658  729 1177 1634 1526 1351 1594   15\n",
      "  544 1019  224  111  445 1295  971 1129 1327  516 1038  753 1570  769\n",
      " 1319  979  451 1602  833 1482 1446  414 1720  327 1141 1808 1403  697\n",
      " 1379 1271 1259  883  923  175 1614 1510 1303 1776  576 1844 1892  284\n",
      " 1470 1598  849 1622 1558  336 1696   99 1502 1255 1828 1065 1153 1740\n",
      "  311 1546  939 1077 1606  821  202  891 1554   87  180 1952  322 1197\n",
      "  789 1275  162  967 1744  233  105 1007  927  797 1426 1712 1418  579\n",
      " 1105  470  693 1650 1788  300 1011  817  855  809 1109 1876 1173  863\n",
      " 1022  907  772 1165 1283  157  220 1081  494 1764  280 1211  911 1101\n",
      "  641 1046 1407 1812  209]\n",
      "\n",
      "\n",
      "Unique values for the column: review_date\n",
      "[2007 2014 2011 2015 2016 2008 2009 2013 2010 2012 2006 2017]\n",
      "\n",
      "\n",
      "Unique values for the column: cocoa_percent\n",
      "['64%' '80%' '70%' '65%' '68%' '73%' '72%' '74%' '60%' '75%' '55%' '67%'\n",
      " '63%' '99%' '77%' '72.50%' '62%' '85%' '78%' '82%' '53%' '66%' '88%'\n",
      " '100%' '83%' '69%' '71%' '76%' '61%' '57%' '58%' '90%' '91%' '86%' '84%'\n",
      " '42%' '46%' '89%' '73.50%' '60.50%' '81%' '79%' '56%' '50%' '87%']\n",
      "\n",
      "\n",
      "Unique values for the column: company_location\n",
      "['France' 'Switzerland' 'U.S.A.' 'Colombia' 'Argentina' 'Japan' 'U.K.'\n",
      " 'Canada' 'Scotland' 'Germany' 'Ecuador' 'Guatemala' 'Mexico' 'Nicaragua'\n",
      " 'Sweden' 'Belgium' 'Italy' 'St. Lucia' 'Brazil' 'New Zealand' 'Australia'\n",
      " 'Denmark' 'Lithuania' 'Spain' 'Fiji' 'Austria' 'Madagascar' 'Costa Rica'\n",
      " 'Hungary' 'Portugal' 'Vietnam' 'Peru' 'Ireland' 'Venezuela' 'Israel'\n",
      " 'Chile' 'Domincan Republic' 'Netherlands' 'Bolivia' 'Puerto Rico'\n",
      " 'Suriname' 'Iceland' 'South Africa' 'Grenada' 'Martinique' 'Amsterdam'\n",
      " 'Czech Republic' 'Honduras' 'Poland' 'South Korea' 'Russia' 'India'\n",
      " 'Ghana' 'Sao Tome' 'Philippines' 'Singapore' 'Wales']\n",
      "\n",
      "\n",
      "Unique values for the column: rating\n",
      "[4.   3.25 3.5  3.   2.75 3.75 2.5  2.   2.25 5.   1.5  1.   1.75]\n",
      "\n",
      "\n",
      "Unique values for the column: bean_typ\n",
      "['Criollo, Trinitario' 'Blend' 'Matina' '\\xc2\\xa0' 'CCN51'\n",
      " 'Forastero (Arriba) ASSS' 'Trinitario' 'Criollo' 'Amazon mix' 'Nacional'\n",
      " 'Forastero (Arriba)' 'Forastero (Nacional)' 'Forastero (Arriba) ASS'\n",
      " 'Forastero' 'Criollo (Ocumare 61)' nan 'Trinitario, Criollo'\n",
      " 'Trinitario (85% Criollo)' 'Criollo (Amarru)' 'Beniano'\n",
      " 'Forastero (Parazinho)' 'Criollo (Porcelana)' 'Criollo (Ocumare 67)'\n",
      " 'Criollo (Wild)' 'Nacional (Arriba)' 'Criollo, Forastero' 'Amazon, ICS'\n",
      " 'Blend-Forastero,Criollo' 'Trinitario (Amelonado)' 'EET'\n",
      " 'Trinitario, TCGA' 'Forastero, Trinitario' 'Trinitario, Nacional'\n",
      " 'Trinitario, Forastero' 'Trinitario (Scavina)' 'Criollo (Ocumare 77)'\n",
      " 'Forastero (Catongo)']\n",
      "\n",
      "\n",
      "Unique values for the column: country_origin\n",
      "['Madagascar' '\\xc2\\xa0' 'Costa Rica' 'Nicaragua' 'Colombia' 'Ecuador'\n",
      " 'Trinidad' 'Venezuela' 'Dominican Republic' 'Peru'\n",
      " 'Peru, Mad., Dom. Rep.' 'Domincan Republic' 'Jamaica' 'Vietnam' 'Mexico'\n",
      " 'Honduras' 'Guatemala' 'Sri Lanka' 'Indonesia' 'Puerto Rico' 'St. Lucia'\n",
      " 'Hawaii' 'Peru, Ecuador' 'Ghana' 'Brazil' 'Ivory Coast' 'Uganda' 'Samoa'\n",
      " 'Tanzania' 'Papua New Guinea' 'Togo' 'Bolivia' 'Panama' 'Belize'\n",
      " 'Vanuatu' 'Fiji' 'Peru, Madagascar' 'Central and S. America' 'Tobago'\n",
      " 'Sao Tome' 'Trinidad, Tobago' 'Cuba' 'Ven, Bolivia, D.R.'\n",
      " 'Sao Tome & Principe' 'Carribean' 'Venezuela, Dom. Rep.'\n",
      " 'Dominican Rep., Bali' 'Ghana, Panama, Ecuador' 'Grenada' 'Principe'\n",
      " 'Venezuela, Trinidad' 'Australia' 'South America' 'Colombia, Ecuador'\n",
      " 'Ecuador, Costa Rica' 'South America, Africa' 'Malaysia' 'Congo'\n",
      " 'PNG, Vanuatu, Mad' 'Gre., PNG, Haw., Haiti, Mad' 'Haiti' 'Philippines'\n",
      " 'Ven., Trinidad, Mad.' 'Suriname' 'Peru, Ecuador, Venezuela' 'Martinique'\n",
      " 'Dom. Rep., Madagascar' 'Peru, Belize' 'Africa, Carribean, C. Am.'\n",
      " 'Trinidad-Tobago' 'Trinidad, Ecuador' 'Solomon Islands'\n",
      " 'Ghana, Domin. Rep' 'Indonesia, Ghana' nan 'Carribean(DR/Jam/Tri)'\n",
      " 'India' 'Venez,Africa,Brasil,Peru,Mex' 'Ghana & Madagascar'\n",
      " 'Venezuela, Java' 'El Salvador' 'Cost Rica, Ven' 'Madagascar & Ecuador'\n",
      " 'Ecuador, Mad., PNG' 'Mad., Java, PNG' 'West Africa' 'DR, Ecuador, Peru'\n",
      " 'Nigeria' 'Peru, Dom. Rep' 'Liberia' 'Venezuela, Ghana'\n",
      " 'Ven.,Ecu.,Peru,Nic.' 'Burma' 'Ven, Trinidad, Ecuador']\n",
      "\n",
      "\n",
      "The total number of null values in : company\n",
      "0\n",
      "The total number of null values in : bean_origin\n",
      "0\n",
      "The total number of null values in : REF\n",
      "0\n",
      "The total number of null values in : review_date\n",
      "0\n",
      "The total number of null values in : cocoa_percent\n",
      "0\n",
      "The total number of null values in : company_location\n",
      "0\n",
      "The total number of null values in : rating\n",
      "0\n",
      "The total number of null values in : bean_typ\n",
      "1\n",
      "The total number of null values in : country_origin\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of null values for each column in a dataset\n",
    "def count_null_values(dataset, column_list):\n",
    "    for i in range (len(column_list)):\n",
    "        print \"The total number of null values in :\",column_list[i]\n",
    "        print dataset[column_list[i]].isnull().sum()\n",
    "    return\n",
    "\n",
    "# Function to print the unique values of a dataset\n",
    "def print_uniques(dataset, column_list):\n",
    "    for i in range (len(column_list)):\n",
    "        print \"Unique values for the column:\",column_list[i]\n",
    "        print dataset[column_list[i]].unique()\n",
    "        print '\\n'\n",
    "    return\n",
    "\n",
    "# Printing the unique values for each feature in the dataset\n",
    "print_uniques(data_clean, data_clean.columns)\n",
    "# Printing the null values for each feature in the dataset\n",
    "count_null_values(data_clean, data_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     company                                 bean_origin  \\\n",
      "0                   Valrhona                                     Manjari   \n",
      "1  Original Beans (Felchlin)  Grand Cru Blend No.1, 5 yr. Anniversary Ed   \n",
      "2                    Potomac                               Upala w/ nibs   \n",
      "3                 Middlebury                   Matagalpa, Cacao Bisiesto   \n",
      "\n",
      "    REF  review_date cocoa_percent company_location  rating  \\\n",
      "0   129         2007           64%           France    4.00   \n",
      "1  1442         2014           80%      Switzerland    3.25   \n",
      "2   647         2011           70%           U.S.A.    3.50   \n",
      "3  1538         2015           70%           U.S.A.    3.50   \n",
      "\n",
      "              bean_typ country_origin   company_coffee      maker  \n",
      "0  Criollo, Trinitario     Madagascar         Valrhona        NaN  \n",
      "1                Blend                 Original Beans   Felchlin)  \n",
      "2               Matina     Costa Rica          Potomac        NaN  \n",
      "3                           Nicaragua       Middlebury        NaN  \n",
      "                     company                                 bean_origin  \\\n",
      "0                   Valrhona                                     Manjari   \n",
      "1  Original Beans (Felchlin)  Grand Cru Blend No.1, 5 yr. Anniversary Ed   \n",
      "2                    Potomac                               Upala w/ nibs   \n",
      "3                 Middlebury                   Matagalpa, Cacao Bisiesto   \n",
      "4          Carlotta Chocolat                                       Cesar   \n",
      "\n",
      "    REF  review_date cocoa_percent company_location  rating  \\\n",
      "0   129         2007           64%           France    4.00   \n",
      "1  1442         2014           80%      Switzerland    3.25   \n",
      "2   647         2011           70%           U.S.A.    3.50   \n",
      "3  1538         2015           70%           U.S.A.    3.50   \n",
      "4  1888         2016           65%         Colombia    3.50   \n",
      "\n",
      "              bean_typ country_origin     company_coffee      maker  \n",
      "0  Criollo, Trinitario     Madagascar           Valrhona    Unknown  \n",
      "1                Blend                   Original Beans   Felchlin)  \n",
      "2               Matina     Costa Rica            Potomac    Unknown  \n",
      "3                           Nicaragua         Middlebury    Unknown  \n",
      "4                CCN51       Colombia  Carlotta Chocolat    Unknown  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 11 columns):\n",
      "company             1500 non-null object\n",
      "bean_origin         1500 non-null object\n",
      "REF                 1500 non-null int64\n",
      "review_date         1500 non-null int64\n",
      "cocoa_percent       1500 non-null object\n",
      "company_location    1500 non-null object\n",
      "rating              1500 non-null float64\n",
      "bean_typ            1499 non-null object\n",
      "country_origin      1499 non-null object\n",
      "company_coffee      1500 non-null object\n",
      "maker               1500 non-null object\n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 129.0+ KB\n",
      "None\n",
      "                                  bean_origin   REF  review_date  \\\n",
      "0                                     Manjari   129         2007   \n",
      "1  Grand Cru Blend No.1, 5 yr. Anniversary Ed  1442         2014   \n",
      "2                               Upala w/ nibs   647         2011   \n",
      "3                   Matagalpa, Cacao Bisiesto  1538         2015   \n",
      "\n",
      "  cocoa_percent company_location  rating             bean_typ country_origin  \\\n",
      "0           64%           France    4.00  Criollo, Trinitario     Madagascar   \n",
      "1           80%      Switzerland    3.25                Blend                  \n",
      "2           70%           U.S.A.    3.50               Matina     Costa Rica   \n",
      "3           70%           U.S.A.    3.50                           Nicaragua   \n",
      "\n",
      "    company_coffee     maker  \n",
      "0         Valrhona   Unknown  \n",
      "1  Original Beans   Felchlin  \n",
      "2          Potomac   Unknown  \n",
      "3       Middlebury   Unknown  \n"
     ]
    }
   ],
   "source": [
    "# Creating new column named maker \n",
    "data_clean['company_coffee'], data_clean['maker'] = data_clean['company'].str.split('(', 1).str\n",
    "# Replacing the missing values with \"Unknown\"\n",
    "print data_clean.head(4)\n",
    "data_clean['maker'].fillna(value='Unknown', inplace = True) \n",
    "#data_clean[\"maker\"].replace(np.nan, \"Unknown\")\n",
    "print data_clean.head()\n",
    "print data_clean.info()\n",
    "# Removing unwanted character\n",
    "data_clean['maker'] = data_clean['maker'].apply(lambda x: x.split(')')[0])\n",
    "# Dropping the original column\n",
    "data_clean = data_clean.drop('company', 1)\n",
    "\n",
    "print data_clean.head(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  bean_origin   REF  review_date  \\\n",
      "0                                     Manjari   129         2007   \n",
      "1  Grand Cru Blend No.1, 5 yr. Anniversary Ed  1442         2014   \n",
      "2                               Upala w/ nibs   647         2011   \n",
      "3                   Matagalpa, Cacao Bisiesto  1538         2015   \n",
      "4                                       Cesar  1888         2016   \n",
      "5                                  Rio Arriba   292         2008   \n",
      "6                                  Madagascar  1684         2015   \n",
      "7                                        Noir   404         2009   \n",
      "8                                  Gran Couva  1042         2013   \n",
      "9                           Colombia, Batch 9  1676         2015   \n",
      "\n",
      "  cocoa_percent company_location  rating country_origin     company_coffee  \\\n",
      "0           64%           France    4.00     Madagascar           Valrhona   \n",
      "1           80%      Switzerland    3.25                   Original Beans    \n",
      "2           70%           U.S.A.    3.50     Costa Rica            Potomac   \n",
      "3           70%           U.S.A.    3.50      Nicaragua         Middlebury   \n",
      "4           65%         Colombia    3.50       Colombia  Carlotta Chocolat   \n",
      "5           70%        Argentina    3.50        Ecuador            Salgado   \n",
      "6           70%            Japan    3.00     Madagascar        Cacao Store   \n",
      "7           70%           France    2.75                            Kaoka    \n",
      "8           70%           U.S.A.    3.50       Trinidad          Woodblock   \n",
      "9           80%             U.K.    2.75       Colombia           Dormouse   \n",
      "\n",
      "      maker                bean_type sub_bean_type  \n",
      "0   Unknown                  criollo    trinitario  \n",
      "1  Felchlin                    blend       unknown  \n",
      "2   Unknown                   matina       unknown  \n",
      "3   Unknown                                unknown  \n",
      "4   Unknown                    ccn51       unknown  \n",
      "5   Unknown  forastero (arriba) asss       unknown  \n",
      "6   Unknown               trinitario       unknown  \n",
      "7     Cemoi                                unknown  \n",
      "8   Unknown               trinitario       unknown  \n",
      "9   Unknown                  criollo    trinitario  \n",
      "['criollo' 'blend' 'matina' '\\xc2\\xa0' 'ccn51' 'forastero (arriba) asss'\n",
      " 'trinitario' 'amazon mix' 'nacional' 'forastero (arriba)'\n",
      " 'forastero (nacional)' 'forastero (arriba) ass' 'forastero'\n",
      " 'criollo (ocumare 61)' nan 'trinitario (85% criollo)' 'criollo (amarru)'\n",
      " 'beniano' 'forastero (parazinho)' 'criollo (porcelana)'\n",
      " 'criollo (ocumare 67)' 'criollo (wild)' 'nacional (arriba)' 'amazon'\n",
      " 'blend-forastero' 'trinitario (amelonado)' 'eet' 'trinitario (scavina)'\n",
      " 'criollo (ocumare 77)' 'forastero (catongo)']\n"
     ]
    }
   ],
   "source": [
    "# Converting the string values to lower case\n",
    "data_clean['bean_typ'] = data_clean['bean_typ'].str.lower()\n",
    "\n",
    "# Creating new column named sub_bean_type \n",
    "data_clean['bean_type'], data_clean['sub_bean_type'] = data_clean['bean_typ'].str.split(',', 1).str\n",
    "# Replacing the missing values with \"Unknown\"\n",
    "data_clean[\"sub_bean_type\"].fillna(\"unknown\", inplace = True) \n",
    "# Removing unwanted character\n",
    "data_clean['sub_bean_type'] = data_clean['sub_bean_type'].apply(lambda x: x.split(')')[0])\n",
    "# Dropping the original column\n",
    "data_clean = data_clean.drop('bean_typ', 1)\n",
    "\n",
    "print data_clean.head(10)\n",
    "print data_clean['bean_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['criollo' 'blend' 'matina' '\\xc2\\xa0' 'ccn51' 'forastero (arriba)'\n",
      " 'trinitario' 'amazon mix' 'nacional' 'forastero (nacional)' 'forastero'\n",
      " 'criollo (ocumare 61)' nan 'trinitario (85% criollo)' 'criollo (amarru)'\n",
      " 'beniano' 'forastero (parazinho)' 'criollo (porcelana)'\n",
      " 'criollo (ocumare 67)' 'criollo (wild)' 'nacional (arriba)' 'amazon'\n",
      " 'blend-forastero' 'trinitario (amelonado)' 'eet' 'trinitario (scavina)'\n",
      " 'criollo (ocumare 77)' 'forastero (catongo)']\n"
     ]
    }
   ],
   "source": [
    "# Some data cleaning regarding bean type name\n",
    "data_clean['bean_type'] = data_clean['bean_type'].replace('forastero (arriba) asss', 'forastero (arriba)')\n",
    "data_clean['bean_type'] = data_clean['bean_type'].replace('forastero (arriba) ass', 'forastero (arriba)')\n",
    "print data_clean['bean_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning country_origin column: \n",
      "['Madagascar' '\\xc2\\xa0' 'Costa Rica' 'Nicaragua' 'Colombia' 'Ecuador'\n",
      " 'Trinidad' 'Venezuela' 'Dominican Republic' 'Peru'\n",
      " 'Peru, Mad., Dom. Rep.' 'Domincan Republic' 'Jamaica' 'Vietnam' 'Mexico'\n",
      " 'Honduras' 'Guatemala' 'Sri Lanka' 'Indonesia' 'Puerto Rico' 'St. Lucia'\n",
      " 'Hawaii' 'Peru, Ecuador' 'Ghana' 'Brazil' 'Ivory Coast' 'Uganda' 'Samoa'\n",
      " 'Tanzania' 'Papua New Guinea' 'Togo' 'Bolivia' 'Panama' 'Belize'\n",
      " 'Vanuatu' 'Fiji' 'Peru, Madagascar' 'Central and S. America' 'Tobago'\n",
      " 'Sao Tome' 'Trinidad, Tobago' 'Cuba' 'Ven, Bolivia, D.R.'\n",
      " 'Sao Tome & Principe' 'Carribean' 'Venezuela, Dom. Rep.'\n",
      " 'Dominican Rep., Bali' 'Ghana, Panama, Ecuador' 'Grenada' 'Principe'\n",
      " 'Venezuela, Trinidad' 'Australia' 'South America' 'Colombia, Ecuador'\n",
      " 'Ecuador, Costa Rica' 'South America, Africa' 'Malaysia' 'Congo'\n",
      " 'PNG, Vanuatu, Mad' 'Gre., PNG, Haw., Haiti, Mad' 'Haiti' 'Philippines'\n",
      " 'Ven., Trinidad, Mad.' 'Suriname' 'Peru, Ecuador, Venezuela' 'Martinique'\n",
      " 'Dom. Rep., Madagascar' 'Peru, Belize' 'Africa, Carribean, C. Am.'\n",
      " 'Trinidad-Tobago' 'Trinidad, Ecuador' 'Solomon Islands'\n",
      " 'Ghana, Domin. Rep' 'Indonesia, Ghana' nan 'Carribean(DR/Jam/Tri)'\n",
      " 'India' 'Venez,Africa,Brasil,Peru,Mex' 'Ghana & Madagascar'\n",
      " 'Venezuela, Java' 'El Salvador' 'Cost Rica, Ven' 'Madagascar & Ecuador'\n",
      " 'Ecuador, Mad., PNG' 'Mad., Java, PNG' 'West Africa' 'DR, Ecuador, Peru'\n",
      " 'Nigeria' 'Peru, Dom. Rep' 'Liberia' 'Venezuela, Ghana'\n",
      " 'Ven.,Ecu.,Peru,Nic.' 'Burma' 'Ven, Trinidad, Ecuador']\n",
      "After Cleaning country_origin column: \n",
      "['Madagascar' '\\xc2\\xa0' 'Costa Rica' 'Nicaragua' 'Colombia' 'Ecuador'\n",
      " 'Trinidad' 'Venezuela' 'Dominican Republic' 'Peru'\n",
      " 'Peru, Madagascar, Dominican Republic' 'Jamaica' 'Vietnam' 'Mexico'\n",
      " 'Honduras' 'Guatemala' 'Sri Lanka' 'Indonesia' 'Puerto Rico' 'St. Lucia'\n",
      " 'Hawaii' 'Peru, Ecuador' 'Ghana' 'Brazil' 'Ivory Coast' 'Uganda' 'Samoa'\n",
      " 'Tanzania' 'Papua New Guinea' 'Togo' 'Bolivia' 'Panama' 'Belize'\n",
      " 'Vanuatu' 'Fiji' 'Peru, Madagascar' 'Central and South America' 'Tobago'\n",
      " 'Sao Tome' 'Trinidad, Tobago' 'Cuba' 'Ven, Bolivia, D.R.'\n",
      " 'Sao Tome & Principe' 'Carribean' 'Venezuela, Dom. Rep.'\n",
      " 'Dominican Rep., Bali' 'Ghana, Panama, Ecuador' 'Grenada' 'Principe'\n",
      " 'Venezuela, Trinidad' 'Australia' 'South America' 'Colombia, Ecuador'\n",
      " 'Ecuador, Costa Rica' 'South America, Africa' 'Malaysia' 'Congo'\n",
      " 'Papua New Guinea, Vanuatu, Madagascar'\n",
      " 'Grenada, Papua New Guinea, Hawaii, Haiti, Madagascar' 'Haiti'\n",
      " 'Philippines' 'Venezuela, Trinidad, Madagascar' 'Suriname'\n",
      " 'Peru, Ecuador, Venezuela' 'Martinique' 'Dom. Rep., Madagascar'\n",
      " 'Peru, Belize' 'Africa, Carribean, C. Am.' 'Trinidad, Ecuador'\n",
      " 'Solomon Islands' 'Ghana, Dominican Republic' 'Indonesia, Ghana' nan\n",
      " 'India' 'Venez,Africa,Brasil,Peru,Mex' 'Ghana & Madagascar'\n",
      " 'Venezuela, Java' 'El Salvador' 'Cost Rica, Ven' 'Madagascar & Ecuador'\n",
      " 'Ecuador, Madagascar, Papua New Guinea'\n",
      " 'Madagascar, Java, Papua New Guinea' 'West Africa' 'DR, Ecuador, Peru'\n",
      " 'Nigeria' 'Peru, Dom. Rep' 'Liberia' 'Venezuela, Ghana'\n",
      " 'Venezuela, Ecuador, Peru, Nicaragua' 'Burma'\n",
      " 'Venezuela, Trinidad, Ecuador']\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning regarding the Broad Bean Origin column\n",
    "print \"Before Cleaning country_origin column: \"\n",
    "print data_clean['country_origin'].unique()\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace('Domincan Republic', 'Dominican Republic')\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace('Carribean(DR/Jam/Tri)', 'Carribean')\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace('Trinidad-Tobago', 'Trinidad, Tobago')\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Peru, Mad., Dom. Rep.\", \"Peru, Madagascar, Dominican Republic\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Central and S. America\", \"Central and South America\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"PNG, Vanuatu, Mad\", \"Papua New Guinea, Vanuatu, Madagascar\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven., Trinidad, Mad.\", \"Venezuela, Trinidad, Madagascar\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven.,Ecu.,Peru,Nic.\", \"Venezuela, Ecuador, Peru, Nicaragua\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Ven, Trinidad, Ecuador\",\"Venezuela, Trinidad, Ecuador\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Ghana, Domin. Rep\", \"Ghana, Dominican Republic\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Ecuador, Mad., PNG\",\"Ecuador, Madagascar, Papua New Guinea\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Mad., Java, PNG\",\"Madagascar, Java, Papua New Guinea\")\n",
    "data_clean['country_origin'] = data_clean['country_origin'].replace(\"Gre., PNG, Haw., Haiti, Mad\", \"Grenada, Papua New Guinea, Hawaii, Haiti, Madagascar\")\n",
    "\n",
    "print \"After Cleaning country_origin column: \"\n",
    "print data_clean['country_origin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manjari' 'grand cru blend no' 'upala w' 'matagalpa' 'cesar' 'rio arriba'\n",
      " 'madagascar' 'noir' 'gran couva' 'colombia' 'ocumare'\n",
      " 'dominican republic' 'sambirano' 'peru' 'shake shack' 'hispaniola'\n",
      " 'coto brus' 'matina ' 'jamaica' 'vietnam' 'colombie' 'los rios' 'arriba'\n",
      " 'matiguas' 'selva maya' 'ecuador' 'cortes' 'los ujuxtes' 'mexico'\n",
      " 'maranon' 'xoconusco' 'mombacho' 'el oro' 'capistrano' 'sri lanka' 'bali'\n",
      " 'cahabon region' 'toscano black' 'blend no' 'manabi' 'ilblend'\n",
      " 'puerto rico' 'emerald estate' 'opaeula estate' 'bellavista coop'\n",
      " 'tabasco' 'criollo' 'nyangbo' 'fazenda camboa' 'san andres' 'porcelana'\n",
      " 'ocumare ' 'ivory coast' 'uganda' 'choobua' 'oko caribe' 'sang yum coop'\n",
      " 'camino verde p' 'kokoa kamili' 'png' 'trinidad' 'upala' 'conacado coop'\n",
      " 'chuao' 'marabel farms' \"kaua'i\" 'akata' 'lachua' 'roberto' 'la red'\n",
      " 'vale do juliana e' 'medagla' 'venezuela' 'bolivia' 'santo domingo'\n",
      " 'maranon canyon' 'papua new guinea' 'corona arriba' 'colombian semi dark'\n",
      " 'panama' 'belize' 'alta verapaz' 'vanuatu' 'quilla' 'perla negra'\n",
      " 'vanua levu' 'papua' 'tien giang' 'san francisco de macoris' 'huila'\n",
      " 'robson estate' 'djual island' 'star of peru' 'brazil' 'san martin'\n",
      " 'ambanja' 'sierra nevada' 'madagascar w' 'tumaco' 'chiapas' ''\n",
      " 'ambolikapiky p' 'midnight' 'nicaragua' 'markham valley' 'la selva'\n",
      " 'roxborough' 'ghana' 'sao tome' 'oro' 'cuyagua' 'costa rica' 'haleiwa e'\n",
      " 'kakao kamili' 'colombian' 'tanzania' 'hilo' 'trinidad & tobago'\n",
      " 'venzuela' 'baracoa' 'somia plantation' 'alto beni' 'three amigos'\n",
      " 'maya mountain' 'madagared' 'java' 'cota brus' 'sao tome & principe'\n",
      " 'lever du soleil' 'carribean' 'cacao nacional w' 'bahia' 'cedeno'\n",
      " 'rio caribe' 'tingo maria' 'purple haze' 'napa' 'ciaab coop' 'tumbes'\n",
      " 'amina' \"akesson's estate\" 'non pariel estate' 'costa esmeraldas'\n",
      " 'grenada' 'house blend' 'conacado' 'principe' 'piura blanco' 'esmeraldas'\n",
      " 'alpaco' 'carenero superior' 'rio dulce' 'papua kerafat' 'belize south'\n",
      " 'australia' 'san juan estate' 'manickchand estate' 'autumn' 'lam dong'\n",
      " 'concepcion' 'maralumi p' 'south america' 'cumbia' 'orinoqua region'\n",
      " 'ambolikapkly p' 'carenero' 'coopertiva amazona' 'jutiapa'\n",
      " 'zorzal reserva w' 'arauca' 'mora mora ' 'pangoa' 'agrocriso plantation'\n",
      " 'barba' 'le noir extra amer' 'kokoa kamili coop' 'rizek cacao' 'macondo'\n",
      " 'satipo region' 'las acacias e' 'houseblend' 'cacao verapaz' 'complexite'\n",
      " 'nibby' 'satipo pangoa region' 'wild bolivian' 'duo' 'classic'\n",
      " 'fazenda sempre firme' 'chuno' 'indonesia' 'johe' 'matasawalevu'\n",
      " 'winak coop' 'duarte province' 'bocas del toro' 'elvesia' 'haleiwa'\n",
      " 'diego ' 'south america and africa' 'gruppo salinas' 'tamarina' 'tobago'\n",
      " 'cusco' 'guyave' 'cuba' 'huallabamba' 'momotombo' 'guantupi river'\n",
      " 'pichincha' 'san juan' 'island growers' 'toledo district' 'nourish'\n",
      " 'asajaya e' 'sangre grande p' 'ankasa' 'sharkey' 'piura' 'elvesia p' 'd'\n",
      " 'loma los pinos' 'oscuro' 'blend' 'anselmo paraiso estate' 'granella'\n",
      " 'ben tre' 'montanya' 'umoho r' 'dancing in your head' 'abocfa coop'\n",
      " 'chuao ' 'maya mtn' 'goodman estate' 'akosombo' 'haiti' 'amazonia'\n",
      " 'la tronca' 'guatemala' 'brazilian' 'samar' 'nocturne' 'raw' 'choroni'\n",
      " 'sambirano valley' \"akesson's\" 'finisterra' 'paramaribo'\n",
      " 'wampusirpi region' 'san joaquin' 'asochivite' 'kendari' 'boyaca'\n",
      " 'trintade' 'hawaiian crown' 'nacional' 'philly blend' \"grand 'anse\"\n",
      " 'mid mountain' 'kpime' 'akessons estate' 'chulucanas'\n",
      " \"jamaica a l'ancienne\" 'san jose' 'amazonas' 'akesson estate' 'mantuano'\n",
      " 'carenero s' 'tokiala' 'tchopro ' 'los llanos' 'balinese' 'guasare'\n",
      " 'dark' 'carupano' 'somia' 'malekula island' 'sisa ' 'maracaibo'\n",
      " 'ghana puristique' 'bittersweet' 'barinas' 'guadeloupe'\n",
      " 'chanchamayo province' 'orinoco' 'hispaniola w' 'tres hombres'\n",
      " 'kakoa kamili' 'signature blend' 'wild beni' 'noir infini' 'dual origins'\n",
      " 'bolivian' 'mara' 'trinite' 'french laundry ' 'la red de guanconejo'\n",
      " 'chocolatey' 'puerto plata' 'cuyagua village' 'cuana' 'star of ecuador'\n",
      " 'tan phu dong' 'piaroa' 'honduras' 'equateur' 'patanemo' 'machu pichu'\n",
      " 'moxos' 'la dalia' 'solomon island' 'brazil rio doce' 'palo blanco'\n",
      " 'africa' 'san jose del tambo' 'selva' 'tranquilidad' 'congo' 'lachua w'\n",
      " 'rugoso' 'la amistad' 'peruvian' 'duarte' 'sur del lago' 'lumas'\n",
      " 'norandino' 'maleku' 'palos blancos' 'bolivar' 'puerto cabello'\n",
      " 'cacao blanco' 'asante' 'huiwani coop' 'dark ' 'tawau' 'acopagro'\n",
      " 'gran couva ' 'semisweet' 'monte alegre' 'colombian dark' 'onyx'\n",
      " 'guaniamo' 'indio rojo' 'andoa' \"o'ahu\" 'corazon del ecuador' 'antigua'\n",
      " 'millot plantation' 'maragda' 'djakarta' 'kendem lembu' 'kolumbia'\n",
      " 'uba budo' 'loma sotavento' 'hamakua coast' 'zorzal reserva'\n",
      " 'sur del lago classificado' 'a case of the xerces blues' 'lacri blend'\n",
      " 'caranero' \"bachelor's hall e\" 'latino' 'organic dark' 'trinitario'\n",
      " 'ba ria' 'red mayan' 'piura select' 'las islas' 'moho valley'\n",
      " \"akesson's e\" 'rainforest' 'teyuna' 'goddess blend' \"sisa's secret\"\n",
      " 'tenende' 'don homero' 'tumbes coop' 'colombian w' 'epique' 'kuruba'\n",
      " 'dong nai' 'la bahia' 'aranama' 'misterio' 'los ancones p' 'tangara'\n",
      " 'ba ria vung tau province' 'kerala state' 'manhattan' 'diego' 'morobe'\n",
      " 'monte alegre ' 'cumboto' 'camino verde' 'criollo blend'\n",
      " 'ghana prototype' 'phantom' 'bundibugyo district' 'el carmen' 'rio tuma'\n",
      " 'winak' 'surfin' 'bahia superior' 'bahia black' 'nicaliso'\n",
      " 'claudio corallo w' 'kumasi sambirano' 'extra dark' 'rio eni'\n",
      " 'suchitepequez e' 'guadalcanal' 'aragua' 'colombian ' 'canoabo'\n",
      " 'papaua new guinea' 'dak lak' 'cabosse' 'jamaique' 'india ' 'ceylan'\n",
      " 'solomon island w' 'rico rugoso' 'roig' 'nutty' 'caraibe'\n",
      " 'grand cru ecuador' 'malgascio' 'oahu' 'maya belize' 'barlovento'\n",
      " 'rio peripa h' 'el salvador' 'cordoba' 'lamasdelchanka' 'little big man'\n",
      " 'acul' 'tapanti' 'puerto quito' 'kulili p' 'black label' 'dos rios'\n",
      " 'mababa' 'wild bolivia' 'garaua' 'ecuador puristique' 'the other one'\n",
      " 'san juan de cheni' 'perfect illusion' 'kaori' 'bayou blend'\n",
      " 'mekong delta' 'crudo' 'guanaja' 'il' 'punta galera' 'agua grande'\n",
      " 'bundibugyo' 'davao' 'peru brutus' 'millot p' 'carre grand noir'\n",
      " 'ham luong' 'the lost city' 'eastern promises' 'morogoro'\n",
      " 'sensations intense' 'espada' 'sylvestre' 'hacienda victoria' 'virunga'\n",
      " 'santander' 'bellavista gran pajeten' 'tome acu e' 'vale do juliana'\n",
      " 'tsaranta' 'kilombero valley' 'amazonas frucht' 'west africa' 'libanio'\n",
      " 'caribe' 'twilight' 'supremo' 'la masica' \"hawai'i\"\n",
      " 'dominican republic prototype' 'gru grococo' 'kafupbo' 'excellence '\n",
      " 'black science blend ' 'new ireland' 'uranga' 'vinces' 'agua fria'\n",
      " 'juliana' 'chimelb' 'arawak' 'brazil blend' 'india' 'nigeria' 'samana'\n",
      " 'maunawili' 'guayas' 'patanemo vil' 'kulili estate' 'equator' 'marcial'\n",
      " 'cooproagro' 'peru ' 'indigena amazonia' 'makwale village' '\"heirloom\"'\n",
      " 'liberia' 'lago di como' 'unocace' 'saidor estate' 'dominican'\n",
      " 'campesino w' 'acarigua' 'tarakan' 'tenor' 'peruvian amazon' 'baking'\n",
      " 'caracas' 'quito' 'grand cru dominican republic' 'papouasie'\n",
      " 'special maker reserve' 'serian e' 'abstract s' \"o'payo\"\n",
      " 'tan phu dong island' 'villa andina' 'fazenda leolinda' 'one hundred'\n",
      " 'moho river' 'tome acu' 'almendra blanca' 'cacao nib crunch' 'arhuacos'\n",
      " 'apurimac' 'maragnam' 'namau village' 'single estate' 'cahabon'\n",
      " 'la dorado' 'palos blancos ' 'birmanie' 'noula coop' 'lanquin estate'\n",
      " 'spring' 'nicaraqua' 'la patriota' 'pablino' 'trinatario treasure'\n",
      " 'chocoan rainforest' 'catongo' 'chucuri' 'palo blanco w' 'xocunusco'\n",
      " 'blue mountain' 'bahia brazil' 'trincheras' 'mindo' 'vila gracinda'\n",
      " 'wasatch' 'buto' 'kilombero' 'gran blanco' 'dominican republicm'\n",
      " 'ambolikapiky' 'fazenda sempre firme p' 'pisa' 'kongo' 'pinchincha'\n",
      " 'brooklyn blend' 'beniamo' 'wild beniano' 'csb chama' 'guapiles'\n",
      " 'los colorados' 'montubia' 'maranura' 'silvestre' 'morropon' 'nine'\n",
      " 'indianer' 'hacienda las trincheras' 'pepiniere']\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning the bean origin column\n",
    "\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].str.lower()\n",
    "\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split(',')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('/')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('*')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('.')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('+')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split(';')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('-')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('(')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('#')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('1')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('2')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('3')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('4')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('5')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('6')[0])\n",
    "data_clean['bean_origin'] = data_clean['bean_origin'].apply(lambda x: x.split('7')[0])\n",
    "print data_clean['bean_origin'].unique()\n",
    "#print data_clean['bean_origin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 11 columns):\n",
      "bean_origin         1500 non-null object\n",
      "REF                 1500 non-null int64\n",
      "review_date         1500 non-null int64\n",
      "cocoa_percent       1500 non-null object\n",
      "company_location    1500 non-null object\n",
      "rating              1500 non-null float64\n",
      "country_origin      1499 non-null object\n",
      "company_coffee      1500 non-null object\n",
      "maker               1500 non-null object\n",
      "bean_type           1499 non-null object\n",
      "sub_bean_type       1500 non-null object\n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 129.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 11 columns):\n",
      "bean_origin         1500 non-null object\n",
      "REF                 1500 non-null int64\n",
      "review_date         1500 non-null int64\n",
      "cocoa_percent       1500 non-null float64\n",
      "company_location    1500 non-null object\n",
      "rating              1500 non-null float64\n",
      "country_origin      1499 non-null object\n",
      "company_coffee      1500 non-null object\n",
      "maker               1500 non-null object\n",
      "bean_type           1499 non-null object\n",
      "sub_bean_type       1500 non-null object\n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 129.0+ KB\n",
      "None\n",
      "0    64.0\n",
      "1    80.0\n",
      "2    70.0\n",
      "3    70.0\n",
      "4    65.0\n",
      "Name: cocoa_percent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Converting cocoa_percent to integer\n",
    "\n",
    "print data_clean.info()\n",
    "data_clean['cocoa_percent'] = data_clean['cocoa_percent'].apply(lambda x: x.split('%')[0])\n",
    "data_clean['cocoa_percent'] = pd.to_numeric(data_clean['cocoa_percent'], errors='coerce')\n",
    "print data_clean.info()\n",
    "print data_clean['cocoa_percent'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 11 columns):\n",
      "bean_origin         1500 non-null object\n",
      "REF                 1500 non-null int64\n",
      "review_date         1500 non-null object\n",
      "cocoa_percent       1500 non-null float64\n",
      "company_location    1500 non-null object\n",
      "rating              1500 non-null object\n",
      "country_origin      1499 non-null object\n",
      "company_coffee      1500 non-null object\n",
      "maker               1500 non-null object\n",
      "bean_type           1499 non-null object\n",
      "sub_bean_type       1500 non-null object\n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 129.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Changing the type for review_date from int to object to address it as a classification problem\n",
    "data_clean['review_date'] = data_clean['review_date'].astype(str)\n",
    "data_clean['rating'] = data_clean['rating'].astype(str)\n",
    "print data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of null values in : bean_origin\n",
      "0\n",
      "The total number of null values in : REF\n",
      "0\n",
      "The total number of null values in : review_date\n",
      "0\n",
      "The total number of null values in : cocoa_percent\n",
      "0\n",
      "The total number of null values in : company_location\n",
      "0\n",
      "The total number of null values in : rating\n",
      "0\n",
      "The total number of null values in : country_origin\n",
      "62\n",
      "The total number of null values in : company_coffee\n",
      "0\n",
      "The total number of null values in : maker\n",
      "0\n",
      "The total number of null values in : bean_type\n",
      "751\n",
      "The total number of null values in : sub_bean_type\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Replacing the empty cells with null\n",
    "data_clean = data_clean.replace('\\xc2\\xa0', np.nan)\n",
    "count_null_values(data_clean, data_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          bean_origin       REF review_date  cocoa_percent company_location  \\\n",
      "0             manjari -1.637271        2007      -1.207865           France   \n",
      "1  grand cru blend no  0.730590        2014       1.347112      Switzerland   \n",
      "2             upala w -0.703111        2011      -0.249749           U.S.A.   \n",
      "3           matagalpa  0.903716        2015      -0.249749           U.S.A.   \n",
      "4               cesar  1.534905        2016      -1.048179         Colombia   \n",
      "5          rio arriba -1.343317        2008      -0.249749        Argentina   \n",
      "6          madagascar  1.167012        2015      -0.249749            Japan   \n",
      "7                noir -1.141337        2009      -0.249749           France   \n",
      "8          gran couva  0.009231        2013      -0.249749           U.S.A.   \n",
      "9            colombia  1.152585        2015       1.347112             U.K.   \n",
      "\n",
      "  rating country_origin     company_coffee     maker           bean_type  \\\n",
      "0    4.0     Madagascar           Valrhona   Unknown             criollo   \n",
      "1   3.25            NaN    Original Beans   Felchlin               blend   \n",
      "2    3.5     Costa Rica            Potomac   Unknown              matina   \n",
      "3    3.5      Nicaragua         Middlebury   Unknown                 NaN   \n",
      "4    3.5       Colombia  Carlotta Chocolat   Unknown               ccn51   \n",
      "5    3.5        Ecuador            Salgado   Unknown  forastero (arriba)   \n",
      "6    3.0     Madagascar        Cacao Store   Unknown          trinitario   \n",
      "7   2.75            NaN             Kaoka      Cemoi                 NaN   \n",
      "8    3.5       Trinidad          Woodblock   Unknown          trinitario   \n",
      "9   2.75       Colombia           Dormouse   Unknown             criollo   \n",
      "\n",
      "  sub_bean_type  \n",
      "0    trinitario  \n",
      "1       unknown  \n",
      "2       unknown  \n",
      "3       unknown  \n",
      "4       unknown  \n",
      "5       unknown  \n",
      "6       unknown  \n",
      "7       unknown  \n",
      "8       unknown  \n",
      "9    trinitario  \n",
      "REF              2.097581e-16\n",
      "cocoa_percent    9.536446e-16\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 11 columns):\n",
      "bean_origin         1500 non-null object\n",
      "REF                 1500 non-null float64\n",
      "review_date         1500 non-null object\n",
      "cocoa_percent       1500 non-null float64\n",
      "company_location    1500 non-null object\n",
      "rating              1500 non-null object\n",
      "country_origin      1438 non-null object\n",
      "company_coffee      1500 non-null object\n",
      "maker               1500 non-null object\n",
      "bean_type           749 non-null object\n",
      "sub_bean_type       1500 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 129.0+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikita/opt/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/nikita/opt/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the column with integer type\n",
    "\n",
    "# Data Normalizing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "data_norm = data\n",
    "scaler_z = StandardScaler()\n",
    "# Only the columns with integer and float type values are normalized\n",
    "num_d = data_clean.select_dtypes(exclude=['object'])\n",
    "data_clean[num_d.columns] = scaler_z.fit(num_d).transform(num_d)\n",
    "\n",
    "# Getting information of the dataset after normalization\n",
    "print data_clean.head(10)\n",
    "print data_clean[num_d.columns].mean(axis= 0)\n",
    "print data_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the null values for country_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 10) (1438, 10)\n",
      "(1294, 9) (144, 9)\n",
      "[50 47 17 50 18 12  8 17 50 17 75 12 37 29 39 39  3 17  3 18 48 18 15 48\n",
      " 30  3  2 79  2 13 18 39 79 79 18 17 76 18 44 18  7 17 48 18  8  6 27 23\n",
      " 37  4 17  3 50  3 39 39 79 17  4 18 22 17  2  7 18  3 33 13  9 50 18 23\n",
      "  2 39  8 18 12 32 79  2 29 27  2  7 60  4 22  3 18 18 18 17 17  2 31 50\n",
      " 48  3 17  3 31 18 17  2 39 18 17 13 39  2 18  3 72 39 39  7 79 39 31 50\n",
      " 39 17 31  3  3 18 18 45 39 33 17 17 39 17  3 39 23 48 29  3 50 34 39 39]\n",
      "144\n",
      "Model performace for replacing the missing values: \n",
      "Accuracy:  0.3472222222222222\n",
      "Precision:  0.3472222222222222\n",
      "Recall:  0.3472222222222222\n",
      "F1 Score:  0.3472222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating a temp dataset\n",
    "temp_data_co = data_clean\n",
    "# dropping the bean_type column since it has missing values\n",
    "temp_data_co= temp_data_co.drop('bean_type', 1)\n",
    "\n",
    "# Splitting the dataset into null and not null dataframe\n",
    "test_data_co = temp_data_co[temp_data_co[\"country_origin\"].isnull()]\n",
    "train_data_co = temp_data_co[temp_data_co[\"country_origin\"].notnull()]\n",
    "\n",
    "# Label encoding only the categorical columns \n",
    "test_data_co_l = test_data_co.apply(LabelEncoder().fit_transform)\n",
    "test_data_co_l['REF'] = data_clean['REF']\n",
    "test_data_co_l['cocoa_percent'] = data_clean['cocoa_percent']\n",
    "\n",
    "train_data_co_l = train_data_co.apply(LabelEncoder().fit_transform)\n",
    "train_data_co_l ['REF'] = data_clean['REF']\n",
    "train_data_co_l['cocoa_percent'] = data_clean['cocoa_percent']\n",
    "\n",
    "# Defining the X and y \n",
    "X = train_data_co_l.drop('country_origin', axis=1).values\n",
    "y = train_data_co_l['country_origin'].values\n",
    "\n",
    "print test_data_co.shape, train_data_co.shape\n",
    "\n",
    "X_train_co, X_test_co, y_train_co, y_test_co = train_test_split(X, y, test_size= 0.1, train_size=0.9, random_state=42)\n",
    "\n",
    "print X_train_co.shape, X_test_co.shape\n",
    "\n",
    "# Training a KNN machine leanring model to replace the missing values\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# The model gave the best result at n_neighbors = 3\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train_co, y_train_co)\n",
    "y_pred = knn.predict(X_test_co)\n",
    "print y_pred\n",
    "print y_pred.size\n",
    "\n",
    "# Getting the accuracy metric\n",
    "acc = accuracy_score(y_pred, y_test_co)\n",
    "pre = precision_score(y_pred, y_test_co, average='micro')\n",
    "rec = recall_score(y_pred, y_test_co, average='micro')\n",
    "f1 = f1_score(y_pred, y_test_co, average='micro')\n",
    "\n",
    "print 'Model performace for replacing the missing values: '\n",
    "print 'Accuracy: ', acc\n",
    "print 'Precision: ', pre\n",
    "print 'Recall: ',rec\n",
    "print 'F1 Score: ', f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62,)\n",
      "[4 2 2 4 4 2 8 2 8 4 2 2 4 4 2 4 2 2 2 2 4 2 4 4 8 2 2 2 2 8 2 2 4 4 2 2 8\n",
      " 4 4 8 2 4 2 4 2 2 8 2 4 2 4 2 2 2 2 4 4 2 4 2 2 8]\n",
      "    bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "1            16  0.730590            8       1.347112                 8   \n",
      "7            33 -1.141337            3      -0.249749                 4   \n",
      "39           48 -1.563332            1      -1.367552                 5   \n",
      "40            5 -0.658026            5      -0.249749                 4   \n",
      "42           19 -1.379385            2      -0.249749                 5   \n",
      "\n",
      "    rating  country_origin  company_coffee  maker  sub_bean_type  \n",
      "1        6               4              23      3              0  \n",
      "7        4               2              15      2              0  \n",
      "39       7               2               0      6              0  \n",
      "40       7               4               4      6              0  \n",
      "42       8               4              10      6              0  \n",
      "   bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "0          308 -1.637271            1      -1.207865                16   \n",
      "2          518 -0.703111            5      -0.249749                53   \n",
      "3          320  0.903716            9      -0.249749                53   \n",
      "4          104  1.534905           10      -1.048179                 9   \n",
      "5          420 -1.343317            2      -0.249749                 1   \n",
      "\n",
      "   rating  country_origin  company_coffee  maker  sub_bean_type  \n",
      "0      11              39             353     30              5  \n",
      "2       9              12             280     30              7  \n",
      "3       9              45             227     30              7  \n",
      "4       9               8              62     30              7  \n",
      "5       9              18             300     30              7  \n",
      "(1500, 10)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the null values for country_origin\n",
    "pred_data_co_l = test_data_co_l.drop('country_origin', axis=1).values\n",
    "\n",
    "# y_pred containing the predicted value\n",
    "y_pred = knn.predict(pred_data_co_l)\n",
    "print y_pred.shape\n",
    "print y_pred\n",
    "\n",
    "# Storing the y_pred values in a column of a dataframe\n",
    "temp1 = test_data_co_l\n",
    "temp1[\"country_origin\"] = y_pred\n",
    "print temp1.head()\n",
    "\n",
    "# Incrporating the result into the main dataset\n",
    "dataset_clean_co = pd.concat([train_data_co_l, temp1], join = 'inner')\n",
    "print dataset_clean_co.head()\n",
    "print dataset_clean_co.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the null values for bean_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "3           211  0.903716            9      -0.249749                41   \n",
      "7           235 -1.141337            3      -0.249749                12   \n",
      "11           99 -0.838366            4      -0.249749                41   \n",
      "15          143  1.246362           10       0.069623                41   \n",
      "17          214 -0.901485            4      -1.048179                41   \n",
      "\n",
      "    rating  company_coffee  maker  bean_type  sub_bean_type  \n",
      "3        9             176     19          0              0  \n",
      "7        6             136      4          0              0  \n",
      "11       8             169     19          0              0  \n",
      "15       7             171     19          0              0  \n",
      "17       7             166     19          0              0  \n",
      "(751, 10) (749, 10)\n",
      "(599, 9) (150, 9)\n",
      "[22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22]\n",
      "150\n",
      "Model Performance: \n",
      "Accuracy:  0.46\n",
      "Precision:  0.46\n",
      "Recall:  0.46\n",
      "F1 Score:  0.46\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset by propping the country_origin column since it had missing values\n",
    "temp_data_bt = data_clean\n",
    "temp_data_bt = temp_data_bt.drop('country_origin', 1)\n",
    "\n",
    "# Splitting the dataset into null and not null dataframe\n",
    "test_data_bt = temp_data_bt[temp_data_bt[\"bean_type\"].isnull()]\n",
    "train_data_bt = temp_data_bt[temp_data_bt[\"bean_type\"].notnull()]\n",
    "\n",
    "# Label encoding only the categorical columns \n",
    "test_data_bt = test_data_bt.apply(LabelEncoder().fit_transform)\n",
    "test_data_bt['REF'] = data_clean['REF']\n",
    "test_data_bt['cocoa_percent'] = data_clean['cocoa_percent']\n",
    "\n",
    "train_data_bt = train_data_bt.apply(LabelEncoder().fit_transform)\n",
    "train_data_bt['cocoa_percent'] = data_clean['cocoa_percent']\n",
    "train_data_bt['REF'] = data_clean['REF']\n",
    "\n",
    "# Defining the X and y \n",
    "X = train_data_bt.drop('bean_type', axis=1).values\n",
    "y = train_data_bt['bean_type'].values\n",
    "print test_data_bt.head()\n",
    "print test_data_bt.shape, train_data_bt.shape\n",
    "\n",
    "X_train_bt, X_test_bt, y_train_bt, y_test_bt = train_test_split(X, y, test_size= 0.2, train_size=0.8, random_state=42)\n",
    "\n",
    "print X_train_bt.shape, X_test_bt.shape\n",
    "\n",
    "# Training a KNN machine leanring model to replace the missing values\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# The model gave the best result at n_neighbors = 80\n",
    "knn = KNeighborsClassifier(n_neighbors = 80)\n",
    "knn.fit(X_train_bt, y_train_bt)\n",
    "y_pred = knn.predict(X_test_bt)\n",
    "print y_pred\n",
    "print y_pred.size\n",
    "\n",
    "# Getting the accuracy metric\n",
    "acc = accuracy_score(y_pred, y_test_bt)\n",
    "pre = precision_score(y_pred, y_test_bt, average='micro')\n",
    "rec = recall_score(y_pred, y_test_bt, average='micro')\n",
    "f1 = f1_score(y_pred, y_test_bt, average='micro')\n",
    "\n",
    "print 'Model Performance: '\n",
    "print 'Accuracy: ', acc\n",
    "print 'Precision: ', pre\n",
    "print 'Recall: ',rec\n",
    "print 'F1 Score: ', f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751,)\n",
      "    bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "3           211  0.903716            9      -0.249749                41   \n",
      "7           235 -1.141337            3      -0.249749                12   \n",
      "11           99 -0.838366            4      -0.249749                41   \n",
      "15          143  1.246362           10       0.069623                41   \n",
      "17          214 -0.901485            4      -1.048179                41   \n",
      "\n",
      "    rating  company_coffee  maker  bean_type  sub_bean_type  \n",
      "3        9             176     19         22              0  \n",
      "7        6             136      4         22              0  \n",
      "11       8             169     19         22              0  \n",
      "15       7             171     19         22              0  \n",
      "17       7             166     19         22              0  \n",
      "   bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "0          170 -1.637271            1      -1.207865                14   \n",
      "1          120  0.730590            8       1.347112                42   \n",
      "2          290 -0.703111            5      -0.249749                44   \n",
      "4           60  1.534905           10      -1.048179                 8   \n",
      "5          227 -1.343317            2      -0.249749                 1   \n",
      "\n",
      "   rating  company_coffee  maker  bean_type  sub_bean_type  \n",
      "0      10             236     24          6              5  \n",
      "1       7             178     11          3              7  \n",
      "2       8             186     24         19              7  \n",
      "4       8              50     24          5              7  \n",
      "5       8             201     24         15              7  \n",
      "(1500, 10)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the null values for bean_type with the KNN model\n",
    "\n",
    "pred_data_bean_l = test_data_bt.drop('bean_type', axis=1).values\n",
    "\n",
    "# y_pred storing the predicted values\n",
    "y_pred = knn.predict(pred_data_bean_l)\n",
    "print y_pred.shape\n",
    "\n",
    "temp = test_data_bt\n",
    "temp[\"bean_type\"] = y_pred\n",
    "print temp.head()\n",
    "\n",
    "# Incrporating the result into the main dataset\n",
    "dataset_clean_bean = pd.concat([train_data_bt, temp], join = 'inner')\n",
    "print dataset_clean_bean.head()\n",
    "print dataset_clean_bean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of null values in : bean_origin\n",
      "0\n",
      "The total number of null values in : REF\n",
      "0\n",
      "The total number of null values in : review_date\n",
      "0\n",
      "The total number of null values in : cocoa_percent\n",
      "0\n",
      "The total number of null values in : company_location\n",
      "0\n",
      "The total number of null values in : rating\n",
      "0\n",
      "The total number of null values in : company_coffee\n",
      "0\n",
      "The total number of null values in : maker\n",
      "0\n",
      "The total number of null values in : bean_type\n",
      "0\n",
      "The total number of null values in : sub_bean_type\n",
      "0\n",
      "The total number of null values in : country_origin\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_clean_label_encoding = dataset_clean_bean\n",
    "data_clean_label_encoding['country_origin'] = dataset_clean_co['country_origin']\n",
    "# Checking the dataset for null values after data processing\n",
    "count_null_values(data_clean_label_encoding, data_clean_label_encoding.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the final dataframe after data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      bean_origin       REF  review_date  cocoa_percent  company_location  \\\n",
      "0             170 -1.637271            1      -1.207865                14   \n",
      "1             120  0.730590            8       1.347112                42   \n",
      "2             290 -0.703111            5      -0.249749                44   \n",
      "4              60  1.534905           10      -1.048179                 8   \n",
      "5             227 -1.343317            2      -0.249749                 1   \n",
      "6             162  1.167012            9      -0.249749                24   \n",
      "8             117  0.009231            7      -0.249749                44   \n",
      "9              68  1.152585            9       1.347112                43   \n",
      "10            201 -1.171994            3      -0.249749                 6   \n",
      "12            239 -0.850990            4      -0.249749                44   \n",
      "13            212 -0.212587            6      -0.569121                36   \n",
      "14            252 -0.140451            6       0.229309                44   \n",
      "16             85  1.340138           10       0.388995                44   \n",
      "18            138 -0.712128            5      -0.249749                44   \n",
      "19            295  0.745017            9      -1.846610                24   \n",
      "20            201 -1.343317            2      -0.249749                 1   \n",
      "21             74 -1.797773            0       0.548681                14   \n",
      "22            158 -0.863613            4       0.069623                15   \n",
      "23             20 -0.993458            4      -2.645040                12   \n",
      "27             82 -0.677863            5      -0.249749                44   \n",
      "29            182  1.368993           10      -0.249749                27   \n",
      "30            173  1.469983           10      -0.249749                44   \n",
      "31            304  0.328432            8      -0.249749                 6   \n",
      "33            105 -1.604810            1      -0.728807                12   \n",
      "35            259  1.047988            9      -0.249749                41   \n",
      "36             30 -0.504737            5       0.069623                 4   \n",
      "37             50 -0.154878            6      -0.249749                 6   \n",
      "38            239 -1.516443            1       0.069623                15   \n",
      "39            274 -1.563332            1      -1.367552                23   \n",
      "42            135 -1.379385            2      -0.249749                23   \n",
      "...           ...       ...          ...            ...               ...   \n",
      "1453           95 -1.045757            3      -0.728807                41   \n",
      "1456          325  1.011920            9      -0.249749                41   \n",
      "1457           49 -0.658026            5       0.069623                41   \n",
      "1459           35  0.946998            9      -0.249749                41   \n",
      "1461           13 -0.548019            5      -0.888493                39   \n",
      "1462          113 -1.141337            3       0.069623                12   \n",
      "1465           99  0.458277            8      -0.249749                41   \n",
      "1468           24  0.716163            8      -0.249749                 7   \n",
      "1470          245  0.393355            8      -0.249749                41   \n",
      "1471          354  1.578187           10       0.708367                41   \n",
      "1475          109  0.357287            8       0.388995                41   \n",
      "1476          131 -1.123303            3      -1.048179                41   \n",
      "1477           99 -0.964604            4       0.548681                41   \n",
      "1478          273 -0.005196            7      -0.728807                41   \n",
      "1479          232  0.838794            9       0.548681                41   \n",
      "1480          338 -0.605728            5       0.069623                 1   \n",
      "1481          216  1.296857           10      -0.249749                 1   \n",
      "1482          195  0.687308            8       0.548681                12   \n",
      "1483          211  0.465491            8      -0.569121                41   \n",
      "1484          226  0.681898            8      -0.249749                11   \n",
      "1487          309  0.079563            7       0.548681                41   \n",
      "1488          245  1.397847           10       0.069623                 6   \n",
      "1490           13 -0.612941            5      -0.569121                 1   \n",
      "1491          128  1.628682           11       0.229309                41   \n",
      "1493          273  1.506051           10      -0.249749                27   \n",
      "1495          105 -0.483097            5      -0.249749                41   \n",
      "1496          151 -0.277509            6      -2.165982                 2   \n",
      "1497           38 -1.229703            3      -1.207865                20   \n",
      "1498          134 -0.800495            4       0.069623                40   \n",
      "1499          264  1.112910            9      -0.249749                40   \n",
      "\n",
      "      rating  company_coffee  maker  bean_type  sub_bean_type  country_origin  \n",
      "0         10             236     24          6              5              39  \n",
      "1          7             178     11          3              7               4  \n",
      "2          8             186     24         19              7              12  \n",
      "4          8              50     24          5              7               8  \n",
      "5          8             201     24         15              7              18  \n",
      "6          6              43     24         22              7              39  \n",
      "8          8             246     24         22              7              72  \n",
      "9          5              85     24          6              5               8  \n",
      "10         9             214     24          6              7              79  \n",
      "12         6              27     24         22              7              39  \n",
      "13         5              60     24          6              7              50  \n",
      "14         8             147     24          3              7              56  \n",
      "16        10             104     24          1              7              12  \n",
      "18         7             102     24         22              7              37  \n",
      "19         7             224     33         22              7              87  \n",
      "20         9              70     21          6              7              79  \n",
      "21         8             187     24         22              7               8  \n",
      "22         4              72     24         20              7              18  \n",
      "23         5              53     23         15              7              18  \n",
      "27         9              19     24         22              7              32  \n",
      "29         5             189     26          6              7              44  \n",
      "30         5             126     24         17              7              50  \n",
      "31         4              61     24          6              7              44  \n",
      "33         8             194     31         15              7              18  \n",
      "35         5             221     24          6              5              66  \n",
      "36        10              24     24         22              7              34  \n",
      "37         8             214     24         22              7              29  \n",
      "38         6              72     24         22              7              39  \n",
      "39         8              14     24          3              7               2  \n",
      "42         9              84     24          3              7               4  \n",
      "...      ...             ...    ...        ...            ...             ...  \n",
      "1453       6             199     19         22              0              25  \n",
      "1456       7             198     19         22              0              69  \n",
      "1457       9             169     19         22              0               4  \n",
      "1459       8             146     19         22              0               3  \n",
      "1461      10             202      9         22              0               3  \n",
      "1462       6              51     19         22              0              18  \n",
      "1465       9             172     19         22              0              17  \n",
      "1468       8              40     19         22              0               8  \n",
      "1470       9             209     19         22              0              17  \n",
      "1471       9             272     19         22              0               3  \n",
      "1475       9              82     19         22              0              17  \n",
      "1476       3              96     19         22              0              12  \n",
      "1477       5              38     19         22              0              17  \n",
      "1478      11             211     19         22              0              50  \n",
      "1479       3             151     19         22              0              45  \n",
      "1480      10             277     19         22              0               3  \n",
      "1481      10             237     19         22              0               2  \n",
      "1482       8              28     19         22              0              17  \n",
      "1483       8             107     19         22              0              45  \n",
      "1484      10             205     19         22              0              18  \n",
      "1487      11             225     19         22              0               3  \n",
      "1488       9             183     19         22              0              17  \n",
      "1490       9             277     19         22              0               3  \n",
      "1491       9             154     19         22              0              63  \n",
      "1493       7             164     19         22              0              50  \n",
      "1495       8              82     19         22              0              18  \n",
      "1496       9             278     19         22              0               8  \n",
      "1497       8             116     19         22              0              85  \n",
      "1498       9             271     19         22              0              79  \n",
      "1499       6             124     19         22              0              67  \n",
      "\n",
      "[1500 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print data_clean_label_encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specify the learning type of the problem. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select three learning algorithms based on the previous task and identify the corresponding\n",
    "hyperparameters if any. There must be at least one hyperparameter (to be optimised in Task\n",
    "5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 10)\n",
      "(1500,)\n",
      "bean_origin         8566.815947\n",
      "REF                    1.000667\n",
      "review_date            8.606795\n",
      "cocoa_percent          1.000667\n",
      "company_location     245.754325\n",
      "company_coffee      6299.633111\n",
      "maker                 21.745082\n",
      "bean_type             35.562615\n",
      "sub_bean_type         11.972526\n",
      "country_origin       656.753295\n",
      "dtype: float64\n",
      "Dataset Size Before Feature Selection \n",
      "(1500, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikita/opt/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size After Feature Selection \n",
      "(1500, 5)\n",
      "Relative Feature importance for each of the Features- \n",
      "[0.15932141 0.16578026 0.07789894 0.1472021  0.09372952 0.14079329\n",
      " 0.03624818 0.03622944 0.01763291 0.12516395]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAEWCAYAAAATnlw4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xu8HfO9//HXW0IiCYkQTijZ1QY/qQiCUtQleqOl5edat7ZSrUu1RbX0UKoN2lLNOUdDCSmlcTtuLZpK3C+JXLZQ6pKW0LpHggbJ5/wx313ja19Wstfaa+3k/Xw81mPPmvnOd96z9uaT78ysGUUEZmZm9p4V6h3AzMys0bg4mpmZZVwczczMMi6OZmZmGRdHMzOzjIujmZlZxsXRzMws4+Jo1oUkzZE0qt45ACRNlvS1KvbX7r5J2lHSYkkLSq8bqrDd8ZJ+3Nl+zMp61juAmXUtSQJUp80/FxEfqtO2WyWpZ0S8W+8c1lg8cjSrE0mHSrpb0jmSXpP0lKRt0/xnJL0g6ZBS+/GSzpd0m6T5kqZIGlJavq2kByXNSz+3LS2bLOkMSXcDbwITgO2BsWkENza1+2Xa9uuSpknavtTHqZJ+L+nStP3ZkkamZROA9YAbUn8nLOFnsYKkEyU9KenltJ2BpeUTJf0j7dsdkoal+aOBA4ETyiNRSSHpo9ln9+M0vaOkZyV9T9I/gIvT/N0lzUi/i3skDS+t/z1Jc9N+PyZplyXZP+t+XBzN6mtrYBawOnA5cAWwJfBR4MsUxatfqf2BwOnAGsAM4DKAVEhuAs5Lff0CuEnS6qV1DwJGA6sAhwJ3AkdFRL+IOCq1eRAYAQxMeSZK6l3q4wsp4wDgemAsQEQcBPwd+Hzq76wl/ByOAfYEPgmsDbwK/Fdp+R+AocCawEMt+x0R49L0WWm7n69we/+R9nEIMFrS5sBFwNcpPr9fA9dL6iVpQ+AoYMuIWAX4NDBnCffPuhkXR7P6ejoiLo6IRcCVwLrAaRGxMCJuBd6mKJQtboqIOyJiIXASsI2kdYHdgL9GxISIeDcifgf8BSgXi/ERMTstf6e1MBHx24h4ObX5OdAL2LDU5K6IuDnlnQBsuoT7u3YambW89knzvw6cFBHPpn07FdhbUs+U66KImF9atqmk/ku47bLFwCnpc34LOBz4dUTcHxGLIuISYCHwcWARxeewsaQVI2JORDzZiW1bN+DiaFZf/yxNvwUQEfm88sjxmZaJiFgAvEIx0lob+FvW99+AdVpbty2Svivp0XT48jWgP8UotcU/StNvAr1bCliFnouIAaXX79P8IcC1LUUTeJSiKK0lqYekMemQ6+u8N2pb44PdV+zFiPhX6f0Q4Lvlwk3xD5W1I+IJ4FiKovyCpCskrd2JbVs34OJo1r2s2zKRDrcOBJ5LryFZ2/WAuaX3+SN43vc+nV/8HrAPsFpEDADmUfnFO515xM8zwGezwtk7IuYCBwB7AKMoinVTS+R2tvsm0Kf0/j86yPoMcEa2/T5pBE5EXB4R21F8xgGcuXS7ad2Fi6NZ9/I5SdtJWoni3OP9EfEMcDOwgaQDJPWUtC+wMXBjO339E1i/9H4V4F3gRaCnpP8EVl2CbHl/S+J84IyWC4wkDZK0RynXQuBlioL3kwq2OwM4II06P0NxLrM9FwBHSNpahb6SdpO0iqQNJe0sqRfwL4rR/KKl3E/rJlwczbqXy4FTKA6nbkFxgQ4R8TKwO/BdiiJyArB7RLzUTl+/pDiv96qk84BbKC58eZzikOy/qOBQbMlPgZPTYcnjlmiviizXA7dKmg/cR3GxEsClKc9c4JG0rOw3FOcDX5N0XZr3LYrzra9RfEbX0Y6ImEpx3nEsxcVAT1BctATF+cYxwEsUh5XXBH6whPtn3Yz8sGOz7kHSeODZiDi53lnMlnUeOZqZmWVcHM3MzDI+rGpmZpbxyNHMzCzjG493U2ussUY0NTXVO4aZWbcybdq0lyJiUEftXBy7qaamJqZOnVrvGGZm3Yqk/E5SrfJhVTMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMi6OZmZmGRdHMzOzjO+Q0001z51H04k31TuGmS2lOWN2q3cEa4dHjmZmZhkXRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMst8cZR0rKQ+XbzNCyVt3EGbIyQd3FWZzMyscsvDHXKOBX4LvJkvkNQjIhZVc2Opz6911C4izq/mds3MrHoaYuQo6WBJsyTNlDRB0hBJk9K8SZLWS+3GS9q7tN6C9HNHSZMlXSXpL5IuU+EYYG3gdkm3t6wj6TRJ9wMnS7q21N+ukq5pJ+f+kpolPSzpzHKOUp/bpCwj07KvSno8zbtA0tg0/1RJx6XpyZLOlPRAart9G9sfLWmqpKmL3py3tB+3mZl1oO7FUdIw4CRg54jYFPgWMBa4NCKGA5cB51XQ1WYUo8SNgfWBT0TEecBzwE4RsVNq1xd4OCK2Bk4D/p+kQWnZYcDFbeRcGzgT2BkYAWwpac+8z4i4K1vnh8DHgV2BjdrJ3zMitkr7cEprDSJiXESMjIiRPfr0b6crMzPrjLoXR4pic1VEvAQQEa8A2wCXp+UTgO0q6OeBiHg2IhYDM4CmNtotAq5O24rU/5clDUjb/UMb620JTI6IFyPiXYqivUPeZ2YrYEpEvBIR7wAT28nfMmKd1k52MzPrAo1wzlFAdNCmZfm7pIIuScBKpTYLS9OLaHvf/pWdZ7wYuAH4FzAxFb62crYl77OSdXIt+dvLbmZmXaARRo6TgH0krQ4gaSBwD7BfWn4g0HKocg6wRZreA1ixgv7nA6u0tTAinqM49HoyML6dfu4HPilpDUk9gP2BKR1s+4G0zmqSegJ7VZDXzMzqrO4jlIiYLekMYIqkRcB04BjgIknHAy9SnAsEuAD4X0kPUBTVNyrYxDjgD5KeL513zF0GDIqIR9rJ+byk7wO3U4wIb46I/+1g3+ZK+glFYX0OeATwlTRmZg1OxWm35Vu6gnR6RPymBn33i4gFaeR4LXBRRFzb0Xod6TV4aAw+5NzOBzSzuvDDjutD0rSIGNlRu0Y4rFpXkqYBwym+C1kLp0qaATwMPA1cV6PtmJlZldT9sGq9RcQW+bz0fcVe2eyDIqJ5Kfo/bmmzmZlZfSz3xbE16TuQZma2nFruD6uamZnlPHLspjZZpz9TfULfzKwmPHI0MzPLuDiamZllXBzNzMwyLo5mZmYZF0czM7OMi6OZmVnGxdHMzCzj4mhmZpZxcTQzM8u4OJqZmWVcHM3MzDIujmZmZhkXRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMi6OZmZmGRdHMzOzTM96B7Cl0zx3Hk0n3lTvGGbWzcwZs1u9I3QLHjmamZllXBzNzMwyLo5mZmYZF0czM7OMi6OZmVnGxdHMzCzj4mhmZpZxcawySdtLmi1phqSVJZ2d3p9d72xmZlYZ3wSg+g4EfhYRFwNI+jowKCIW1jeWmZlVqqYjR0kHS5olaaakCZKGSJqU5k2StF5qN17S/0i6XdJTkj4p6SJJj0oaX+pvgaSfS3oorT8ozT9c0oNpO1dL6lPq9zxJ96R+907zJ0jao9TvZZK+0MY+9JD0M0nNKffRaf4ukqan+RdJ6iXpa8A+wH+mPq8H+gL3S9pX0qCU78H0+kTqq2/q48HU5x6tZTEzs65Rs+IoaRhwErBzRGwKfAsYC1waEcOBy4DzSqusBuwMfBu4ATgHGAZsImlEatMXeCgiNgemAKek+ddExJZpO48CXy31OxjYDtgdGJPmXQgclnL2B7YFbm5jV0YDHwY2a8ktqTcwHtg3IjahGIF/IyIuBK4Hjo+IAyPiC8BbETEiIq4EfgmcExFbAnulHKTP6c9p/k7A2ZL6tvKZjpY0VdLURW/OayOumZl1Vi1HjjsDV0XESwAR8QqwDXB5Wj6Bomi1uCEiAmgG/hkRzRGxGJgNNKU2i4Er0/RvS+t/TNKdkpopDmsOK/V7XUQsjohHgLVSlinARyWtCewPXB0R77axH6OA81uWp/3YEHg6Ih5PbS4BdqjgMxkFjJU0g6KIrippFeBTwIlp/mSgN7BevnJEjIuIkRExskef/hVszszMlkYtzzkKiA7alJe3nJNbXJpued9Wzpb1xwN7RsRMSYcCO7bSb0umFhMoCul+wFfaydjafqi1hhVYAdgmIt56X2eSgL0i4rGl7NfMzKqoliPHScA+klYHkDQQuIeiGEFRmO5awj5XAPZO0weU1l8FeF7SiqnfSowHjgWIiNnttLsVOEJST/j3fvwFaJL00dTmIIrDvB25FTiq5U3pcPEtwNGpSCJpswr3wczMaqBmI8eImC3pDGCKpEXAdOAY4CJJxwMvks77LYE3gGGSpgHzgH3T/B8C9wN/ozgsu0oF+f4p6VHgug6aXghsAMyS9A5wQUSMlXQYMDEVzQeB8yvIfwzwX5JmUXz2dwBHAKcD56ZtCJhDcY7UzMzqQMVpvu5B0oKI6FelvvpQFNLNI6LbXd3Sa/DQGHzIufWOYWbdzPL+PEdJ0yJiZEftlsubAEgaRXFo9FfdsTCamVltdaubAFRr1BgRfyK7GlTSp4Ezs6ZPR8QXq7FNMzPrPrpVcayliLiF4sIYMzNbzi2Xh1XNzMza45FjN7XJOv2ZupyfWDczqxWPHM3MzDIujmZmZhkXRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMi6OZmZmGRdHMzOzjIujmZlZxsXRzMwss8TFUdJqkobXIoyZmVkjqKg4SposaVVJA4GZwMWSflHbaGZmZvVR6cixf0S8DnwJuDgitgBG1S6WmZlZ/VRaHHtKGgzsA9xYwzxmZmZ1V2lxPA24BXgyIh6UtD7w19rFMjMzq5+elTSKiInAxNL7p4C9ahXKzMysniq9IGcDSZMkPZzeD5d0cm2jmZmZ1Uelh1UvAL4PvAMQEbOA/WoVyszMrJ4qOqwK9ImIBySV571bgzxWoea582g68aZ6xzCzZdycMbvVO0JdVDpyfEnSR4AAkLQ38HzNUpmZmdVRpSPHI4FxwEaS5gJPAwfWLJWZmVkddVgcJa0AjIyIUZL6AitExPzaRzMzM6uPDg+rRsRi4Kg0/YYLo5mZLesqPed4m6TjJK0raWDLq6bJzMzM6qTSc45fST+PLM0LYP3qxjEzM6u/Su+Q8+FaBzEzM2sUFRVHSQe3Nj8iLq1unOWbpB2BtyPinnpnMTNbnlV6WHXL0nRvYBfgIWC5L46SekZEtW6IsCOwAHBxNDOro4ouyImIo0uvw4HNgJWqEUDSwZJmSZopaYKkIek+rrPSz/VSu7UkXZvazZS0bZr/HUkPp9expX6vkzRN0mxJozvIsEDSzyU9lLY5KM3/iKQ/pn7ulLRRmj9e0i8k3Q6cKamfpIslNafce6V2n5J0b+p3oqR+af4cST9K85slbSSpCTgC+LakGZK2r8bna2ZmS67Sq1VzbwJDO7txScOAk4CdI2JT4FvAWODSiBgOXAacl5qfB0xJ7TYHZkvaAjgM2Br4OHC4pM1S+6+khzKPBI6RtHo7UfoCD0XE5sAU4JQ0fxxwdOrnOOC/S+tsAIyKiO8CPwTmRcQmKfefJa0BnJzabA5MBb5TWv+lNP9/gOMiYg5wPnBORIyIiDtb+bxGS5oqaeqiN+e1sztmZtYZlZ5zvIF06ziKgroxpUdYdcLOwFUR8RJARLwiaRvgS2n5BOCsUtuDU7tFwDxJ2wHXRsQbKec1wPbAdIqC+MW07roUxfzlNnIsBq5M078FrkmjvG2BiaV7yvYqrTMx5QAYRelG7BHxqqTdKT6nu9P6KwH3lta/Jv2cVtrfdkXEOIqCTa/BQ6OD5mZmtpQqPef4s9L0u8DfIuLZKmxfvFd029LecrU6s7iwZRSwTUS8KWkyxbnSSgXFPwJei4gRbbR5I8uR5xRwW0Ts38b6C9PPRVT+ezAzsy5Q6WHVz0XElPS6OyKelXRmFbY/Cdin5ZBnurHAPbw3CjsQuKvU9hupXQ9JqwJ3AHtK6pNubfdF4E6gP/BqKowbURxybc8KwN5p+gDgroh4HXha0v9P25SkTdtY/1bSXYRS29WA+4BPSPpomtdH0gYd5JgPrNJBGzMzq7FKi+Ourcz7bGc3HhGzgTOAKZJmAr8AjgEOkzQLOIjiPCTp506SmikORQ6LiIeA8cADwP3AhRExHfgj0DP1cTpFoWrPG8AwSdMoDt+eluYfCHw1ZZsN7NHG+j8GVksXBc0EdoqIF4FDgd+lHPcBG3WQ4wbgi74gx8ysvhTR9lFLSd8AvklxJ5wnS4tWAe6OiC/XNl7XkLQgIvrVO8eS6DV4aAw+5Nx6xzCzZdyy9jxHSdMiYmRH7To613U58Afgp8CJpfnzI+KVTuQzMzNrWO0Wx4iYB8wD9geQtCbFhS39JPWLiL/XPmL1SLqf919xCnBQdxs1mplZbVX6VY7PU5wPXBt4ARgCPAoMq1206ouIreudwczMGl+lF+T8mOKKz8fTTch3Ae6uWSozM7M6qrQ4vhMRLwMrSFohIm4H2vr+n5mZWbdW6ZfPX0t3jLkTuEzSCxQ3A7A62WSd/kxdxq4iMzNrFJWOHPeguJ/qsRTfIXwS+HytQpmZmdVTpQ87fkPSEGBoRFwiqQ/Qo7bRzMzM6qOikaOkw4GrgF+nWesA19UqlJmZWT1Velj1SOATwOsAEfFXYM1ahTIzM6unSovjwoh4u+WNpJ50/DQNMzOzbqnS4jhF0g+AlSXtSvEsxxtqF8vMzKx+Ki2OJwIvAs3A14GbKZ5yb2Zmtsxp92pVSetFxN8jYjFwQXqZmZkt0zoaOf77ilRJV9c4i5mZWUPoqDiqNL1+LYOYmZk1io6KY7QxbWZmtszq6A45m0p6nWIEuXKaJr2PiFi1punMzMzqoKOHHfsWcWZmttyp9KscZmZmyw0XRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMh3dIccaVPPceTSdeFO9Y5iZLbU5Y3ard4Q2eeRoZmaWcXE0MzPLuDiamZllXBzNzMwyLo5mZmYZF0czM7OMi6OZmVmmWxZHSU2SHq53jrZIuqeCNhdK2rgr8piZ2ZLxTQCqSFKPiFgUEdt21DYivtYVmczMbMl1y5Fj0lPSJZJmSbpKUh9JW0iaImmapFskDQaQdLikByXNlHS1pD5p/nhJ50m6R9JTkvZua2MqnC3pYUnNkvZN83eUdLuky4HmNG9B+rmCpP+WNFvSjZJubtmGpMmSRra0l3RGynefpLVq+smZmVm7unNx3BAYFxHDgdeBI4FfAXtHxBbARcAZqe01EbFlRGwKPAp8tdTPYGA7YHdgTDvb+xIwAtgUGAWc3VJ8ga2AkyIiP0z6JaAJ2AT4GrBNG333Be5L+e4ADm+tkaTRkqZKmrrozXntRDUzs87ozodVn4mIu9P0b4EfAB8DbpME0AN4Pi3/mKQfAwOAfsAtpX6ui4jFwCMdjNi2A34XEYuAf0qaAmxJUZgfiIin21hnYur/H5Jub6Pvt4Eb0/Q0YNfWGkXEOGAcQK/BQ6OdrGZm1gnduTjmxWE+MDsiWhudjQf2jIiZkg4FdiwtW1iaVjvba2/ZG0uxTtk7EdGyP4vo3r8XM7NurzsfVl1PUksh3B+4DxjUMk/SipKGpeWrAM9LWhE4cCm3dwewr6QekgYBOwAPdLDOXcBe6dzjWry/KJuZWYPqzsXxUeAQSbOAgaTzjcCZkmYCM4CWq0Z/CNwP3Ab8ZSm3dy0wC5gJ/Bk4ISL+0cE6VwPPAg8Dv04ZfLLQzKzB6b2jeVYLkvpFxAJJq1OMND9RQVHtUK/BQ2PwIed2PqCZWZ3U43mOkqZFxMiO2vncVu3dKGkAsBJwejUKo5mZ1ZaLY0bSJsCEbPbCiNh6afqLiB07HcrMzLqUi2MmIpopvs9oZmbLqe58QY6ZmVlNeOTYTW2yTn+m1uFktpnZ8sAjRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNDMzy7g4mpmZZVwczczMMi6OZmZmGRdHMzOzjIujmZlZxsXRzMws4+JoZmaWcXE0MzPLuDiamZllXBzNzMwyLo5mZmYZF0czM7OMi6OZmVnGxdHMzCzj4mhmZpZxcTQzM8v0rHcAWzrNc+fRdOJN9Y5hZtal5ozZrUu245GjmZlZxsXRzMws4+JoZmaWcXE0MzPLuDiamZllXBzNzMwyLo5mZmYZf8+xBiQtApopPt+ngYMi4jVJTcCjwGOl5r+IiEslzQHmA4vS/G9GxD1dFtrMzP7NxbE23oqIEQCSLgGOBM5Iy55sWdaKnSLipa4IaGZmbfNh1dq7F1in3iHMzKxyHjnWkKQewC7Ab0qzPyJpRun90RFxZ5q+PR2SXRgRW7fS32hgNECPVQfVKLWZmbk41sbKqQA2AdOA20rLlvqwakSMA8YB9Bo8NKqU1czMMj6sWhst5xyHACtRnHM0M7NuwsWxhiJiHnAMcJykFeudx8zMKuPiWGMRMR2YCeyXZn1E0ozS65g6xjMzs1b4nGMNRES/7P3nS29XbmOdplpmMjOzynnkaGZmlnFxNDMzy7g4mpmZZVwczczMMi6OZmZmGV+t2k1tsk5/po7Zrd4xzMyWSR45mpmZZVwczczMMi6OZmZmGRdHMzOzjIujmZlZxsXRzMws4+JoZmaWcXE0MzPLuDiamZllXBzNzMwyioh6Z7ClIGk+8Fi9c7RiDeCleodoQ6Nma9Rc0LjZGjUXNG62Rs0FXZttSEQM6qiR763afT0WESPrHSInaWoj5oLGzdaouaBxszVqLmjcbI2aCxozmw+rmpmZZVwczczMMi6O3de4egdoQ6PmgsbN1qi5oHGzNWouaNxsjZoLGjCbL8gxMzPLeORoZmaWcXE0MzPLuDg2IEmfkfSYpCckndjK8l6SrkzL75fUVFr2/TT/MUmfboRcknaVNE1Sc/q5czVzdSZbafl6khZIOq5RckkaLuleSbPTZ9e73rkkrSjpkpTnUUnfr1amJci2g6SHJL0rae9s2SGS/ppehzRCLkkjSr/HWZL2rWauzmQrLV9V0lxJYxslV/pv8tb0d/ZI/t9szUWEXw30AnoATwLrAysBM4GNszbfBM5P0/sBV6bpjVP7XsCHUz89GiDXZsDaafpjwNxG+cxKy68GJgLHNUIuiu8gzwI2Te9Xb5Df5QHAFWm6DzAHaOriz6wJGA5cCuxdmj8QeCr9XC1Nr9YAuTYAhqbptYHngQGN8JmVlv8SuBwY2yi5gMnArmm6H9CnWtkqeXnk2Hi2Ap6IiKci4m3gCmCPrM0ewCVp+ipgF0lK86+IiIUR8TTwROqvrrkiYnpEPJfmzwZ6S+pVpVydygYgaU+K/5HOrmKmzub6FDArImYCRMTLEbGoAXIF0FdST2Bl4G3g9SrlqihbRMyJiFnA4mzdTwO3RcQrEfEqcBvwmXrniojHI+Kvafo54AWgwzu0dEU2AElbAGsBt1YxU6dySdoY6BkRt6V2CyLizSrna5eLY+NZB3im9P7ZNK/VNhHxLjCPYmRRybr1yFW2FzA9IhZWKVensknqC3wP+FEV83Q6F8VoIyTdkg47ndAgua4C3qAY/fwd+FlEvNLF2Wqxbpf0LWkrilHUk1XKBZ3IJmkF4OfA8VXM0+lcFH//r0m6RtJ0SWdL6lH1hO3w7eMaj1qZl3/fpq02lay7tDqTq1goDQPOpBgVVVNnsv0IOCciFqSBZKPk6glsB2wJvAlMkjQtIibVOddWwCKKw4OrAXdK+lNEPFWFXJVmq8W6Ne9b0mBgAnBIRHxgBNcJncn2TeDmiHimTn//bekJbE9xSubvwJXAocBvqpKsAh45Np5ngXVL7z8EPNdWm3R4qz/wSoXr1iMXkj4EXAscHBHV/FdzZ7NtDZwlaQ5wLPADSUc1QK5ngSkR8VI6nHQzsHkD5DoA+GNEvBMRLwB3A9W8J2Zn/obr/fffJkmrAjcBJ0fEfVXKVI1s2wBHpb//nwEHSxrTALmepTjC9FQ6cnEd1fv7r0xXnuD0q+MXxb+YnqK4oKblJPawrM2RvP9iid+n6WG8/4Kcp6jeRRydyTUgtd+r0T6zrM2pVPeCnM58ZqsBD1Fc9NIT+BOwWwPk+h5wMcWooC/wCDC8Kz+zUtvxfPCCnKfTZ7damh7YALlWAiYBx9br77+tbNmyQ6nuBTmd+cx6pPaD0vuLgSNr8fm1mb8rN+ZXhb8U+BzwOMV5iZPSvNOAL6Tp3hRXVj4BPACsX1r3pLTeY8BnGyEXcDLFeaoZpdeajZAt6+NUqlgcq/C7/DLFRUIPA2c1Qi6KqwYnplyPAMfX4e9/S4qRxRvAy8Ds0rpfSZmfAA5rhFzp9/hO9vc/ohGyZX0cShWLYxV+l7tSXLHdTFE8V6r231p7L98+zszMLONzjmZmZhkXRzMzs4yLo5mZWcbF0czMLOPiaGZmlnFxNGsgkhZJmiHpYUk3SBpQwToLOlg+QNI3S+/XlnRVFbI2SXq4s/0s4TZHSPpcV27Tlk8ujmaN5a2IGBERH6O4I82RVehzAMVtwoDi5tcR8YHHFjW6dKeeERTfnTOrKRdHs8Z1L6UbNUs6XtKD6ZmAH7hRuqR+kialG5U3S2p5AsIY4CNpRHp2ecSXntU4rNTHZElbSOor6aK0vemlvlol6VBJ16XR7tOSjpL0nbTufZIGlvo/V9I9aXS8VZo/MK0/K7UfnuafKmmcpFspHmt0GrBv2pd9JW2V+pqefm5YynONpD+qeLbjWaWsn0mf0UxJk9K8JdpfWw505R0H/PLLr/ZfwIL0swfFnWg+k95/ChhHcdu2FYAbgR2ydXoCq6bpNSjuEiOKZ+Y9XNrGv98D3wZ+lKYHA4+n6Z8AX07TAyjuctI3y1ru59C0vVUoHsc0DzgiLTuHdOs0imf0XZCmdyit/yvglDS9MzAjTZ8KTANWLm1nbCnDqhSPNgIYBVxdavcvX1nDAAACUUlEQVQUxT1hewN/o7jP5yCKJ0V8OLUbWOn++rV8vfxUDrPGsrKkGRSFZxrFMwmhKI6fAqan9/2AocAdpXUF/ETSDhTPx1uH4jl97fl92sYpwD4UBblle1+QdFx63xtYD3i0nb5uj4j5wHxJ84Ab0vxmigfatvgdQETcoeIJ9AMonkCyV5r/Z0mrS+qf2l8fEW+1sc3+wCWShlI88WHF0rJJETEPQNIjwBCKe67eEcXzTon3Hre1NPtryzAXR7PG8lZEjEiF4UaKc47nURS+n0bEr9tZ90CKkdEWEfFOetJC7/Y2FhFzJb2cDmPuC3w9LRLFjeIfW4Ls5Wd0Li69X8z7/1+T37Oyo8etvdHONk+nKMpflNREMTJtLc+ilKHlgc25pdlfW4b5nKNZA0ojnmOA4yStCNwCfEVSPwBJ60haM1utP/BCKow7UYyUAOZTHO5syxXACUD/iGhO824BjlZ6yJ+kzaqxX8m+qc/tgHlpX++gKO5I2hF4KSJeb2XdfF/6A3PT9KEVbPte4JOSPpy2NTDNr+X+Wjfk4mjWoCJiOsVje/aLiFuBy4F7JTUDV/HBgncZMFLSVIpC85fUz8vA3ekCmLNb2dRVpMdSleadTnGIcla6eOf06u0Zr0q6Bzgf+Gqad2rKPoviAqJD2lj3dmDjlgtygLOAn0q6m+I8bbsi4kVgNHCNpJkUD9GF2u6vdUN+KoeZdRlJkykeCza13lnM2uORo5mZWcYjRzMzs4xHjmZmZhkXRzMzs4yLo5mZWcbF0czMLOPiaGZmlvk/1sckLaji2DsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#Splitting the variables into features and target\n",
    "X = data_clean_label_encoding.drop('rating', axis=1)\n",
    "y = data_clean_label_encoding['rating'].values\n",
    "\n",
    "print(X.shape)#printing dimensions of features\n",
    "print(y.shape)#printing dimensions of label\n",
    "\n",
    "#Printing the variability of all the features\n",
    "#Since the Variability of any column is not very low so selecting all the features based on variability\n",
    "print(X.var())\n",
    "\n",
    "\n",
    "# Using ExtraTreesClassifier for feature selection\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print \"Dataset Size Before Feature Selection \"\n",
    "print X.shape\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_feat_select = model.transform(X)\n",
    "print \"Dataset Size After Feature Selection \"\n",
    "print X_feat_select.shape            \n",
    "print \"Relative Feature importance for each of the Features- \"\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "feat_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(5).plot(kind='barh')\n",
    "plt.title('Important Features')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suite-1 training and testing size\n",
      "(1200, 5)\n",
      "(300, 5)\n",
      "Suite-2 training and testing size\n",
      "(900, 5)\n",
      "(600, 5)\n",
      "Suite-3 training and testing size\n",
      "(1050, 5)\n",
      "(450, 5)\n"
     ]
    }
   ],
   "source": [
    "# We have prepared three suits for the partition\n",
    "\n",
    "# Suite-1 training- 80% testing- 20%\n",
    "X_suite1_train, X_suite1_test, y_suite1_train, y_suite1_test = train_test_split(X_feat_select, y, test_size= 0.2, random_state=42)\n",
    "print 'Suite-1 training and testing size'\n",
    "print X_suite1_train.shape\n",
    "print X_suite1_test.shape\n",
    "\n",
    "# Suite-1 training- 60% testing- 40%\n",
    "X_suite2_train, X_suite2_test, y_suite2_train, y_suite2_test = train_test_split(X_feat_select, y, test_size= 0.4, random_state=42)\n",
    "print 'Suite-2 training and testing size'\n",
    "print X_suite2_train.shape\n",
    "print X_suite2_test.shape\n",
    "\n",
    "# Suite-1 training- 70% testing- 30%\n",
    "X_suite3_train, X_suite3_test, y_suite3_train, y_suite3_test = train_test_split(X_feat_select, y, test_size= 0.3, random_state=42)\n",
    "print 'Suite-3 training and testing size'\n",
    "print X_suite3_train.shape\n",
    "print X_suite3_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with suite-1\n",
      "[0.21452145 0.22847682 0.22073579 0.23648649]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00177503, 0.00141287, 0.00139308, 0.00122499]), 'test_score': array([0.21452145, 0.22847682, 0.22073579, 0.23648649]), 'train_score': array([0.26532887, 0.24276169, 0.27524972, 0.25774336]), 'fit_time': array([0.02845192, 0.02675509, 0.02323604, 0.02253008])}\n",
      "\n",
      "\n",
      "('Suite-1 mean test cvscores for max_leaf_nodes =5', 0.2250551364442328)\n",
      "[0.20132013 0.20860927 0.21404682 0.23648649]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00123501, 0.00124407, 0.00137401, 0.00135612]), 'test_score': array([0.20132013, 0.20860927, 0.21404682, 0.23648649]), 'train_score': array([0.27090301, 0.25946548, 0.27857936, 0.25884956]), 'fit_time': array([0.02232003, 0.02209783, 0.02432919, 0.02752495])}\n",
      "\n",
      "\n",
      "('Suite-1 mean test cvscores for max_leaf_nodes =6', 0.2151156781913354)\n",
      "[0.19141914 0.20860927 0.2006689  0.23648649]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00127101, 0.00124478, 0.00121903, 0.00137687]), 'test_score': array([0.19141914, 0.20860927, 0.2006689 , 0.23648649]), 'train_score': array([0.26978818, 0.26948775, 0.28523862, 0.26216814]), 'fit_time': array([0.02466416, 0.02188706, 0.0218358 , 0.02874207])}\n",
      "\n",
      "\n",
      "('Suite-1 mean test cvscores for max_leaf_nodes =7', 0.20929594906123175)\n",
      "[0.19141914 0.20860927 0.2006689  0.23648649]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00130701, 0.00130296, 0.00144506, 0.00139999]), 'test_score': array([0.19141914, 0.20860927, 0.2006689 , 0.23648649]), 'train_score': array([0.26978818, 0.26948775, 0.28523862, 0.26216814]), 'fit_time': array([0.02685094, 0.02365398, 0.02384806, 0.0271821 ])}\n",
      "\n",
      "\n",
      "('Suite-1 mean test cvscores for max_leaf_nodes =8', 0.20929594906123175)\n",
      "\n",
      " Results with suite-2\n",
      "[0.25217391 0.18666667 0.21973094 0.23423423]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00119495, 0.00116014, 0.00132394, 0.00127196]), 'test_score': array([0.25217391, 0.18666667, 0.21973094, 0.23423423]), 'train_score': array([0.2761194 , 0.28444444, 0.28360414, 0.26843658]), 'fit_time': array([0.02279711, 0.02107787, 0.02294087, 0.03190994])}\n",
      "\n",
      "\n",
      "('Suite-2 mean test cvscores for max_leaf_nodes =5', 0.22320143891210376)\n",
      "[0.24347826 0.18222222 0.20627803 0.22972973]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00120497, 0.00117183, 0.00118303, 0.00131297]), 'test_score': array([0.24347826, 0.18222222, 0.20627803, 0.22972973]), 'train_score': array([0.27014925, 0.28296296, 0.30132939, 0.28318584]), 'fit_time': array([0.02437615, 0.02121902, 0.02198696, 0.02812481])}\n",
      "\n",
      "\n",
      "('Suite-2 mean test cvscores for max_leaf_nodes =6', 0.2154270599318367)\n",
      "[0.25652174 0.2        0.20179372 0.22972973]\n",
      "\n",
      "\n",
      "{'score_time': array([0.001261  , 0.0012188 , 0.00134611, 0.00133991]), 'test_score': array([0.25652174, 0.2       , 0.20179372, 0.22972973]), 'train_score': array([0.28507463, 0.28296296, 0.31610044, 0.29056047]), 'fit_time': array([0.02632403, 0.02397513, 0.02349997, 0.02688622])}\n",
      "\n",
      "\n",
      "('Suite-2 mean test cvscores for max_leaf_nodes =7', 0.22201129770831468)\n",
      "[0.25652174 0.2        0.20179372 0.22972973]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00128102, 0.00117207, 0.0011611 , 0.00292802]), 'test_score': array([0.25652174, 0.2       , 0.20179372, 0.22972973]), 'train_score': array([0.28507463, 0.28296296, 0.31610044, 0.29056047]), 'fit_time': array([0.02305913, 0.02141404, 0.02143097, 0.02493811])}\n",
      "\n",
      "\n",
      "('Suite-2 mean test cvscores for max_leaf_nodes =8', 0.22201129770831468)\n",
      "\n",
      " Results with suite-3\n",
      "[0.23880597 0.23106061 0.20769231 0.20930233]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00137186, 0.00138307, 0.00137401, 0.00131297]), 'test_score': array([0.23880597, 0.23106061, 0.20769231, 0.20930233]), 'train_score': array([0.26726343, 0.26081425, 0.27341772, 0.26641414]), 'fit_time': array([0.02379513, 0.02497888, 0.02497101, 0.02662396])}\n",
      "\n",
      "\n",
      "('Suite-3 mean test cvscores for max_leaf_nodes =5', 0.2217153023708907)\n",
      "[0.25       0.22348485 0.19615385 0.20542636]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00138998, 0.00119495, 0.00238609, 0.00234509]), 'test_score': array([0.25      , 0.22348485, 0.19615385, 0.20542636]), 'train_score': array([0.28132992, 0.28117048, 0.28101266, 0.26893939]), 'fit_time': array([0.02413893, 0.02161884, 0.02180696, 0.02632499])}\n",
      "\n",
      "\n",
      "('Suite-3 mean test cvscores for max_leaf_nodes =6', 0.2187662628069605)\n",
      "[0.23880597 0.21969697 0.19615385 0.20155039]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00130701, 0.00120997, 0.00119209, 0.00197792]), 'test_score': array([0.23880597, 0.21969697, 0.19615385, 0.20155039]), 'train_score': array([0.2915601 , 0.28880407, 0.28734177, 0.28030303]), 'fit_time': array([0.02419186, 0.02191114, 0.02158189, 0.02579594])}\n",
      "\n",
      "\n",
      "('Suite-3 mean test cvscores for max_leaf_nodes =7', 0.2140517933992422)\n",
      "[0.23880597 0.21969697 0.19615385 0.20155039]\n",
      "\n",
      "\n",
      "{'score_time': array([0.00128508, 0.00119996, 0.00118899, 0.00135207]), 'test_score': array([0.23880597, 0.21969697, 0.19615385, 0.20155039]), 'train_score': array([0.2915601 , 0.28880407, 0.28734177, 0.28030303]), 'fit_time': array([0.02336979, 0.02171707, 0.02172613, 0.02899289])}\n",
      "\n",
      "\n",
      "('Suite-3 mean test cvscores for max_leaf_nodes =8', 0.2140517933992422)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  #max_leaf_nodes = 7\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Model Hypertuning with Suite-1\n",
    "print 'Results with suite-1'\n",
    "# max_leaf_nodes =5\n",
    "model_suite1_node5 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =5)\n",
    "\n",
    "cv = cross_validate(model_suite1_node5, X_suite1_train, y_suite1_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-1 mean test cvscores for max_leaf_nodes =5', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =6\n",
    "model_suite1_node6 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =6)\n",
    "\n",
    "cv = cross_validate(model_suite1_node6, X_suite1_train, y_suite1_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-1 mean test cvscores for max_leaf_nodes =6', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =7\n",
    "model_suite1_node7 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite1_node7, X_suite1_train, y_suite1_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-1 mean test cvscores for max_leaf_nodes =7', cv['test_score'].mean())\n",
    "\n",
    "\n",
    "# max_leaf_nodes =8\n",
    "model_suite1_node8 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite1_node8, X_suite1_train, y_suite1_train, cv=4)\n",
    "\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-1 mean test cvscores for max_leaf_nodes =8', cv['test_score'].mean())\n",
    "\n",
    "# Model Hypertuning with Suite-2\n",
    "print '\\n Results with suite-2'\n",
    "# max_leaf_nodes =5\n",
    "model_suite2_node5 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =5)\n",
    "\n",
    "cv = cross_validate(model_suite2_node5, X_suite2_train, y_suite2_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-2 mean test cvscores for max_leaf_nodes =5', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =6\n",
    "model_suite2_node6 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =6)\n",
    "\n",
    "cv = cross_validate(model_suite2_node6, X_suite2_train, y_suite2_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-2 mean test cvscores for max_leaf_nodes =6', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =7\n",
    "model_suite2_node7 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite2_node7, X_suite2_train, y_suite2_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-2 mean test cvscores for max_leaf_nodes =7', cv['test_score'].mean())\n",
    "\n",
    "\n",
    "# max_leaf_nodes =8\n",
    "model_suite2_node8 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite2_node8, X_suite2_train, y_suite2_train, cv=4)\n",
    "\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-2 mean test cvscores for max_leaf_nodes =8', cv['test_score'].mean())\n",
    "\n",
    "# Model Hypertuning with Suite-3\n",
    "print '\\n Results with suite-3'\n",
    "# max_leaf_nodes =5\n",
    "model_suite3_node5 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =5)\n",
    "\n",
    "cv = cross_validate(model_suite3_node5, X_suite3_train, y_suite3_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-3 mean test cvscores for max_leaf_nodes =5', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =6\n",
    "model_suite3_node6 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =6)\n",
    "\n",
    "cv = cross_validate(model_suite3_node6, X_suite3_train, y_suite3_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-3 mean test cvscores for max_leaf_nodes =6', cv['test_score'].mean())\n",
    "\n",
    "# max_leaf_nodes =7\n",
    "model_suite3_node7 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite3_node7, X_suite3_train, y_suite3_train, cv=4)\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-3 mean test cvscores for max_leaf_nodes =7', cv['test_score'].mean())\n",
    "\n",
    "\n",
    "# max_leaf_nodes =8\n",
    "model_suite3_node8 = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =7)\n",
    "\n",
    "cv = cross_validate(model_suite3_node8, X_suite3_train, y_suite3_train, cv=4)\n",
    "\n",
    "print(cv['test_score'])\n",
    "print (\"\\n\")\n",
    "print cv\n",
    "print (\"\\n\")\n",
    "print('Suite-3 mean test cvscores for max_leaf_nodes =8', cv['test_score'].mean())\n",
    "\n",
    "# Random Forest classifier with max_leaf_nodes= 5 gave the best validation performance of 22.32% with suite=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest classifier with max_leaf_nodes= 5 gave the best validation performance of 22.32% with suite=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Fold]', 'Actual Training data:', array([ 240,  241,  242,  243,  244,  245,  246,  247,  248,  249,  250,\n",
      "        251,  252,  253,  254,  255,  256,  257,  258,  259,  260,  261,\n",
      "        262,  263,  264,  265,  266,  267,  268,  269,  270,  271,  272,\n",
      "        273,  274,  275,  276,  277,  278,  279,  280,  281,  282,  283,\n",
      "        284,  285,  286,  287,  288,  289,  290,  291,  292,  293,  294,\n",
      "        295,  296,  297,  298,  299,  300,  301,  302,  303,  304,  305,\n",
      "        306,  307,  308,  309,  310,  311,  312,  313,  314,  315,  316,\n",
      "        317,  318,  319,  320,  321,  322,  323,  324,  325,  326,  327,\n",
      "        328,  329,  330,  331,  332,  333,  334,  335,  336,  337,  338,\n",
      "        339,  340,  341,  342,  343,  344,  345,  346,  347,  348,  349,\n",
      "        350,  351,  352,  353,  354,  355,  356,  357,  358,  359,  360,\n",
      "        361,  362,  363,  364,  365,  366,  367,  368,  369,  370,  371,\n",
      "        372,  373,  374,  375,  376,  377,  378,  379,  380,  381,  382,\n",
      "        383,  384,  385,  386,  387,  388,  389,  390,  391,  392,  393,\n",
      "        394,  395,  396,  397,  398,  399,  400,  401,  402,  403,  404,\n",
      "        405,  406,  407,  408,  409,  410,  411,  412,  413,  414,  415,\n",
      "        416,  417,  418,  419,  420,  421,  422,  423,  424,  425,  426,\n",
      "        427,  428,  429,  430,  431,  432,  433,  434,  435,  436,  437,\n",
      "        438,  439,  440,  441,  442,  443,  444,  445,  446,  447,  448,\n",
      "        449,  450,  451,  452,  453,  454,  455,  456,  457,  458,  459,\n",
      "        460,  461,  462,  463,  464,  465,  466,  467,  468,  469,  470,\n",
      "        471,  472,  473,  474,  475,  476,  477,  478,  479,  480,  481,\n",
      "        482,  483,  484,  485,  486,  487,  488,  489,  490,  491,  492,\n",
      "        493,  494,  495,  496,  497,  498,  499,  500,  501,  502,  503,\n",
      "        504,  505,  506,  507,  508,  509,  510,  511,  512,  513,  514,\n",
      "        515,  516,  517,  518,  519,  520,  521,  522,  523,  524,  525,\n",
      "        526,  527,  528,  529,  530,  531,  532,  533,  534,  535,  536,\n",
      "        537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
      "        548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,\n",
      "        559,  560,  561,  562,  563,  564,  565,  566,  567,  568,  569,\n",
      "        570,  571,  572,  573,  574,  575,  576,  577,  578,  579,  580,\n",
      "        581,  582,  583,  584,  585,  586,  587,  588,  589,  590,  591,\n",
      "        592,  593,  594,  595,  596,  597,  598,  599,  600,  601,  602,\n",
      "        603,  604,  605,  606,  607,  608,  609,  610,  611,  612,  613,\n",
      "        614,  615,  616,  617,  618,  619,  620,  621,  622,  623,  624,\n",
      "        625,  626,  627,  628,  629,  630,  631,  632,  633,  634,  635,\n",
      "        636,  637,  638,  639,  640,  641,  642,  643,  644,  645,  646,\n",
      "        647,  648,  649,  650,  651,  652,  653,  654,  655,  656,  657,\n",
      "        658,  659,  660,  661,  662,  663,  664,  665,  666,  667,  668,\n",
      "        669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
      "        680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,\n",
      "        691,  692,  693,  694,  695,  696,  697,  698,  699,  700,  701,\n",
      "        702,  703,  704,  705,  706,  707,  708,  709,  710,  711,  712,\n",
      "        713,  714,  715,  716,  717,  718,  719,  720,  721,  722,  723,\n",
      "        724,  725,  726,  727,  728,  729,  730,  731,  732,  733,  734,\n",
      "        735,  736,  737,  738,  739,  740,  741,  742,  743,  744,  745,\n",
      "        746,  747,  748,  749,  750,  751,  752,  753,  754,  755,  756,\n",
      "        757,  758,  759,  760,  761,  762,  763,  764,  765,  766,  767,\n",
      "        768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
      "        779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
      "        790,  791,  792,  793,  794,  795,  796,  797,  798,  799,  800,\n",
      "        801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
      "        812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,\n",
      "        823,  824,  825,  826,  827,  828,  829,  830,  831,  832,  833,\n",
      "        834,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
      "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
      "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
      "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
      "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
      "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
      "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
      "        911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
      "        922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
      "        933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
      "        944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
      "        955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
      "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
      "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
      "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
      "        999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009,\n",
      "       1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,\n",
      "       1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031,\n",
      "       1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042,\n",
      "       1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053,\n",
      "       1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064,\n",
      "       1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "       1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086,\n",
      "       1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097,\n",
      "       1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n",
      "       1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "       1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130,\n",
      "       1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141,\n",
      "       1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152,\n",
      "       1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163,\n",
      "       1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174,\n",
      "       1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
      "       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
      "       1197, 1198, 1199]), 'Validation data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239]))\n",
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,\n",
      "        220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,\n",
      "        231,  232,  233,  234,  235,  236,  237,  238,  239,  480,  481,\n",
      "        482,  483,  484,  485,  486,  487,  488,  489,  490,  491,  492,\n",
      "        493,  494,  495,  496,  497,  498,  499,  500,  501,  502,  503,\n",
      "        504,  505,  506,  507,  508,  509,  510,  511,  512,  513,  514,\n",
      "        515,  516,  517,  518,  519,  520,  521,  522,  523,  524,  525,\n",
      "        526,  527,  528,  529,  530,  531,  532,  533,  534,  535,  536,\n",
      "        537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
      "        548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,\n",
      "        559,  560,  561,  562,  563,  564,  565,  566,  567,  568,  569,\n",
      "        570,  571,  572,  573,  574,  575,  576,  577,  578,  579,  580,\n",
      "        581,  582,  583,  584,  585,  586,  587,  588,  589,  590,  591,\n",
      "        592,  593,  594,  595,  596,  597,  598,  599,  600,  601,  602,\n",
      "        603,  604,  605,  606,  607,  608,  609,  610,  611,  612,  613,\n",
      "        614,  615,  616,  617,  618,  619,  620,  621,  622,  623,  624,\n",
      "        625,  626,  627,  628,  629,  630,  631,  632,  633,  634,  635,\n",
      "        636,  637,  638,  639,  640,  641,  642,  643,  644,  645,  646,\n",
      "        647,  648,  649,  650,  651,  652,  653,  654,  655,  656,  657,\n",
      "        658,  659,  660,  661,  662,  663,  664,  665,  666,  667,  668,\n",
      "        669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
      "        680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,\n",
      "        691,  692,  693,  694,  695,  696,  697,  698,  699,  700,  701,\n",
      "        702,  703,  704,  705,  706,  707,  708,  709,  710,  711,  712,\n",
      "        713,  714,  715,  716,  717,  718,  719,  720,  721,  722,  723,\n",
      "        724,  725,  726,  727,  728,  729,  730,  731,  732,  733,  734,\n",
      "        735,  736,  737,  738,  739,  740,  741,  742,  743,  744,  745,\n",
      "        746,  747,  748,  749,  750,  751,  752,  753,  754,  755,  756,\n",
      "        757,  758,  759,  760,  761,  762,  763,  764,  765,  766,  767,\n",
      "        768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
      "        779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
      "        790,  791,  792,  793,  794,  795,  796,  797,  798,  799,  800,\n",
      "        801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
      "        812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,\n",
      "        823,  824,  825,  826,  827,  828,  829,  830,  831,  832,  833,\n",
      "        834,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
      "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
      "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
      "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
      "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
      "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
      "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
      "        911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
      "        922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
      "        933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
      "        944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
      "        955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
      "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
      "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
      "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
      "        999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009,\n",
      "       1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,\n",
      "       1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031,\n",
      "       1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042,\n",
      "       1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053,\n",
      "       1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064,\n",
      "       1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "       1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086,\n",
      "       1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097,\n",
      "       1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n",
      "       1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "       1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130,\n",
      "       1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141,\n",
      "       1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152,\n",
      "       1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163,\n",
      "       1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174,\n",
      "       1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
      "       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
      "       1197, 1198, 1199]), 'Validation data:', array([240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "       253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278,\n",
      "       279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291,\n",
      "       292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304,\n",
      "       305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317,\n",
      "       318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330,\n",
      "       331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
      "       344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356,\n",
      "       357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369,\n",
      "       370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
      "       383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
      "       396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
      "       409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,\n",
      "        220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,\n",
      "        231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,\n",
      "        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,\n",
      "        253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,\n",
      "        264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,\n",
      "        275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,\n",
      "        286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,\n",
      "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,\n",
      "        308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,\n",
      "        319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,\n",
      "        330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,\n",
      "        341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,\n",
      "        352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,\n",
      "        363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,\n",
      "        374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,\n",
      "        385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,\n",
      "        396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,\n",
      "        407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,\n",
      "        418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,\n",
      "        429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,\n",
      "        440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,\n",
      "        451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,\n",
      "        462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,\n",
      "        473,  474,  475,  476,  477,  478,  479,  720,  721,  722,  723,\n",
      "        724,  725,  726,  727,  728,  729,  730,  731,  732,  733,  734,\n",
      "        735,  736,  737,  738,  739,  740,  741,  742,  743,  744,  745,\n",
      "        746,  747,  748,  749,  750,  751,  752,  753,  754,  755,  756,\n",
      "        757,  758,  759,  760,  761,  762,  763,  764,  765,  766,  767,\n",
      "        768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
      "        779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
      "        790,  791,  792,  793,  794,  795,  796,  797,  798,  799,  800,\n",
      "        801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
      "        812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,\n",
      "        823,  824,  825,  826,  827,  828,  829,  830,  831,  832,  833,\n",
      "        834,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
      "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
      "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
      "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
      "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
      "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
      "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
      "        911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
      "        922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
      "        933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
      "        944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
      "        955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
      "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
      "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
      "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
      "        999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009,\n",
      "       1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,\n",
      "       1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031,\n",
      "       1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042,\n",
      "       1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053,\n",
      "       1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064,\n",
      "       1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "       1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086,\n",
      "       1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097,\n",
      "       1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n",
      "       1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "       1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130,\n",
      "       1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141,\n",
      "       1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152,\n",
      "       1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163,\n",
      "       1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174,\n",
      "       1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
      "       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
      "       1197, 1198, 1199]), 'Validation data:', array([480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492,\n",
      "       493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505,\n",
      "       506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518,\n",
      "       519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "       532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544,\n",
      "       545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557,\n",
      "       558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570,\n",
      "       571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583,\n",
      "       584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596,\n",
      "       597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609,\n",
      "       610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622,\n",
      "       623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635,\n",
      "       636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648,\n",
      "       649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661,\n",
      "       662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674,\n",
      "       675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687,\n",
      "       688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700,\n",
      "       701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713,\n",
      "       714, 715, 716, 717, 718, 719]))\n",
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,\n",
      "        220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,\n",
      "        231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,\n",
      "        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,\n",
      "        253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,\n",
      "        264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,\n",
      "        275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,\n",
      "        286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,\n",
      "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,\n",
      "        308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,\n",
      "        319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,\n",
      "        330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,\n",
      "        341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,\n",
      "        352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,\n",
      "        363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,\n",
      "        374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,\n",
      "        385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,\n",
      "        396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,\n",
      "        407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,\n",
      "        418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,\n",
      "        429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,\n",
      "        440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,\n",
      "        451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,\n",
      "        462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,\n",
      "        473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,\n",
      "        484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,\n",
      "        495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,\n",
      "        506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,\n",
      "        517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,\n",
      "        528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,\n",
      "        539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,\n",
      "        550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,\n",
      "        561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
      "        572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,\n",
      "        583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,\n",
      "        594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,\n",
      "        605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,\n",
      "        616,  617,  618,  619,  620,  621,  622,  623,  624,  625,  626,\n",
      "        627,  628,  629,  630,  631,  632,  633,  634,  635,  636,  637,\n",
      "        638,  639,  640,  641,  642,  643,  644,  645,  646,  647,  648,\n",
      "        649,  650,  651,  652,  653,  654,  655,  656,  657,  658,  659,\n",
      "        660,  661,  662,  663,  664,  665,  666,  667,  668,  669,  670,\n",
      "        671,  672,  673,  674,  675,  676,  677,  678,  679,  680,  681,\n",
      "        682,  683,  684,  685,  686,  687,  688,  689,  690,  691,  692,\n",
      "        693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
      "        704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,\n",
      "        715,  716,  717,  718,  719,  960,  961,  962,  963,  964,  965,\n",
      "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
      "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
      "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
      "        999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009,\n",
      "       1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,\n",
      "       1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031,\n",
      "       1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042,\n",
      "       1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053,\n",
      "       1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064,\n",
      "       1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "       1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086,\n",
      "       1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097,\n",
      "       1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n",
      "       1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "       1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130,\n",
      "       1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141,\n",
      "       1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152,\n",
      "       1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163,\n",
      "       1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174,\n",
      "       1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
      "       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
      "       1197, 1198, 1199]), 'Validation data:', array([720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
      "       733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745,\n",
      "       746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758,\n",
      "       759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771,\n",
      "       772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784,\n",
      "       785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "       798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810,\n",
      "       811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823,\n",
      "       824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836,\n",
      "       837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849,\n",
      "       850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862,\n",
      "       863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875,\n",
      "       876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888,\n",
      "       889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901,\n",
      "       902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914,\n",
      "       915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927,\n",
      "       928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940,\n",
      "       941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953,\n",
      "       954, 955, 956, 957, 958, 959]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
      "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
      "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
      "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
      "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
      "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
      "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
      "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
      "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
      "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
      "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
      "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
      "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
      "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
      "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
      "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
      "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
      "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
      "       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
      "       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
      "       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
      "       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
      "       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
      "       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
      "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
      "       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n",
      "       832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n",
      "       845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n",
      "       858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n",
      "       871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n",
      "       884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n",
      "       897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n",
      "       910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n",
      "       923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n",
      "       936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n",
      "       949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959]), 'Validation data:', array([ 960,  961,  962,  963,  964,  965,  966,  967,  968,  969,  970,\n",
      "        971,  972,  973,  974,  975,  976,  977,  978,  979,  980,  981,\n",
      "        982,  983,  984,  985,  986,  987,  988,  989,  990,  991,  992,\n",
      "        993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
      "       1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014,\n",
      "       1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025,\n",
      "       1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036,\n",
      "       1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047,\n",
      "       1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058,\n",
      "       1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069,\n",
      "       1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080,\n",
      "       1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091,\n",
      "       1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102,\n",
      "       1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113,\n",
      "       1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124,\n",
      "       1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
      "       1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146,\n",
      "       1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,\n",
      "       1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168,\n",
      "       1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179,\n",
      "       1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190,\n",
      "       1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikita/opt/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test cvscores:', array([0.17886179, 0.1875    , 0.14166667, 0.17226891, 0.20762712]))\n",
      "{'score_time': array([0.00991583, 0.01011682, 0.00980496, 0.00913095, 0.01132512]), 'test_score': array([0.17886179, 0.1875    , 0.14166667, 0.17226891, 0.20762712]), 'train_score': array([0.28301887, 0.29479167, 0.28125   , 0.27546778, 0.26659751]), 'fit_time': array([0.000633  , 0.00093722, 0.00074196, 0.00060511, 0.00081897])}\n",
      "('mean test cvscores for k = 19', 0.1775848962983292)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.17479675, 0.16666667, 0.15      , 0.16806723, 0.20338983]))\n",
      "{'score_time': array([0.0082891 , 0.00808406, 0.00825691, 0.00815201, 0.00804114]), 'test_score': array([0.17479675, 0.16666667, 0.15      , 0.16806723, 0.20338983]), 'train_score': array([0.28092243, 0.284375  , 0.28645833, 0.27754678, 0.26970954]), 'fit_time': array([0.00056911, 0.00052309, 0.00057006, 0.00053883, 0.00054097])}\n",
      "('mean test cvscores for k = 20', 0.17258409440667546)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.16260163, 0.17916667, 0.14583333, 0.18907563, 0.21610169]))\n",
      "{'score_time': array([0.01108694, 0.00818896, 0.00840402, 0.00812697, 0.00812292]), 'test_score': array([0.16260163, 0.17916667, 0.14583333, 0.18907563, 0.21610169]), 'train_score': array([0.27777778, 0.290625  , 0.27708333, 0.27442827, 0.26970954]), 'fit_time': array([0.00061822, 0.00053906, 0.00051594, 0.00055814, 0.00057912])}\n",
      "('mean test cvscores for k = 21', 0.17855579023672305)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.17479675, 0.16666667, 0.15416667, 0.21428571, 0.21610169]))\n",
      "{'score_time': array([0.01048517, 0.00827098, 0.00821114, 0.00815606, 0.00813007]), 'test_score': array([0.17479675, 0.16666667, 0.15416667, 0.21428571, 0.21610169]), 'train_score': array([0.27987421, 0.2875    , 0.27395833, 0.26611227, 0.26659751]), 'fit_time': array([0.00097084, 0.00055695, 0.00052094, 0.00054884, 0.000561  ])}\n",
      "('mean test cvscores for k = 22', 0.1852034981003563)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.15853659, 0.17916667, 0.15833333, 0.20168067, 0.20338983]))\n",
      "{'score_time': array([0.00890589, 0.00844002, 0.00842094, 0.00827718, 0.00846195]), 'test_score': array([0.15853659, 0.17916667, 0.15833333, 0.20168067, 0.20338983]), 'train_score': array([0.28301887, 0.28958333, 0.26979167, 0.28066528, 0.26556017]), 'fit_time': array([0.00060892, 0.00071192, 0.00058007, 0.00056696, 0.00067496])}\n",
      "('mean test cvscores for k = 23', 0.18022141762864716)\n"
     ]
    }
   ],
   "source": [
    "#Task 5: KNN Model Development with Suite-1\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Model Development on Suite1 i.e. 80 - 20 partition\n",
    "\n",
    "#Performing 5-fold cross validation within the training data\n",
    "kfCV = KFold(n_splits=5)\n",
    "split = kfCV.split(X_suite1_train)\n",
    "\n",
    "#printing the actual training data and validation data in each fold\n",
    "for trData, vlData in kfCV.split(X_suite1_train):\n",
    "   print(\"[Fold]\", 'Actual Training data:', trData, 'Validation data:', vlData)\n",
    "\n",
    "#Proposing 5 possible values for n_neighbors as 19,20,21,22 and 23\n",
    "\n",
    "#Obtaining the performance on each fold for k = 19\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "s = cross_validate(knn, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 19', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 20\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "s = cross_validate(knn, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 20', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 21\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "s = cross_validate(knn, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 21', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 22\n",
    "knn = KNeighborsClassifier(n_neighbors=22)\n",
    "s = cross_validate(knn,X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 22', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "#Obtaining the performance on each fold for k = 23\n",
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "s = cross_validate(knn,X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 23', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "#Based on the results for suite-1 , model has highest performance for k = 22 validation accuracy- 18.52%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Fold]', 'Actual Training data:', array([180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
      "       193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "       206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218,\n",
      "       219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
      "       232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
      "       245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257,\n",
      "       258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270,\n",
      "       271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "       284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296,\n",
      "       297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "       323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "       336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
      "       349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361,\n",
      "       362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
      "       375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387,\n",
      "       388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
      "       401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413,\n",
      "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
      "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
      "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
      "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
      "       492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
      "       505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "       518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530,\n",
      "       531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543,\n",
      "       544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556,\n",
      "       557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569,\n",
      "       570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582,\n",
      "       583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
      "       596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608,\n",
      "       609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621,\n",
      "       622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
      "       635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673,\n",
      "       674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686,\n",
      "       687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "       700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
      "       713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725,\n",
      "       726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738,\n",
      "       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n",
      "       752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764,\n",
      "       765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,\n",
      "       778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790,\n",
      "       791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803,\n",
      "       804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816,\n",
      "       817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829,\n",
      "       830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842,\n",
      "       843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855,\n",
      "       856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868,\n",
      "       869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "       882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894,\n",
      "       895, 896, 897, 898, 899]), 'Validation data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 360, 361,\n",
      "       362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
      "       375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387,\n",
      "       388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
      "       401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413,\n",
      "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
      "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
      "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
      "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
      "       492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
      "       505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "       518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530,\n",
      "       531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543,\n",
      "       544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556,\n",
      "       557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569,\n",
      "       570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582,\n",
      "       583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
      "       596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608,\n",
      "       609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621,\n",
      "       622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
      "       635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673,\n",
      "       674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686,\n",
      "       687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "       700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
      "       713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725,\n",
      "       726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738,\n",
      "       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n",
      "       752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764,\n",
      "       765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,\n",
      "       778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790,\n",
      "       791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803,\n",
      "       804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816,\n",
      "       817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829,\n",
      "       830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842,\n",
      "       843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855,\n",
      "       856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868,\n",
      "       869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "       882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894,\n",
      "       895, 896, 897, 898, 899]), 'Validation data:', array([180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
      "       193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "       206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218,\n",
      "       219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
      "       232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
      "       245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257,\n",
      "       258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270,\n",
      "       271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "       284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296,\n",
      "       297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "       323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "       336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
      "       349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 540, 541, 542, 543,\n",
      "       544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556,\n",
      "       557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569,\n",
      "       570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582,\n",
      "       583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
      "       596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608,\n",
      "       609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621,\n",
      "       622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
      "       635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673,\n",
      "       674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686,\n",
      "       687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "       700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
      "       713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725,\n",
      "       726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738,\n",
      "       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n",
      "       752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764,\n",
      "       765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,\n",
      "       778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790,\n",
      "       791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803,\n",
      "       804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816,\n",
      "       817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829,\n",
      "       830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842,\n",
      "       843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855,\n",
      "       856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868,\n",
      "       869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "       882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894,\n",
      "       895, 896, 897, 898, 899]), 'Validation data:', array([360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n",
      "       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
      "       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n",
      "       399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "       412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424,\n",
      "       425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
      "       438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n",
      "       451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463,\n",
      "       464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
      "       477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
      "       503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
      "       516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528,\n",
      "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
      "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
      "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
      "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
      "       533, 534, 535, 536, 537, 538, 539, 720, 721, 722, 723, 724, 725,\n",
      "       726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738,\n",
      "       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n",
      "       752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764,\n",
      "       765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777,\n",
      "       778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790,\n",
      "       791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803,\n",
      "       804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816,\n",
      "       817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829,\n",
      "       830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842,\n",
      "       843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855,\n",
      "       856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868,\n",
      "       869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "       882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894,\n",
      "       895, 896, 897, 898, 899]), 'Validation data:', array([540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552,\n",
      "       553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565,\n",
      "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
      "       579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591,\n",
      "       592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604,\n",
      "       605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617,\n",
      "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
      "       631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "       644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656,\n",
      "       657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669,\n",
      "       670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682,\n",
      "       683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,\n",
      "       696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708,\n",
      "       709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
      "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
      "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
      "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
      "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
      "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
      "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
      "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
      "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
      "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
      "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
      "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
      "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
      "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
      "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
      "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
      "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
      "       715, 716, 717, 718, 719]), 'Validation data:', array([720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
      "       733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745,\n",
      "       746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758,\n",
      "       759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771,\n",
      "       772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784,\n",
      "       785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "       798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810,\n",
      "       811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823,\n",
      "       824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836,\n",
      "       837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849,\n",
      "       850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862,\n",
      "       863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875,\n",
      "       876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888,\n",
      "       889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test cvscores:', array([0.17837838, 0.13186813, 0.17222222, 0.16384181, 0.1875    ]))\n",
      "{'score_time': array([0.00698805, 0.00678492, 0.00659704, 0.00647497, 0.00926518]), 'test_score': array([0.17837838, 0.13186813, 0.17222222, 0.16384181, 0.1875    ]), 'train_score': array([0.32027972, 0.2729805 , 0.29722222, 0.31120332, 0.27348066]), 'fit_time': array([0.00058103, 0.00060296, 0.00052094, 0.00052905, 0.00047588])}\n",
      "('mean test cvscores for k = 19', 0.1667621080756674)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.17297297, 0.15384615, 0.16666667, 0.15254237, 0.19318182]))\n",
      "{'score_time': array([0.00720882, 0.00725794, 0.00741291, 0.0064199 , 0.00596595]), 'test_score': array([0.17297297, 0.15384615, 0.16666667, 0.15254237, 0.19318182]), 'train_score': array([0.31468531, 0.29526462, 0.28611111, 0.29598893, 0.28314917]), 'fit_time': array([0.00056815, 0.00064206, 0.00064611, 0.00055099, 0.00051308])}\n",
      "('mean test cvscores for k = 20', 0.1678419969097935)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.17837838, 0.14285714, 0.17777778, 0.15254237, 0.20454545]))\n",
      "{'score_time': array([0.00874686, 0.00635695, 0.00624299, 0.00619316, 0.00615811]), 'test_score': array([0.17837838, 0.14285714, 0.17777778, 0.15254237, 0.20454545]), 'train_score': array([0.2979021 , 0.29526462, 0.2875    , 0.29322268, 0.28176796]), 'fit_time': array([0.00050807, 0.00053   , 0.00048995, 0.00048494, 0.00047302])}\n",
      "('mean test cvscores for k = 21', 0.17122022528802192)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.18378378, 0.13736264, 0.19444444, 0.15819209, 0.18181818]))\n",
      "{'score_time': array([0.00658607, 0.00818396, 0.00642395, 0.00620914, 0.00615501]), 'test_score': array([0.18378378, 0.13736264, 0.19444444, 0.15819209, 0.18181818]), 'train_score': array([0.29230769, 0.29247911, 0.29444444, 0.28907331, 0.26933702]), 'fit_time': array([0.00053287, 0.00050592, 0.00053906, 0.00052381, 0.00048184])}\n",
      "('mean test cvscores for k = 22', 0.17112022756090553)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.19459459, 0.14285714, 0.18888889, 0.13559322, 0.17613636]))\n",
      "{'score_time': array([0.00664401, 0.00892997, 0.00651002, 0.00628686, 0.00651288]), 'test_score': array([0.19459459, 0.14285714, 0.18888889, 0.13559322, 0.17613636]), 'train_score': array([0.28391608, 0.27855153, 0.28333333, 0.28077455, 0.26104972]), 'fit_time': array([0.00052381, 0.00097919, 0.00051403, 0.00055218, 0.00048208])}\n",
      "('mean test cvscores for k = 23', 0.16761404206319458)\n"
     ]
    }
   ],
   "source": [
    "#Model Development on Suite2 \n",
    "\n",
    "#Performing 5-fold cross validation within the training data\n",
    "kfCV = KFold(n_splits=5)\n",
    "split = kfCV.split(X_suite2_train)\n",
    "\n",
    "#printing the actual training data and validation data in each fold\n",
    "for trData, vlData in kfCV.split(X_suite2_train):\n",
    "   print(\"[Fold]\", 'Actual Training data:', trData, 'Validation data:', vlData)\n",
    "\n",
    "#Proposing 5 possible values for n_neighbors as 19,20,21,22 and 23\n",
    "\n",
    "#Obtaining the performance on each fold for k = 19\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "s = cross_validate(knn, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 19', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 20\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "s = cross_validate(knn, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 20', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 21\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "s = cross_validate(knn, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 21', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 22\n",
    "knn = KNeighborsClassifier(n_neighbors=22)\n",
    "s = cross_validate(knn, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 22', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "#Obtaining the performance on each fold for k = 23\n",
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "s = cross_validate(knn, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 23', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "#Based on the results for suite-2 , model has highest performance for k = 21 validation accuracy- 17.12%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Fold]', 'Actual Training data:', array([ 210,  211,  212,  213,  214,  215,  216,  217,  218,  219,  220,\n",
      "        221,  222,  223,  224,  225,  226,  227,  228,  229,  230,  231,\n",
      "        232,  233,  234,  235,  236,  237,  238,  239,  240,  241,  242,\n",
      "        243,  244,  245,  246,  247,  248,  249,  250,  251,  252,  253,\n",
      "        254,  255,  256,  257,  258,  259,  260,  261,  262,  263,  264,\n",
      "        265,  266,  267,  268,  269,  270,  271,  272,  273,  274,  275,\n",
      "        276,  277,  278,  279,  280,  281,  282,  283,  284,  285,  286,\n",
      "        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  297,\n",
      "        298,  299,  300,  301,  302,  303,  304,  305,  306,  307,  308,\n",
      "        309,  310,  311,  312,  313,  314,  315,  316,  317,  318,  319,\n",
      "        320,  321,  322,  323,  324,  325,  326,  327,  328,  329,  330,\n",
      "        331,  332,  333,  334,  335,  336,  337,  338,  339,  340,  341,\n",
      "        342,  343,  344,  345,  346,  347,  348,  349,  350,  351,  352,\n",
      "        353,  354,  355,  356,  357,  358,  359,  360,  361,  362,  363,\n",
      "        364,  365,  366,  367,  368,  369,  370,  371,  372,  373,  374,\n",
      "        375,  376,  377,  378,  379,  380,  381,  382,  383,  384,  385,\n",
      "        386,  387,  388,  389,  390,  391,  392,  393,  394,  395,  396,\n",
      "        397,  398,  399,  400,  401,  402,  403,  404,  405,  406,  407,\n",
      "        408,  409,  410,  411,  412,  413,  414,  415,  416,  417,  418,\n",
      "        419,  420,  421,  422,  423,  424,  425,  426,  427,  428,  429,\n",
      "        430,  431,  432,  433,  434,  435,  436,  437,  438,  439,  440,\n",
      "        441,  442,  443,  444,  445,  446,  447,  448,  449,  450,  451,\n",
      "        452,  453,  454,  455,  456,  457,  458,  459,  460,  461,  462,\n",
      "        463,  464,  465,  466,  467,  468,  469,  470,  471,  472,  473,\n",
      "        474,  475,  476,  477,  478,  479,  480,  481,  482,  483,  484,\n",
      "        485,  486,  487,  488,  489,  490,  491,  492,  493,  494,  495,\n",
      "        496,  497,  498,  499,  500,  501,  502,  503,  504,  505,  506,\n",
      "        507,  508,  509,  510,  511,  512,  513,  514,  515,  516,  517,\n",
      "        518,  519,  520,  521,  522,  523,  524,  525,  526,  527,  528,\n",
      "        529,  530,  531,  532,  533,  534,  535,  536,  537,  538,  539,\n",
      "        540,  541,  542,  543,  544,  545,  546,  547,  548,  549,  550,\n",
      "        551,  552,  553,  554,  555,  556,  557,  558,  559,  560,  561,\n",
      "        562,  563,  564,  565,  566,  567,  568,  569,  570,  571,  572,\n",
      "        573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
      "        584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,\n",
      "        595,  596,  597,  598,  599,  600,  601,  602,  603,  604,  605,\n",
      "        606,  607,  608,  609,  610,  611,  612,  613,  614,  615,  616,\n",
      "        617,  618,  619,  620,  621,  622,  623,  624,  625,  626,  627,\n",
      "        628,  629,  630,  631,  632,  633,  634,  635,  636,  637,  638,\n",
      "        639,  640,  641,  642,  643,  644,  645,  646,  647,  648,  649,\n",
      "        650,  651,  652,  653,  654,  655,  656,  657,  658,  659,  660,\n",
      "        661,  662,  663,  664,  665,  666,  667,  668,  669,  670,  671,\n",
      "        672,  673,  674,  675,  676,  677,  678,  679,  680,  681,  682,\n",
      "        683,  684,  685,  686,  687,  688,  689,  690,  691,  692,  693,\n",
      "        694,  695,  696,  697,  698,  699,  700,  701,  702,  703,  704,\n",
      "        705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
      "        716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,\n",
      "        727,  728,  729,  730,  731,  732,  733,  734,  735,  736,  737,\n",
      "        738,  739,  740,  741,  742,  743,  744,  745,  746,  747,  748,\n",
      "        749,  750,  751,  752,  753,  754,  755,  756,  757,  758,  759,\n",
      "        760,  761,  762,  763,  764,  765,  766,  767,  768,  769,  770,\n",
      "        771,  772,  773,  774,  775,  776,  777,  778,  779,  780,  781,\n",
      "        782,  783,  784,  785,  786,  787,  788,  789,  790,  791,  792,\n",
      "        793,  794,  795,  796,  797,  798,  799,  800,  801,  802,  803,\n",
      "        804,  805,  806,  807,  808,  809,  810,  811,  812,  813,  814,\n",
      "        815,  816,  817,  818,  819,  820,  821,  822,  823,  824,  825,\n",
      "        826,  827,  828,  829,  830,  831,  832,  833,  834,  835,  836,\n",
      "        837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "        848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,\n",
      "        859,  860,  861,  862,  863,  864,  865,  866,  867,  868,  869,\n",
      "        870,  871,  872,  873,  874,  875,  876,  877,  878,  879,  880,\n",
      "        881,  882,  883,  884,  885,  886,  887,  888,  889,  890,  891,\n",
      "        892,  893,  894,  895,  896,  897,  898,  899,  900,  901,  902,\n",
      "        903,  904,  905,  906,  907,  908,  909,  910,  911,  912,  913,\n",
      "        914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
      "        925,  926,  927,  928,  929,  930,  931,  932,  933,  934,  935,\n",
      "        936,  937,  938,  939,  940,  941,  942,  943,  944,  945,  946,\n",
      "        947,  948,  949,  950,  951,  952,  953,  954,  955,  956,  957,\n",
      "        958,  959,  960,  961,  962,  963,  964,  965,  966,  967,  968,\n",
      "        969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "        980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,\n",
      "        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001,\n",
      "       1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
      "       1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023,\n",
      "       1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045,\n",
      "       1046, 1047, 1048, 1049]), 'Validation data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209]))\n",
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  420,  421,  422,  423,  424,  425,  426,  427,  428,  429,\n",
      "        430,  431,  432,  433,  434,  435,  436,  437,  438,  439,  440,\n",
      "        441,  442,  443,  444,  445,  446,  447,  448,  449,  450,  451,\n",
      "        452,  453,  454,  455,  456,  457,  458,  459,  460,  461,  462,\n",
      "        463,  464,  465,  466,  467,  468,  469,  470,  471,  472,  473,\n",
      "        474,  475,  476,  477,  478,  479,  480,  481,  482,  483,  484,\n",
      "        485,  486,  487,  488,  489,  490,  491,  492,  493,  494,  495,\n",
      "        496,  497,  498,  499,  500,  501,  502,  503,  504,  505,  506,\n",
      "        507,  508,  509,  510,  511,  512,  513,  514,  515,  516,  517,\n",
      "        518,  519,  520,  521,  522,  523,  524,  525,  526,  527,  528,\n",
      "        529,  530,  531,  532,  533,  534,  535,  536,  537,  538,  539,\n",
      "        540,  541,  542,  543,  544,  545,  546,  547,  548,  549,  550,\n",
      "        551,  552,  553,  554,  555,  556,  557,  558,  559,  560,  561,\n",
      "        562,  563,  564,  565,  566,  567,  568,  569,  570,  571,  572,\n",
      "        573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
      "        584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,\n",
      "        595,  596,  597,  598,  599,  600,  601,  602,  603,  604,  605,\n",
      "        606,  607,  608,  609,  610,  611,  612,  613,  614,  615,  616,\n",
      "        617,  618,  619,  620,  621,  622,  623,  624,  625,  626,  627,\n",
      "        628,  629,  630,  631,  632,  633,  634,  635,  636,  637,  638,\n",
      "        639,  640,  641,  642,  643,  644,  645,  646,  647,  648,  649,\n",
      "        650,  651,  652,  653,  654,  655,  656,  657,  658,  659,  660,\n",
      "        661,  662,  663,  664,  665,  666,  667,  668,  669,  670,  671,\n",
      "        672,  673,  674,  675,  676,  677,  678,  679,  680,  681,  682,\n",
      "        683,  684,  685,  686,  687,  688,  689,  690,  691,  692,  693,\n",
      "        694,  695,  696,  697,  698,  699,  700,  701,  702,  703,  704,\n",
      "        705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
      "        716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,\n",
      "        727,  728,  729,  730,  731,  732,  733,  734,  735,  736,  737,\n",
      "        738,  739,  740,  741,  742,  743,  744,  745,  746,  747,  748,\n",
      "        749,  750,  751,  752,  753,  754,  755,  756,  757,  758,  759,\n",
      "        760,  761,  762,  763,  764,  765,  766,  767,  768,  769,  770,\n",
      "        771,  772,  773,  774,  775,  776,  777,  778,  779,  780,  781,\n",
      "        782,  783,  784,  785,  786,  787,  788,  789,  790,  791,  792,\n",
      "        793,  794,  795,  796,  797,  798,  799,  800,  801,  802,  803,\n",
      "        804,  805,  806,  807,  808,  809,  810,  811,  812,  813,  814,\n",
      "        815,  816,  817,  818,  819,  820,  821,  822,  823,  824,  825,\n",
      "        826,  827,  828,  829,  830,  831,  832,  833,  834,  835,  836,\n",
      "        837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "        848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,\n",
      "        859,  860,  861,  862,  863,  864,  865,  866,  867,  868,  869,\n",
      "        870,  871,  872,  873,  874,  875,  876,  877,  878,  879,  880,\n",
      "        881,  882,  883,  884,  885,  886,  887,  888,  889,  890,  891,\n",
      "        892,  893,  894,  895,  896,  897,  898,  899,  900,  901,  902,\n",
      "        903,  904,  905,  906,  907,  908,  909,  910,  911,  912,  913,\n",
      "        914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
      "        925,  926,  927,  928,  929,  930,  931,  932,  933,  934,  935,\n",
      "        936,  937,  938,  939,  940,  941,  942,  943,  944,  945,  946,\n",
      "        947,  948,  949,  950,  951,  952,  953,  954,  955,  956,  957,\n",
      "        958,  959,  960,  961,  962,  963,  964,  965,  966,  967,  968,\n",
      "        969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "        980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,\n",
      "        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001,\n",
      "       1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
      "       1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023,\n",
      "       1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045,\n",
      "       1046, 1047, 1048, 1049]), 'Validation data:', array([210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
      "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
      "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
      "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "       288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300,\n",
      "       301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "       314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326,\n",
      "       327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "       340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
      "       353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
      "       366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
      "       379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
      "       405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
      "       418, 419]))\n",
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,\n",
      "        220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,\n",
      "        231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,\n",
      "        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,\n",
      "        253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,\n",
      "        264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,\n",
      "        275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,\n",
      "        286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,\n",
      "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,\n",
      "        308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,\n",
      "        319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,\n",
      "        330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,\n",
      "        341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,\n",
      "        352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,\n",
      "        363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,\n",
      "        374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,\n",
      "        385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,\n",
      "        396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,\n",
      "        407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,\n",
      "        418,  419,  630,  631,  632,  633,  634,  635,  636,  637,  638,\n",
      "        639,  640,  641,  642,  643,  644,  645,  646,  647,  648,  649,\n",
      "        650,  651,  652,  653,  654,  655,  656,  657,  658,  659,  660,\n",
      "        661,  662,  663,  664,  665,  666,  667,  668,  669,  670,  671,\n",
      "        672,  673,  674,  675,  676,  677,  678,  679,  680,  681,  682,\n",
      "        683,  684,  685,  686,  687,  688,  689,  690,  691,  692,  693,\n",
      "        694,  695,  696,  697,  698,  699,  700,  701,  702,  703,  704,\n",
      "        705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
      "        716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,\n",
      "        727,  728,  729,  730,  731,  732,  733,  734,  735,  736,  737,\n",
      "        738,  739,  740,  741,  742,  743,  744,  745,  746,  747,  748,\n",
      "        749,  750,  751,  752,  753,  754,  755,  756,  757,  758,  759,\n",
      "        760,  761,  762,  763,  764,  765,  766,  767,  768,  769,  770,\n",
      "        771,  772,  773,  774,  775,  776,  777,  778,  779,  780,  781,\n",
      "        782,  783,  784,  785,  786,  787,  788,  789,  790,  791,  792,\n",
      "        793,  794,  795,  796,  797,  798,  799,  800,  801,  802,  803,\n",
      "        804,  805,  806,  807,  808,  809,  810,  811,  812,  813,  814,\n",
      "        815,  816,  817,  818,  819,  820,  821,  822,  823,  824,  825,\n",
      "        826,  827,  828,  829,  830,  831,  832,  833,  834,  835,  836,\n",
      "        837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "        848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,\n",
      "        859,  860,  861,  862,  863,  864,  865,  866,  867,  868,  869,\n",
      "        870,  871,  872,  873,  874,  875,  876,  877,  878,  879,  880,\n",
      "        881,  882,  883,  884,  885,  886,  887,  888,  889,  890,  891,\n",
      "        892,  893,  894,  895,  896,  897,  898,  899,  900,  901,  902,\n",
      "        903,  904,  905,  906,  907,  908,  909,  910,  911,  912,  913,\n",
      "        914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
      "        925,  926,  927,  928,  929,  930,  931,  932,  933,  934,  935,\n",
      "        936,  937,  938,  939,  940,  941,  942,  943,  944,  945,  946,\n",
      "        947,  948,  949,  950,  951,  952,  953,  954,  955,  956,  957,\n",
      "        958,  959,  960,  961,  962,  963,  964,  965,  966,  967,  968,\n",
      "        969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "        980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,\n",
      "        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001,\n",
      "       1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
      "       1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023,\n",
      "       1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045,\n",
      "       1046, 1047, 1048, 1049]), 'Validation data:', array([420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
      "       446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
      "       459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
      "       472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,\n",
      "       485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
      "       498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510,\n",
      "       511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523,\n",
      "       524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536,\n",
      "       537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549,\n",
      "       550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,\n",
      "       563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575,\n",
      "       576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588,\n",
      "       589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
      "       602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614,\n",
      "       615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627,\n",
      "       628, 629]))\n",
      "('[Fold]', 'Actual Training data:', array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
      "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
      "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
      "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
      "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
      "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
      "         66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
      "         77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
      "         88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,\n",
      "         99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,\n",
      "        121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
      "        132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,\n",
      "        154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,\n",
      "        165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,\n",
      "        176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,\n",
      "        187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,\n",
      "        209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,\n",
      "        220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,\n",
      "        231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,\n",
      "        242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,\n",
      "        253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,\n",
      "        264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,\n",
      "        275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,\n",
      "        286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,\n",
      "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,\n",
      "        308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,\n",
      "        319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,\n",
      "        330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,\n",
      "        341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,\n",
      "        352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,\n",
      "        363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,\n",
      "        374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,\n",
      "        385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,\n",
      "        396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,\n",
      "        407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,\n",
      "        418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,\n",
      "        429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,\n",
      "        440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,\n",
      "        451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,\n",
      "        462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,\n",
      "        473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,\n",
      "        484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,\n",
      "        495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,\n",
      "        506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,\n",
      "        517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,\n",
      "        528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,\n",
      "        539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,\n",
      "        550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,\n",
      "        561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
      "        572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,\n",
      "        583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,\n",
      "        594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,\n",
      "        605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,\n",
      "        616,  617,  618,  619,  620,  621,  622,  623,  624,  625,  626,\n",
      "        627,  628,  629,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "        848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,\n",
      "        859,  860,  861,  862,  863,  864,  865,  866,  867,  868,  869,\n",
      "        870,  871,  872,  873,  874,  875,  876,  877,  878,  879,  880,\n",
      "        881,  882,  883,  884,  885,  886,  887,  888,  889,  890,  891,\n",
      "        892,  893,  894,  895,  896,  897,  898,  899,  900,  901,  902,\n",
      "        903,  904,  905,  906,  907,  908,  909,  910,  911,  912,  913,\n",
      "        914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
      "        925,  926,  927,  928,  929,  930,  931,  932,  933,  934,  935,\n",
      "        936,  937,  938,  939,  940,  941,  942,  943,  944,  945,  946,\n",
      "        947,  948,  949,  950,  951,  952,  953,  954,  955,  956,  957,\n",
      "        958,  959,  960,  961,  962,  963,  964,  965,  966,  967,  968,\n",
      "        969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "        980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,\n",
      "        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001,\n",
      "       1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
      "       1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023,\n",
      "       1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045,\n",
      "       1046, 1047, 1048, 1049]), 'Validation data:', array([630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824,\n",
      "       825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837,\n",
      "       838, 839]))\n",
      "('[Fold]', 'Actual Training data:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
      "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
      "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
      "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
      "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
      "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
      "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
      "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
      "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
      "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
      "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
      "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
      "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
      "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
      "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
      "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
      "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
      "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
      "       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
      "       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
      "       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
      "       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
      "       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
      "       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
      "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
      "       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n",
      "       832, 833, 834, 835, 836, 837, 838, 839]), 'Validation data:', array([ 840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test cvscores:', array([0.15023474, 0.18779343, 0.17619048, 0.16746411, 0.18536585]))\n",
      "{'score_time': array([0.00748682, 0.00717211, 0.00717497, 0.007164  , 0.007761  ]), 'test_score': array([0.15023474, 0.18779343, 0.17619048, 0.16746411, 0.18536585]), 'train_score': array([0.29151732, 0.27479092, 0.28571429, 0.29726516, 0.28639053]), 'fit_time': array([0.00054312, 0.00051785, 0.00052404, 0.00052691, 0.00056601])}\n",
      "('mean test cvscores for k = 19', 0.17340972273912664)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.15492958, 0.18779343, 0.17619048, 0.19617225, 0.17560976]))\n",
      "{'score_time': array([0.00851893, 0.01148105, 0.00749803, 0.00716901, 0.00721693]), 'test_score': array([0.15492958, 0.18779343, 0.17619048, 0.19617225, 0.17560976]), 'train_score': array([0.30585424, 0.28315412, 0.2952381 , 0.28775268, 0.27810651]), 'fit_time': array([0.00068212, 0.00059915, 0.00057197, 0.000494  , 0.0005312 ])}\n",
      "('mean test cvscores for k = 20', 0.17813909715734014)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.15962441, 0.1971831 , 0.16190476, 0.19138756, 0.17560976]))\n",
      "{'score_time': array([0.00918794, 0.00760293, 0.00713515, 0.00708079, 0.00700593]), 'test_score': array([0.15962441, 0.1971831 , 0.16190476, 0.19138756, 0.17560976]), 'train_score': array([0.30346476, 0.27598566, 0.28928571, 0.28180737, 0.2816568 ]), 'fit_time': array([0.00060892, 0.00053501, 0.00049496, 0.00048518, 0.000494  ])}\n",
      "('mean test cvscores for k = 21', 0.1771419179096049)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.12676056, 0.20657277, 0.16190476, 0.17703349, 0.1804878 ]))\n",
      "{'score_time': array([0.00773001, 0.0081389 , 0.00724006, 0.00728798, 0.00715113]), 'test_score': array([0.12676056, 0.20657277, 0.16190476, 0.17703349, 0.1804878 ]), 'train_score': array([0.29271207, 0.27718041, 0.29047619, 0.28061831, 0.28402367]), 'fit_time': array([0.00054693, 0.00054812, 0.00053906, 0.00051498, 0.00050497])}\n",
      "('mean test cvscores for k = 22', 0.17055187858782211)\n",
      "------------------------------------------\n",
      "('test cvscores:', array([0.14084507, 0.20657277, 0.14285714, 0.17703349, 0.17073171]))\n",
      "{'score_time': array([0.00754595, 0.00746202, 0.00728893, 0.00726604, 0.00710392]), 'test_score': array([0.14084507, 0.20657277, 0.14285714, 0.17703349, 0.17073171]), 'train_score': array([0.28793309, 0.28912784, 0.2797619 , 0.30083234, 0.27692308]), 'fit_time': array([0.00055504, 0.00059295, 0.00050902, 0.00050807, 0.0004921 ])}\n",
      "('mean test cvscores for k = 23', 0.16760803667455387)\n"
     ]
    }
   ],
   "source": [
    "# KNN Model Development on Suite3\n",
    "\n",
    "#Performing 5-fold cross validation within the training data\n",
    "kfCV = KFold(n_splits=5)\n",
    "split = kfCV.split(X_suite3_train)\n",
    "\n",
    "#printing the actual training data and validation data in each fold\n",
    "for trData, vlData in kfCV.split(X_suite3_train):\n",
    "   print(\"[Fold]\", 'Actual Training data:', trData, 'Validation data:', vlData)\n",
    "\n",
    "#Proposing 5 possible values for n_neighbors as 19,20,21,22 and 23\n",
    "\n",
    "#Obtaining the performance on each fold for k = 19\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "s = cross_validate(knn, X_suite3_train, y_suite3_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 19', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 20\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "s = cross_validate(knn,  X_suite3_train, y_suite3_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 20', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 21\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "s = cross_validate(knn,  X_suite3_train, y_suite3_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 21', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "#Obtaining the performance on each fold for k = 22\n",
    "knn = KNeighborsClassifier(n_neighbors=22)\n",
    "s = cross_validate(knn,  X_suite3_train, y_suite3_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 22', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "#Obtaining the performance on each fold for k = 23\n",
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "s = cross_validate(knn,  X_suite3_train, y_suite3_train, cv=5, scoring='accuracy')\n",
    "print('test cvscores:', s['test_score'] )\n",
    "print(s)\n",
    "print ('mean test cvscores for k = 23', s['test_score'].mean()) #Calculating the mean of performance of the folds\n",
    "\n",
    "#Based on the results for suite-3 , model has highest performance for k = 20 validation accuracy- 17.81%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN has the best validation perforamce of 17.81% based on suite-3 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cvscores:', array([0.21544715, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = (2,2)', 0.2200283759638622)\n",
      "('cvscores:', array([0.21138211, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = (3,3)', 0.21921536783378087)\n",
      "('cvscores:', array([0.21138211, 0.22083333, 0.21666667, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = (5,5)', 0.21838203450044755)\n",
      "('cvscores:', array([0.19105691, 0.22083333, 0.22083333, 0.21848739, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = (10,10)', 0.21430999104892062)\n",
      "('cvscores:', array([0.21544715, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 3', 0.2200283759638622)\n",
      "('cvscores:', array([0.21544715, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 5', 0.2200283759638622)\n",
      "('cvscores:', array([0.21544715, 0.22083333, 0.21666667, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 7', 0.21919504263052886)\n",
      "('cvscores:', array([0.21544715, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 9', 0.2200283759638622)\n",
      "('cvscores:', array([0.21544715, 0.22083333, 0.22083333, 0.22268908, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 15', 0.2200283759638622)\n",
      "('cvscores:', array([0.21544715, 0.21666667, 0.21666667, 0.22689076, 0.22033898]))\n",
      "('mean cvscores for layer_sizes = 20', 0.2192020454316493)\n"
     ]
    }
   ],
   "source": [
    "# Hypertuning for the model Neural network:\n",
    "# Using the dataset Suite-1\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Hidden layer sizes: (2,2)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2,2), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (2,2)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (3,3)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,3), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (3,3)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (5,5)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,5), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (5,5)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (10,10)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,10), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (10,10)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 3\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=3, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 3', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 5\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=5, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 5', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 7\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=7, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 7', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 9\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=9, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 9', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 10\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=15, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 15', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 20\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=20, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite1_train, y_suite1_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 20', s.mean())\n",
    "\n",
    "# For suite-1 , Neural Network has highest validation accuracy of - 22.08%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (2,2)', 0.21891859896097182)\n",
      "('cvscores:', array([0.20540541, 0.21428571, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (3,3)', 0.2156575356999086)\n",
      "('cvscores:', array([0.2       , 0.21978022, 0.2       , 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (5,5)', 0.21234202238439526)\n",
      "('cvscores:', array([0.21621622, 0.20879121, 0.17222222, 0.21468927, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (10,10)', 0.2067019643714559)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 3', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 5', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 7', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21428571, 0.22222222, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 9', 0.21893080897318185)\n",
      "('cvscores:', array([0.21621622, 0.21428571, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 15', 0.21781969786207073)\n",
      "('cvscores:', array([0.21081081, 0.21978022, 0.21111111, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 20', 0.21672640676877966)\n"
     ]
    }
   ],
   "source": [
    "# Hypertuning for the model Neural network:\n",
    "# Using the dataset Suite-2\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Hidden layer sizes: (2,2)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2,2), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (2,2)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (3,3)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,3), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (3,3)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (5,5)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,5), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (5,5)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (10,10)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,10), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (10,10)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 3\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=3, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 3', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 5\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=5, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 5', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 7\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=7, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 7', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 9\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=9, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,   X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 9', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 10\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=15, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 15', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 20\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=20, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 20', s.mean())\n",
    "\n",
    "# For suite-2 , Neural Network has highest validation accuracy of - 21.89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (2,2)', 0.21891859896097182)\n",
      "('cvscores:', array([0.20540541, 0.21428571, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (3,3)', 0.2156575356999086)\n",
      "('cvscores:', array([0.2       , 0.21978022, 0.2       , 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (5,5)', 0.21234202238439526)\n",
      "('cvscores:', array([0.21621622, 0.20879121, 0.17222222, 0.21468927, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = (10,10)', 0.2067019643714559)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 3', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 5', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21978022, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 7', 0.21891859896097182)\n",
      "('cvscores:', array([0.21621622, 0.21428571, 0.22222222, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 9', 0.21893080897318185)\n",
      "('cvscores:', array([0.21621622, 0.21428571, 0.21666667, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 15', 0.21781969786207073)\n",
      "('cvscores:', array([0.21081081, 0.21978022, 0.21111111, 0.22033898, 0.22159091]))\n",
      "('mean cvscores for layer_sizes = 20', 0.21672640676877966)\n"
     ]
    }
   ],
   "source": [
    "# Hypertuning for the model Neural network:\n",
    "# Using the dataset Suite-3\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Hidden layer sizes: (2,2)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2,2), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (2,2)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (3,3)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,3), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (3,3)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (5,5)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,5), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (5,5)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: (10,10)\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,10), random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = (10,10)', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 3\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=3, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 3', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 5\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=5, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,  X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 5', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 7\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=7, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 7', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 9\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=9, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP,   X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 9', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 10\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=15, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 15', s.mean())\n",
    "\n",
    "# Hidden layer sizes: 20\n",
    "MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=20, random_state=1)\n",
    "\n",
    "s = cross_val_score(MLP, X_suite2_train, y_suite2_train, cv=5, scoring='accuracy')\n",
    "print('cvscores:', s )\n",
    "print ('mean cvscores for layer_sizes = 20', s.mean())\n",
    "\n",
    "# For suite-3 , Neural Network has highest validation accuracy of - 21.89%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network has highest validation accuracy of - 22.08% for both Suite-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-6 Performing Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of three classifier, Random Forest classifier has the best validation performance of 22.32% with suite=2\n",
    "#### Based on this result we have decided to select Random Forest as the best classifier for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2      3     4  predicted\n",
      "0  193.0 -0.263082 -0.888493  141.0  17.0          8\n",
      "1  345.0  1.304070 -0.249749   59.0  75.0          8\n",
      "2   24.0 -0.787871  0.069623   17.0  87.0          9\n",
      "3  162.0 -1.783346 -1.207865   98.0  39.0          6\n",
      "4  188.0 -0.838366 -1.846610    1.0   4.0          8\n",
      "(600, 6)\n",
      "\n",
      "\n",
      "Model Performance Statistic Suite-2: \n",
      "Accuracy:  0.235\n",
      "Precision:  0.235\n",
      "Recall:  0.235\n",
      "F1 Score:  0.235\n",
      "Confusion Metrix:\n",
      "[[  0   1   0   0   0   1   1   1   0   0   0]\n",
      " [  0   0   0   0   0   2   1   0   0   0   0]\n",
      " [  0   1   0   0   0   3   0   3   0   0   0]\n",
      " [  0   0   0   0   0   9   0  19   0   0   0]\n",
      " [  0   0   0   0   0  19   1  48   0   0   0]\n",
      " [  0   0   0   0   0  36   2  79   0   0   0]\n",
      " [  0   0   0   0   0  24   2  68   1   0   0]\n",
      " [  0   0   0   0   0  19   2 103   0   0   0]\n",
      " [  0   0   0   0   0  10   1  80   0   0   0]\n",
      " [  0   0   0   0   0  13   0  43   0   0   0]\n",
      " [  0   0   0   0   0   1   0   6   0   0   0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_estimators= 8, \\\n",
    "                               max_depth= 3, min_samples_leaf= 8, min_samples_split = 26,\\\n",
    "                              max_leaf_nodes =5)\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(X_suite2_train, y_suite2_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_suite2_test)\n",
    "\n",
    "\n",
    "#  Add a column in the test dataset which holds the predictions for each row\n",
    "temp = pd.DataFrame(X_suite2_test)\n",
    "df_pred = pd.DataFrame(y_pred) \n",
    "df_pred.columns = ['predicted']\n",
    "\n",
    "test_dataset = pd.concat([temp, df_pred], axis=1, sort=False)\n",
    "print test_dataset.head()\n",
    "print test_dataset.shape\n",
    "\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Getting the accuracy metric\n",
    "acc = accuracy_score(y_pred, y_suite2_test)\n",
    "pre = precision_score(y_pred, y_suite2_test, average='micro')\n",
    "rec = recall_score(y_pred, y_suite2_test,average='micro')\n",
    "f1 = f1_score(y_pred, y_suite2_test, average='micro')\n",
    "print '\\n'\n",
    "print 'Model Performance Statistic Suite-2: '\n",
    "print 'Accuracy: ', acc\n",
    "print 'Precision: ', pre\n",
    "print 'Recall: ',rec\n",
    "print 'F1 Score: ', f1\n",
    "print 'Confusion Metrix:'\n",
    "print confusion_matrix(y_suite2_test, y_pred)\n",
    "print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-7 Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finding out the correlation between the features\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = data_clean_label_encoding [['bean_origin','REF','review_date','cocoa_percent','company_location'\\\n",
    "                   ,'rating','maker','bean_type', 'sub_bean_type',\\\n",
    "                   'country_origin']].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.set_context(\"paper\", rc={\"font.size\":12,\"axes.titlesize\":12, \"axes.labelsize\":12}) \n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "ax.set_title('Correlation Between The Pairs of Columns ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               rating\n",
      "review_date          \n",
      "2006         3.133333\n",
      "2007         3.132812\n",
      "2008         2.960526\n",
      "2009         3.060185\n",
      "2010         3.152632\n",
      "2011         3.255474\n",
      "2012         3.171474\n",
      "2013         3.190397\n",
      "2014         3.189858\n",
      "2015         3.242647\n",
      "2016         3.243056\n",
      "2017         3.347826\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The trend of chocolate rating over the years\n",
    "data_clean['rating'] = pd.to_numeric(data_clean['rating'], errors='coerce')\n",
    "\n",
    "gp = data_clean.groupby('review_date').aggregate({'rating':'mean'})\n",
    "print(gp)\n",
    "gp = gp.reset_index()\n",
    "\n",
    "\n",
    "# Plotting between review_date and average rating\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.pointplot(x=\"review_date\", y=\"rating\", markers='o', linestyles=\"--\",data=gp)\n",
    "plt.xlabel(\"Review Date\")#X-label\n",
    "plt.ylabel(\"Average Rating\")#Y-label\n",
    "plt.title(\"Average Chocolate Rating over the years\")#Title of the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for ratings:\n",
      "3.50    320\n",
      "3.00    285\n",
      "3.25    251\n",
      "2.75    210\n",
      "3.75    181\n",
      "2.50    115\n",
      "4.00     82\n",
      "2.00     28\n",
      "2.25     12\n",
      "1.50      9\n",
      "1.00      4\n",
      "5.00      2\n",
      "1.75      1\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The distribution of Rating for the chocolates in the dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "rating_counts = data_clean.rating.value_counts()\n",
    "print 'Count for ratings:\\n',rating_counts \n",
    "plt.figure(figsize = (18, 5))\n",
    "sns.barplot(x = rating_counts.index, y = rating_counts.values, palette = \"Blues\")\n",
    "plt.xlabel(\"Rating value\")\n",
    "plt.ylabel(\"Actual counts\")\n",
    "plt.title(\"Frequency of rating values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
